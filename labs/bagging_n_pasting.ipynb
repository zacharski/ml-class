{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bagging..png)\n",
    "\n",
    "\n",
    "Now we are about to embark on our journey from simple decision trees to algorithms that use decision trees as components. The path goes like this:\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dbxg.png)\n",
    "\n",
    "The use of decision trees began in the 1980s and XGBoost was introduced in 2016. Throughout the next few notebooks we will explore this progression of algorithms.  \n",
    "\n",
    "### A collective of classifiers\n",
    "\n",
    "To gain an intuition on how this works, let's look at how our confidence might increase when more people tell us something. Whether it is multiple doctors giving us the same diagnosis or something as simple as ...\n",
    "\n",
    "#### The Mary Spender example\n",
    "\n",
    "Let's say one of your friends mentions over lunch (pre-covid apparently) that you would love a particular musical artist on YouTube, say Mary Spender, who you never heard before. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/MarySpender2.png)\n",
    "\n",
    "What is the chance that you will actually like Mary Spender's music? Maybe slighly better than chance? Let's say you think there is a 60% chance you will like her. You will file away the recommendation but you are not going to rush home and watch a YouTube video.  Now, in addition to the lunch friend's recommendation,  an old music school friend, now living in Austin messages you saying you should check out Mary Spender and the friend predicts you will absolutely love her. Then a week later, while talking with an old bandmate over the phone, that bandmate, again, recommends Mary Spender. Over the course of less than 10 days, three of your friends independently (because they don't know one another) recommend Mary Spender. Now what is the likelihood of you liking Mary Spender? I am guessing you think that now it is higher than 60%. Maybe now you think it is 90% likely you will like her. It is the aggregate of these 3 people's opinions (3 classifiers) that ups the accuracy of the prediction.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/spender22.png)\n",
    "\n",
    "\n",
    "This is similar to how bagging works. One aggregates the votes of a number of classifiers and the vote of that ensemble of classifiers is more accurate than that of a single classifier. Even if the accuracy of each component classifier is low (known as a weak classifier), the ensemble can be a strong (high accuracy) classifier. Of course there are some caveats. \n",
    "\n",
    "Back to the Mary Spender example. Suppose one of your friends went to a Mary Spender concert and then later in the week met with four other of your friends and mentioned that she thought you would love Mary Spender's music. Then, over the course of a week all those friends recommended Mary Spender to you. In this case the recommendations are not that independent---all are based on one person's opinion. Thus, the accuracy would not be as great as in the example above. Similarly, if you made 10 copies of the exact same classifier each trained on exactly the same data, the accuracy of the ensemble of clones would not be any better than the accuracy of a single copy. Moving away from Mary Spender and our musical tastes and back to machine learning, we can try to create independence among the classifier in 2 ways:\n",
    "\n",
    "1. We can change the type of classifier. For example, we can use a k-Nearest Neighbor Classifier with Manhattan distance and a k of 5, a k-Nearest Neighbor Classifier with Euclidean distance and a k of 3, a decision tree classifier using entropy and a max depth of 5, and a decision tree classifier with using gini and no max depth specified. Hopefully, the accuracy of the ensemble of the four classifiers would be greater than that of a single classifier.\n",
    "2. We can have an ensemble of the same classifier (for example, 10 decision tree classifiers with identical hyperparameters) but each classifier can get a different subset of the training data. The classifiers would thus build different models (differents 'rules') and, again, the accuracy of the ensemble should be greather than that of a single classifier. This is the approach we will take.\n",
    "\n",
    "### Bagging and Pasting\n",
    "\n",
    "\n",
    "In this Jupyter notebook, we are going to explore Bagging algorithms. Bagging algorithms come in a variety of 'flavors' including one called 'bagging' and one called 'pasting'.\n",
    "\n",
    "But first an experiment on the what *with replacement* means. As you will see shortly, that term is the crucial difference between bagging and pasting.\n",
    "\n",
    "### A small experiment\n",
    "NOTE: The following code is just used for illustration and is nothing we will be using for machine learning. \n",
    "\n",
    "Consider a list of 5 red balls and 5 blue balls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = ['red', 'red', 'red', 'red', 'red',\n",
    "       'blue', 'blue', 'blue', 'blue', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to pick 7 random balls from this list. Python offers two functions that will give us random elements from a list.One is called `choices` which selects a sample with replacement, which means that once a ball is selected it is put back in the bag so it has the potential to be selected again. Let's give it a try, and just because things are random let's do this 100 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 blue and 0 red\n",
      "6 blue and 1 red\n",
      "7 blue and 0 red\n",
      "1 blue and 6 red\n",
      "0 blue and 7 red\n",
      "1 blue and 6 red\n",
      "1 blue and 6 red\n",
      "6 blue and 1 red\n",
      "6 blue and 1 red\n",
      "1 blue and 6 red\n",
      "6 blue and 1 red\n",
      "1 blue and 6 red\n",
      "6 blue and 1 red\n",
      "6 blue and 1 red\n",
      "Balls selected exceeded balls in bag: 14\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    set = random.choices(bag, k=7)\n",
    "    blue = set.count('blue')\n",
    "    red = set.count('red')\n",
    "    if blue > 5 or red > 5:\n",
    "        print(\"%i blue and %i red\" % (set.count('blue'), set.count('red')))\n",
    "        total +=1\n",
    "print(\"Balls selected exceeded balls in bag: %i\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A reminder: please don't mindlessly execute the code. Look at it and understand it*\n",
    "\n",
    "When I ran this, 14 times out of 100 had more of one color ball than there were in the original bag. In fact, several times I ended up with all 7 of the balls blue, even though the original list had only 5 balls:\n",
    "\n",
    "```\n",
    "7 blue and 0 red\n",
    "6 blue and 1 red\n",
    "7 blue and 0 red\n",
    "1 blue and 6 red\n",
    "0 blue and 7 red\n",
    "1 blue and 6 red\n",
    "1 blue and 6 red\n",
    "6 blue and 1 red\n",
    "6 blue and 1 red\n",
    "1 blue and 6 red\n",
    "6 blue and 1 red\n",
    "1 blue and 6 red\n",
    "6 blue and 1 red\n",
    "6 blue and 1 red\n",
    "Balls selected exceeded balls in bag: 14\n",
    "```\n",
    "Again, this is called selecting with replacement (we put what we selected back in the set before selecting again). \n",
    "\n",
    "The other alternative is to select without replacement--once we select something we can't select it again. Python's `sample` does this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balls selected exceeded balls in bag: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    set = random.sample(bag, k=7)\n",
    "    blue = set.count('blue')\n",
    "    red = set.count('red')\n",
    "    if blue > 5 or red > 5:\n",
    "        print(\"%i blue and %i red\" % (set.count('blue'), set.count('red')))\n",
    "        total +=1\n",
    "print(\"Balls selected exceeded balls in bag: %i\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of a specific colored ball that we select never exceeded the number of balls of that color in the original set.\n",
    "\n",
    "Now back to bagging and pasting. In both approaches we are going to sample the training data. Let's say we want 70% of the training data in our sample. In bagging ([Breiman, 1996](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf)), if we our training dataset is 1000 instances and we want 70% for a particular classifier, the algorithm will randomly select 700 out of the 1,000 **with replacement**. With pasting ([Breiman, 1998](https://link.springer.com/article/10.1023/A:1007563306331)), the selection is done **without replacement**. \n",
    "\n",
    "#### but wait, there is more ...\n",
    "\n",
    "There are two other options. Instead of selecting a random subset of training data instances, we can select a random subset of columns (features). Let's say we have a dataset of 1,000 instances each with 100 features. When we select a random subset of columns, we still have 1,000 instances but now they have just a subset of the features. This is called Random Subspaces ([Ho, 1998](https://pdfs.semanticscholar.org/b41d/0fa5fdaadd47fc882d3db04277d03fb21832.pdf?_ga=2.196949164.1638238666.1596910000-1073138517.1596910000)).\n",
    "\n",
    "Finally, we can train a classifier on both random subsets of instances and random subsets of features. This is known as Random Patches ([Louppe and Geurts, 2012](https://www.researchgate.net/publication/262212941_Ensembles_on_Random_Patches))\n",
    "\n",
    "In summary, the four methods are:\n",
    "\n",
    "* **bagging** - select a subset of data set instances using replacement\n",
    "* **pasting** - select a subset of data set instances without replacement\n",
    "* **Random Subspaces** - select a subset of features\n",
    "* **Random Patches** - select both a subset of features and of instances\n",
    "\n",
    "Let's see how this works!\n",
    "\n",
    "First, let's grab the Wisconsin Cancer data we used before:\n",
    "\n",
    "#### Wisconsin Cancer Dataset\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/aimam.png)\n",
    "(image from Nvidia's [AI Improves Breast Cancer Diagnoses by Factoring Out False Positives](https://blogs.nvidia.com/blog/2018/02/01/making-mammography-more-meaningful/))\n",
    "\n",
    "[A description of the Cancer Database](#Breast-Cancer-Database)\n",
    "\n",
    "In this dataset we are trying to predict the diagnosis---either M (malignant) or B (benign).\n",
    "\n",
    "Let's load the dataset and split it into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radiusAvg</th>\n",
       "      <th>textureAvg</th>\n",
       "      <th>perimeterAvg</th>\n",
       "      <th>areaAvg</th>\n",
       "      <th>smoothnessAvg</th>\n",
       "      <th>compactnessAvg</th>\n",
       "      <th>concavityAvg</th>\n",
       "      <th>concavityPointsAvg</th>\n",
       "      <th>symmetryAvg</th>\n",
       "      <th>...</th>\n",
       "      <th>radiusWorst</th>\n",
       "      <th>textureWorst</th>\n",
       "      <th>perimeterWorst</th>\n",
       "      <th>areaWorst</th>\n",
       "      <th>smoothnessWorst</th>\n",
       "      <th>compactnessWorst</th>\n",
       "      <th>concavityWorst</th>\n",
       "      <th>concavityPointsWorst</th>\n",
       "      <th>symmetryWorst&gt;</th>\n",
       "      <th>FractalDimensionWorst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>874217</th>\n",
       "      <td>M</td>\n",
       "      <td>18.31</td>\n",
       "      <td>18.58</td>\n",
       "      <td>118.60</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.08588</td>\n",
       "      <td>0.08468</td>\n",
       "      <td>0.08169</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>...</td>\n",
       "      <td>21.31</td>\n",
       "      <td>26.36</td>\n",
       "      <td>139.20</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.24450</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>0.06938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868223</th>\n",
       "      <td>B</td>\n",
       "      <td>11.71</td>\n",
       "      <td>16.67</td>\n",
       "      <td>74.72</td>\n",
       "      <td>423.6</td>\n",
       "      <td>0.10510</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>0.03592</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>...</td>\n",
       "      <td>13.33</td>\n",
       "      <td>25.48</td>\n",
       "      <td>86.16</td>\n",
       "      <td>546.7</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.069680</td>\n",
       "      <td>0.1712</td>\n",
       "      <td>0.07343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897604</th>\n",
       "      <td>B</td>\n",
       "      <td>12.99</td>\n",
       "      <td>14.23</td>\n",
       "      <td>84.08</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>0.09965</td>\n",
       "      <td>0.03738</td>\n",
       "      <td>0.020980</td>\n",
       "      <td>0.1652</td>\n",
       "      <td>...</td>\n",
       "      <td>13.72</td>\n",
       "      <td>16.91</td>\n",
       "      <td>87.38</td>\n",
       "      <td>576.0</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.19750</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.10090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895100</th>\n",
       "      <td>M</td>\n",
       "      <td>20.34</td>\n",
       "      <td>21.51</td>\n",
       "      <td>135.90</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.25650</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.2569</td>\n",
       "      <td>...</td>\n",
       "      <td>25.30</td>\n",
       "      <td>31.86</td>\n",
       "      <td>171.10</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.44920</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>0.268500</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.10240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86208</th>\n",
       "      <td>M</td>\n",
       "      <td>20.26</td>\n",
       "      <td>23.03</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>0.09078</td>\n",
       "      <td>0.13130</td>\n",
       "      <td>0.14650</td>\n",
       "      <td>0.086830</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>...</td>\n",
       "      <td>24.22</td>\n",
       "      <td>31.59</td>\n",
       "      <td>156.10</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.35390</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927241</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.74</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912521</th>\n",
       "      <td>B</td>\n",
       "      <td>12.58</td>\n",
       "      <td>18.40</td>\n",
       "      <td>79.83</td>\n",
       "      <td>489.0</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>23.08</td>\n",
       "      <td>85.56</td>\n",
       "      <td>564.1</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.06624</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.06431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8813129</th>\n",
       "      <td>B</td>\n",
       "      <td>13.27</td>\n",
       "      <td>17.02</td>\n",
       "      <td>84.55</td>\n",
       "      <td>546.4</td>\n",
       "      <td>0.08445</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.03554</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.1496</td>\n",
       "      <td>...</td>\n",
       "      <td>15.14</td>\n",
       "      <td>23.60</td>\n",
       "      <td>98.84</td>\n",
       "      <td>708.8</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.13110</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.096780</td>\n",
       "      <td>0.2506</td>\n",
       "      <td>0.07623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866458</th>\n",
       "      <td>B</td>\n",
       "      <td>15.10</td>\n",
       "      <td>16.39</td>\n",
       "      <td>99.58</td>\n",
       "      <td>674.5</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>0.18070</td>\n",
       "      <td>0.11380</td>\n",
       "      <td>0.085340</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>...</td>\n",
       "      <td>16.11</td>\n",
       "      <td>18.33</td>\n",
       "      <td>105.90</td>\n",
       "      <td>762.6</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.28830</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.07779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889403</th>\n",
       "      <td>M</td>\n",
       "      <td>15.61</td>\n",
       "      <td>19.38</td>\n",
       "      <td>100.00</td>\n",
       "      <td>758.6</td>\n",
       "      <td>0.07840</td>\n",
       "      <td>0.05616</td>\n",
       "      <td>0.04209</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>...</td>\n",
       "      <td>17.91</td>\n",
       "      <td>31.67</td>\n",
       "      <td>115.90</td>\n",
       "      <td>988.6</td>\n",
       "      <td>0.1084</td>\n",
       "      <td>0.18070</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.085680</td>\n",
       "      <td>0.2683</td>\n",
       "      <td>0.06829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        diagnosis  radiusAvg  textureAvg  perimeterAvg  areaAvg  \\\n",
       "id                                                                \n",
       "874217          M      18.31       18.58        118.60   1041.0   \n",
       "868223          B      11.71       16.67         74.72    423.6   \n",
       "897604          B      12.99       14.23         84.08    514.3   \n",
       "895100          M      20.34       21.51        135.90   1264.0   \n",
       "86208           M      20.26       23.03        132.40   1264.0   \n",
       "...           ...        ...         ...           ...      ...   \n",
       "927241          M      20.60       29.33        140.10   1265.0   \n",
       "8912521         B      12.58       18.40         79.83    489.0   \n",
       "8813129         B      13.27       17.02         84.55    546.4   \n",
       "866458          B      15.10       16.39         99.58    674.5   \n",
       "889403          M      15.61       19.38        100.00    758.6   \n",
       "\n",
       "         smoothnessAvg  compactnessAvg  concavityAvg  concavityPointsAvg  \\\n",
       "id                                                                         \n",
       "874217         0.08588         0.08468       0.08169            0.058140   \n",
       "868223         0.10510         0.06095       0.03592            0.026000   \n",
       "897604         0.09462         0.09965       0.03738            0.020980   \n",
       "895100         0.11700         0.18750       0.25650            0.150400   \n",
       "86208          0.09078         0.13130       0.14650            0.086830   \n",
       "...                ...             ...           ...                 ...   \n",
       "927241         0.11780         0.27700       0.35140            0.152000   \n",
       "8912521        0.08393         0.04216       0.00186            0.002924   \n",
       "8813129        0.08445         0.04994       0.03554            0.024560   \n",
       "866458         0.11500         0.18070       0.11380            0.085340   \n",
       "889403         0.07840         0.05616       0.04209            0.028470   \n",
       "\n",
       "         symmetryAvg  ...  radiusWorst  textureWorst  perimeterWorst  \\\n",
       "id                    ...                                              \n",
       "874217        0.1621  ...        21.31         26.36          139.20   \n",
       "868223        0.1339  ...        13.33         25.48           86.16   \n",
       "897604        0.1652  ...        13.72         16.91           87.38   \n",
       "895100        0.2569  ...        25.30         31.86          171.10   \n",
       "86208         0.2095  ...        24.22         31.59          156.10   \n",
       "...              ...  ...          ...           ...             ...   \n",
       "927241        0.2397  ...        25.74         39.42          184.60   \n",
       "8912521       0.1697  ...        13.50         23.08           85.56   \n",
       "8813129       0.1496  ...        15.14         23.60           98.84   \n",
       "866458        0.2001  ...        16.11         18.33          105.90   \n",
       "889403        0.1547  ...        17.91         31.67          115.90   \n",
       "\n",
       "         areaWorst  smoothnessWorst  compactnessWorst  concavityWorst  \\\n",
       "id                                                                      \n",
       "874217      1410.0           0.1234           0.24450        0.353800   \n",
       "868223       546.7           0.1271           0.10280        0.104600   \n",
       "897604       576.0           0.1142           0.19750        0.145000   \n",
       "895100      1938.0           0.1592           0.44920        0.534400   \n",
       "86208       1750.0           0.1190           0.35390        0.409800   \n",
       "...            ...              ...               ...             ...   \n",
       "927241      1821.0           0.1650           0.86810        0.938700   \n",
       "8912521      564.1           0.1038           0.06624        0.005579   \n",
       "8813129      708.8           0.1276           0.13110        0.178600   \n",
       "866458       762.6           0.1386           0.28830        0.196000   \n",
       "889403       988.6           0.1084           0.18070        0.226000   \n",
       "\n",
       "         concavityPointsWorst  symmetryWorst>  FractalDimensionWorst  \n",
       "id                                                                    \n",
       "874217               0.157100          0.3206                0.06938  \n",
       "868223               0.069680          0.1712                0.07343  \n",
       "897604               0.058500          0.2432                0.10090  \n",
       "895100               0.268500          0.5558                0.10240  \n",
       "86208                0.157300          0.3689                0.08368  \n",
       "...                       ...             ...                    ...  \n",
       "927241               0.265000          0.4087                0.12400  \n",
       "8912521              0.008772          0.2505                0.06431  \n",
       "8813129              0.096780          0.2506                0.07623  \n",
       "866458               0.142300          0.2590                0.07779  \n",
       "889403               0.085680          0.2683                0.06829  \n",
       "\n",
       "[114 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "colNames = ['id', 'diagnosis', 'radiusAvg', 'textureAvg', 'perimeterAvg', 'areaAvg',\n",
    "            'smoothnessAvg', 'compactnessAvg', 'concavityAvg', 'concavityPointsAvg',\n",
    "            'symmetryAvg', 'FractalDimensionAvg', 'radiusSE', 'textureSE', 'perimeterSE',\n",
    "            'areaSE','smoothnessSE', 'compactnessSE', 'concavitySE', 'concavityPointsSE',\n",
    "            'symmetrySE', 'FractalDimensionSE', 'radiusWorst', 'textureWorst', 'perimeterWorst',\n",
    "            'areaWorst', 'smoothnessWorst', 'compactnessWorst', 'concavityWorst', 'concavityPointsWorst',\n",
    "            'symmetryWorst>', 'FractalDimensionWorst']\n",
    "len(colNames)\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/wdbc.data', names=colNames)\n",
    "data.set_index('id', inplace=True)\n",
    "\n",
    "trainingdata, testdata = train_test_split(data, test_size = 0.2)\n",
    "testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now divide up the data into the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames.remove('id')\n",
    "colNames.remove('diagnosis')\n",
    "trainingDataFeatures = trainingdata[colNames]\n",
    "testDataFeatures = testdata[colNames]\n",
    "trainingDataLabels = trainingdata['diagnosis']\n",
    "testDataLabels = testdata['diagnosis']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a base accuracy using a single decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = clf.predict(testDataFeatures)\n",
    "\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a bagging classifier\n",
    "\n",
    "Let's build a collective of 20 decision tree classifiers (`n_estimators`). Let's train each one with 100 random samples from our dataset (`max_samples`) with replacement (`bootstrap=True`). `n_jobs` means how many jobs to run in parallel. `n_jobs=-1` means use all available CPU cores.   \n",
    "\n",
    "Just to reinforce the vocabulary we are learning, `n_estimators`, `max_samples`, `bootstrap` are among the **hyperparameters** of the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "bagging_clf = BaggingClassifier(clf, n_estimators=20, max_samples=100, \n",
    "                                bootstrap=True, n_jobs=-1)\n",
    "bagging_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = bagging_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this using a single decision tree classifier was 90.3% accurate, while the bagging classifier was 96.5% accurate--halving the error rate! that's pretty good!\n",
    "\n",
    "\n",
    "### Pasting\n",
    "Let's try the same thing with pasting (without replacement):\n",
    "\n",
    "For that we set the hyperparameter: `bootstrap=False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasting_clf = BaggingClassifier(clf, n_estimators=20, max_samples=100, \n",
    "                                bootstrap=False, n_jobs=-1)\n",
    "pasting_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = pasting_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Subspaces\n",
    "Again, random subspaces are when we randomly select feature subsets rather than subsets of the dataset instances. This time we will create 50 classifiers for our collective and each will train on a dataset with 7 features (`max_feature=7`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=50, max_features=7, \n",
    "                                bootstrap=True, n_jobs=-1)\n",
    "subspace_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = subspace_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches\n",
    "Finally, let's combine things and try random patches. In this example each classifier will be given a subset of 100 training instances with 7 features each.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385964912280702"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=100, max_features=7, \n",
    "                                 max_samples=100, bootstrap=False, n_jobs=-1)\n",
    "subspace_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = subspace_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is common to use a decision tree as the base classifier, we can use any classifier. Here we use kNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier()\n",
    "bagging_clf = BaggingClassifier(kNN, n_estimators=20, max_samples=100, \n",
    "                                bootstrap=True, n_jobs=-1)\n",
    "bagging_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = bagging_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "As you can see, any of these simple bagging algorithms typically outperforms using a single classifier. \n",
    "\n",
    "\n",
    "### Review\n",
    "\n",
    "We import the bagging classifier library with:\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "```\n",
    "\n",
    "and create an instance of one with:\n",
    "\n",
    "```\n",
    "my_bagging_classifier = BaggingClassifier(baseClassifier, Hyperparameters,n_jobs=-1)\n",
    "```\n",
    "\n",
    "#### Base Classifier\n",
    "while any classifier can be used we typically use a decision tree\n",
    "\n",
    "#### Hyperparameters\n",
    "Here is a list of the hyperparameters (from the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)):\n",
    "\n",
    "* `n_estimators`: integer, default value = 10, the number of classifiers (estimators) in the ensemble.\n",
    "* `max_samples`: integer or float, default value = 1.0(meaning use all the training instances), the number of samples (instances) to draw from the training dataset to train each base classifier.\n",
    "    * if integer, then draw max_features features.\n",
    "    * if float, then draw max_samples * X.shape[0] samples. For example if `max_samples` is 0.7 and there are 100 instances in the training dataset then draw 70 samples.\n",
    "* `max_features`, integer or float, default value =1.0,  \n",
    "the number of features to draw from the training dataset to train each base estimator \n",
    "    * if integers, then draw max_features features.\n",
    "    * if float, then draw max_features * X.shape[1] features.\n",
    "* `bootstrap` boolean, default value =True, whether samples and features are drawn with replacement. If False, sampling without replacement is performed.\n",
    "\n",
    "For other hyperparamters, consult the documentation.\n",
    "\n",
    "## You try: Predicting musical genres from audio file attributes\n",
    "\n",
    "When you listen even to a few seconds of a song you can identify it as blues, country, classical, or any other genre. How do you do this? What attributes are you hearing in the audio file that helps you make this classification? And, more to the point, can we train a computer to do it?\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bluesClassical.png)\n",
    "\n",
    "We are going to be using  the [GTZAN Dataset for Music Genre Classification](https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification). It provides data of 100 songs for each of 10 genres. The data is in several formats:\n",
    "\n",
    "* 30 second audio files (wav)\n",
    "* spectral images of those 30 second clips (see image above)\n",
    "* a csv file containing acoustic attributes of the 30 second clip\n",
    "* a csv file containing acoustic attributes of 3 second clips (the 30 second clips were split into 3 second ones)\n",
    "\n",
    "We are going to use the 3 second csv file which is available at \n",
    "\n",
    "https://raw.githubusercontent.com/zacharski/ml-class/master/data/gtzan.csv\n",
    "\n",
    "Go ahead and load the data into a dataframe (the first row contains feature names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = \"TO DO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the values of the label column (the genres):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz',\n",
       "       'metal', 'pop', 'reggae', 'rock'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the 10 genres we are trying to predict. So if we were just to guess without hearing the clip, we would be accurate 10% of the time. How accurate do you think you would be based on hearing a 3 second clip? I am pretty confident I could correctly label the 30 second clips, but I am much less confident about labeling 3 second ones. Since guessing randomly would give me 10% accuracy, I am estimating maybe 50-60% accuracy. Let's see how a computer does.\n",
    "\n",
    "#### Feature Names\n",
    "So the column we are trying to predict is `label`. Now let's get the names of the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length', 'chroma_stft_mean', 'chroma_stft_var', 'rms_mean', 'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var', 'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean', 'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var', 'harmony_mean', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'tempo', 'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean', 'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var', 'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean', 'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var', 'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean', 'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var', 'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean', 'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "featureNames = list(music.columns)\n",
    "featureNames.remove('filename')\n",
    "featureNames.remove('label')\n",
    "print(featureNames)\n",
    "print(len(featureNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 58 features. \n",
    "\n",
    "#### Training and test sets\n",
    "Now it is time to construct the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the original data 80% going into the music_training dataset \n",
    "## the rest in music_test\n",
    "music_training, music_test = ('TO DO', 'TO DO)\n",
    "                              \n",
    "## now create the DataFrames for just the features (excluding the label column \n",
    "## filename column)                              \n",
    "music_training_features = \"TO DO\"\n",
    "music_test_features = \"TO DO\"\n",
    "                              \n",
    "## now create the labels data structure for both the training and test sets                              \n",
    "music_training_labels = \"TO DO\"\n",
    "music_test_labels = 'TO DO'\n",
    "music_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a single decision tree classifier\n",
    "Let's build a single decision tree classifier called `clf` using entropy, fit it to the data, make predictions and determine the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6631631631631631"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create clf, an instance of the Decision Tree Classifier\n",
    "# TO DO\n",
    "\n",
    "## Fit it to the data\n",
    "# TO DO\n",
    "\n",
    "## get the predictions for the test set\n",
    "# TO DO\n",
    "\n",
    "## get the accuracy score\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I did this I got 66% accuracy. That doesn't sound great but keep in mind that random guessing would only be 10% accuracy. \n",
    "\n",
    "### Building a Random Patch Classifier\n",
    "\n",
    "Now we are going to build a random patch classifier.\n",
    "\n",
    "* the base classifier will be a decision tree using entropy\n",
    "* the ensemble will contain 20 base classifiers\n",
    "* each classifier will use a random sample of 70% of the training data\n",
    "* each classifier will use a random sample of 70% of the features\n",
    "* the sampling will be done with replacement\n",
    "* it will use all available cpu cores.\n",
    "\n",
    "We are going to\n",
    "\n",
    "1. build the classifier\n",
    "2. train the classifier on the data\n",
    "3. make predictions on the test set\n",
    "4. determine the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8268268268268268"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy did you get? Was it better than using a single classifier?\n",
    "Keep your original code above. Make a copy of it below and \n",
    "experiment a bit with the hyperparameters. (try 3 or 4 different things) What is the best accuracy you can get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "### Breast Cancer Database\n",
    "\n",
    "[back](#Wisconsin-Cancer-Dataset)\n",
    "\n",
    "  This breast cancer databases was obtained from the University of Wisconsin\n",
    "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
    "   when using this database, then please include this information in your\n",
    "   acknowledgements.  Also, please cite one or more of:\n",
    "\n",
    "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology\", \n",
    "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
    "      December 1990, pp 9193-9196.\n",
    "\n",
    "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
    "      via linear programming: Theory and application to medical diagnosis\", \n",
    "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
    "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
    "\n",
    "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
    "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
    "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
    "\n",
    "Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "\n",
    "Sources:\n",
    "   -- Dr. WIlliam H. Wolberg (physician)\n",
    "      University of Wisconsin Hospitals\n",
    "      Madison, Wisconsin\n",
    "      USA\n",
    "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "      Received by David W. Aha (aha@cs.jhu.edu)\n",
    "   -- Date: 15 July 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
