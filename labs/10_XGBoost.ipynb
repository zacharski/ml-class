{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L5KyC6KP_yY"
   },
   "source": [
    "# XGBoost Lab\n",
    "\n",
    "#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "## Reflections\n",
    "Let's go back to thinking about a few algorithms we worked on.\n",
    "\n",
    "\n",
    "### Decisions trees\n",
    "We began our exploration of decision trees with a mountain bike example:\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dtree88.png)\n",
    "\n",
    "Here's is roughly what we did by hand.\n",
    "\n",
    "1. We determined that if we couldn't ask any questions, we would say the person mountain biked since they mountain biked 9 times and didn't 5 times. So our error rate was 5 out of 14 or roughly 36%\n",
    "2. Next, if we could ask one question we determined that the question should be about Outlook. Now our error rate was 4 out of 14 or 29%\n",
    "3. Then we determined the next question to ask and reduced the error rate more. And then the next question ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In some sense, the algorithm is additive. We start with zero questions with whatever error rate. Add a question and reduce the error rate. Add another question and reduce the rate. And so on.\n",
    "\n",
    "**Additive** is the key word. Let's look at an example, from [Gradient boosting: Distance to target](https://explained.ai/gradient-boosting/L2-loss.html)  by Terence Parr and Jeremy Howard. They ask us to imagine writing the formula for *y* that matches this plot:\n",
    "\n",
    "![](https://explained.ai/gradient-boosting/images/L2-loss/L2-loss_additive_2.svg)\n",
    "\n",
    "Like the decision tree example above, our first approximation might be simple, perhaps just the y-intercept:\n",
    "\n",
    "$$y = 30$$\n",
    "\n",
    "as shown in the leftmost picture below. \n",
    "\n",
    "![](https://explained.ai/gradient-boosting/images/L2-loss/L2-loss_additive_3.svg)\n",
    "\n",
    "Next, we may want to add in the slope of the line and get\n",
    "\n",
    "$$y = 30 + x$$\n",
    "\n",
    "and get the middle graph above.  Finally, we add in the squiggle:\n",
    "\n",
    "$$y = 30 + x + sin(x)$$\n",
    "\n",
    "We have decomposed a complex task into subtasks, each refining the previous approximation. So, again, we have an additive algorithm.\n",
    "\n",
    "This approach shouldn't be surprising to us since this is how we typically develop programs. We get some skeleton code working and then incrementally add to it.\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting algorithms work in a similar additive fashion. We first develop a simple model that roughly classifies the data. Next, we add another simple model that is focused on ameliorating the errors of the first. And then we add another and another.\n",
    "\n",
    "$$boosting=model_1 + model_2 + model_3 + ... + model_n$$\n",
    "\n",
    "\n",
    "### How boosting differs from bagging and pasting\n",
    "\n",
    "With bagging and pasting we created a number of decision trees each of which was trained on different data. **One tree did not influence the construction of another.** Thus, each classifier was independent of the others.\n",
    " \n",
    "\n",
    "#### Boosting\n",
    "Boosting is different. \n",
    "\n",
    "Imagine that we create one decision tree classifier. Let's call it Classifier 1. Classifier 1 doesn't perform with 100% accuracy. \n",
    "\n",
    "Next we create a second decision tree classifier and as part of its training data we will use the instances that Classifier 1 got wrong. Now Classifier 2 isn't perfect either and there will be some instances that both Classifier 1 and Classifier 2 got wrong, and, you guessed it, we will use those instances as part of the training data for Classifier 3.\n",
    "\n",
    "#### 400 Classifiers\n",
    "Suppose we created 400 classifiers using the bagging algorithm. Since each classifier is independent of the others, we can run those 400 in parallel. \n",
    "\n",
    "Now think about boosting for a moment. Can we run those in parallel? Think about it for \n",
    "\n",
    "1. second\n",
    "2. seconds\n",
    "3. seconds\n",
    "4. seconds\n",
    "5. seconds\n",
    "\n",
    "\n",
    "\n",
    "Since one classifier is dependent on the errors of the others it seems like we couldn't run them in parallel and training 400 classifiers  sequentially seems impractical. This is true in general with boosting algorithms but as we will see XGBoost is different.\n",
    "\n",
    "### Gradient Boosting\n",
    "Suppose I am interested in taking my camper van \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/travato2.png)\n",
    "\n",
    "\n",
    "to White Horse Road Dispersed Camping in Utah.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/wildHorse.png)\n",
    "\n",
    "And to get there from my home in Santa Fe, I am using an old school paper map.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/map.png)\n",
    "\n",
    "A route will be something like.\n",
    "\n",
    "$$route = road_0 + road_1 + road_2 + ... + road_n$$\n",
    "\n",
    "To get to White Horse Road, it looks like my best bet is to start by taking I25 to Albuquerque.\n",
    "\n",
    "\n",
    "$$route = i25 $$\n",
    "\n",
    "Now the difference between where I am and where I want to go is Albuquerque to White Horse. So I performed an action and now my new problem is dealing with this new problem of getting from Albuquerque to White Horse\n",
    "\n",
    "From Albuquerque I can take 550 to Farmington\n",
    "\n",
    "$$route = i25  + US550$$\n",
    "\n",
    "and from there take 491 to Monticello Utah\n",
    "\n",
    "\n",
    "$$route = i25  + US550+ US491$$\n",
    "\n",
    "and so on.\n",
    "\n",
    "There are some similarities between this old school mapping and gradient boosting. In gradient boosting we start with a poor model (in our case, we decided to go to Albuquerque). Then we are going to look at the difference between what we want and where we are-- and then take the next step, the delta $\\Delta$. \n",
    "\n",
    "\n",
    "Let's look at a simple example of classification of one feature *x* to predict a label *y*. We will label our prediction $\\hat{y}$. For gradient boosting our formula is\n",
    "\n",
    "$$\\hat{y}=f_0(x) + \\Delta_1(x) + \\Delta_2(x) + ... + \\Delta_m(x)$$\n",
    "\n",
    "Where $\\Delta_1$ is the first improvement, $\\Delta_2$ the second and so on.\n",
    "\n",
    "Gradient Boosting is an ensemble method, meaning that it is built with a number of sub-classifiers. So perhaps a better Utah analogy is that I hitchhike from here to Albuquerque with one person (one 'classifier'), then go to Framington with another and so on.\n",
    "\n",
    "\n",
    "This is the rough intuition of gradient boosting. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In any gradient algorithm there is a parameter called *learning rate* and in a sense it is how big of steps we can take. \n",
    "\n",
    "Suppose we are hiking on a mountain in Utah and suddenly we are fogged in and can't see a thing. We want to get back to our van in the valley.\n",
    "\n",
    "In my 2D Utah it looks like this:\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/gradient1b.png)\n",
    "\n",
    "The yellow dot is us near the top of the mountain and the pinkish red dot is our van. So our algorithm is\n",
    "\n",
    "\n",
    "```\n",
    "WHILE NOT AT VAN OR NOT MOVING:\n",
    "  take one step to the left.\n",
    "  IF we are lower than when we started:\n",
    "     stay here at the new location\n",
    "  ELSE\n",
    "     go back to starting point and go one step to the right\n",
    "     IF we are lower than when we started:\n",
    "        stay here at the new location\n",
    "     ELSE\n",
    "        go back to starting point\n",
    "```\n",
    "\n",
    "We repeat the above procedure and get to the state shown on the right above. If we take a step to the right or left we go uphill so we are stuck. We hit what is called a local minima and local minima are a problem with all gradient descent algorithms.\n",
    "\n",
    "Perhaps the one step was too small an increment. So let's say we have a rope. You stay where we are and hold one end of the rope and I walk until I reach the end of the rope. Based on the angle of the rope, we see if I am lower or not and we move accordingly. Now we jump over that local minima and reach a state that looks like the following image on the left. We don't know it, but we are almost to the van!\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/gradient2b.png)\n",
    "\n",
    "We use the rope technique again but this time I jump over the location of our van since I am not at the end of the rope yet and am in the position shown on the right. The learning rate was too large. (Now I am sounding like the three bears tale!)\n",
    "\n",
    "The one step was our learning rate as was our rope technique and you can see that selecting a good one is crucial. \n",
    "\n",
    "#### Loss Function\n",
    "For both these examples, one thing we needed was a measure for how far away are we from our goal. Are we better or worse? For the fog on a mountain example, the loss function was our altitude and we are trying to reduce the loss -- the altitude. \n",
    "\n",
    "\n",
    "### Two more examples\n",
    "\n",
    "#### One Dimensional Team Frisbee Golf\n",
    "Here is my representation of our 1D golf game. The hole is the green circle on the right and our frisbee's location is shown with the lovely pink circle on the left. Let $y$ be the actual distance between the two and $x$ what I see standing by the frisbee--off in the one dimensional distance I see the hole. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf1.png)\n",
    "\n",
    "\n",
    "It is player zero's turn and she estimates the distance to be 70 yards.\n",
    "\n",
    "$$f_0(x) = 70$$ \n",
    "\n",
    "She flings the frisbee and ...\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf2.png)\n",
    "\n",
    "Now it is player two's turn. He is only concerned with the  difference, the $\\Delta_1$ --the current position of the frisbee and the location of the hole. He estimates it to be 20 yards\n",
    "\n",
    "$$\\Delta_1(x) = 20$$\n",
    "\n",
    "So far we have flung the frisbee\n",
    "\n",
    "$$\\hat{y}= f+0(x) + \\Delta_1(x) = 70 + 20 = 90$$\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf3.png)\n",
    "\n",
    "Now it is player two's turn. She estimates the distance remaining ($\\Delta_2$) to be 15 yards...\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf4.png)\n",
    "\n",
    "And she overshot. \n",
    "\n",
    "Player three estimates the remaining distance to be -5 yards and ...\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf5.png)\n",
    "\n",
    "Notice that each player is not concerned with the original problem. She is just concerned with the **residual** --- meaning what is remaining based on the previous players' results.\n",
    "\n",
    "The formula is \n",
    "\n",
    "$$\\hat{y} = f_0(x) + \\Delta_1(x) + \\Delta_2(x) + ... + \\Delta_m(x)$$\n",
    "$$=f_0(x) + \\sum_{m=1}^M{\\Delta_m(x)}$$\n",
    "\n",
    "So the first classifier works on the original problem but all the rest work on the residual.\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "### Expenditures on Makeup and Clothes\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/makeup33.png)\n",
    "Ok, I have exhausted my creativity, so even though I am not keen on this example, let's go back to predicting a young lady's expenditure on makeup based on what she spends on clothes.  And just for readability I am going to make the feature clothes to be represented by *x* and what we want to predict, the makeup, *y*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "d3c2iMbCP9aq",
    "outputId": "dffb080b-a3ef-471e-c95c-82affab9e315"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "makeup =  [3000, 5000, 12000, 2000, 7000,  15000,  5000,  6000, 8000,  10000]\n",
    "clothes = [7000, 8000, 25000, 5000, 12000, 30000, 10000, 15000, 20000, 18000]\n",
    "ladies = ['Ms A','Ms B','Ms C','Ms D','Ms E','Ms F','Ms G','Ms H','Ms I','Ms J',]\n",
    "monthly = DataFrame({'x': clothes, 'y': makeup}, index= ladies)\n",
    "monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3a-bhB1LOCG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8hWOON-0qEU"
   },
   "source": [
    "And for our first prediction $f_0$ let's predict just the average value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "ddG9_pVPTwDW",
    "outputId": "ddc551fe-1c37-4191-9035-a8368f47ea0b"
   },
   "outputs": [],
   "source": [
    "monthly['f0'] = monthly.y.mean()\n",
    "monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijAYdpZd2fzx"
   },
   "source": [
    "Our first classifier, f0, is trying to predict the y column values:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/predictY.png)\n",
    "\n",
    "and the differences between our predictions and the actual values (in other words, the delta Δ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "CKG3OvcB1RWv",
    "outputId": "68314412-9313-47bc-ce5b-c0c42b9454eb"
   },
   "outputs": [],
   "source": [
    "monthly['Δ1'] = monthly.y - monthly.f0\n",
    "monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DnOnlTqFavs"
   },
   "source": [
    "For Ms A, her makeup expenditure was ¥3,000, Our classifier f0 predicted ¥7,300 so Δ1 (the difference) is -4,300.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/delta.png)\n",
    "\n",
    "Let's compute the average error ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoiWhiHkAk91",
    "outputId": "3d055199-22d8-4858-91b6-004cacdbaf31"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"The average error is \", monthly['Δ1'].abs().sum() / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYYABnmp2niq"
   },
   "source": [
    "That Δ1 or ($y-f_0$) is the residual. What is left, or how far the first classifier was off. The residual is what the second classifier is trying to predict.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/predictDelta.png)\n",
    "\n",
    "Next, we are going to create a classifier Δ1 that predicts Δ1 from the x. Since we think that people who spend more on clothes spend more on makeup, let's make f1:\n",
    "\n",
    "$$f1 = (x - mean(x)) / 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "OVqyVkYF2XSO",
    "outputId": "c4439481-2875-4d3c-983a-174a277a207d"
   },
   "outputs": [],
   "source": [
    "x_mean = monthly.x.mean()\n",
    "monthly['f1'] = (monthly.x - x_mean)/2\n",
    "monthly['Δ2'] = monthly['Δ1'] - monthly['f1']\n",
    "monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEhcW8bPFikD",
    "outputId": "b49b2288-13d2-421f-b854-41f61fc540bc"
   },
   "outputs": [],
   "source": [
    "print(\"The average error is \", monthly['Δ2'].abs().sum() / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T72unS4a7HJP"
   },
   "source": [
    "Our error rate have gone down substantially. From 3,160 to 800. We are getting closer!  And our set of classifiers were:\n",
    "\n",
    "\n",
    "$$prediction(x) = f0(x) + f1(x) + ...$$\n",
    "\n",
    "And the next classifier will try to predict Δ2 based on x.\n",
    "\n",
    "If you understand all these examples, from Utah to Makeup, you have a pretty good intuition on how Gradient Boosting works.\n",
    "\n",
    "# XGBoost\n",
    "You may recall that in the first few videos, we mentioned that XGBoost was one of the state-of-the-art algorithms. The Kaggle competition winners are dominated by deep learning and XGBoost solutions.\n",
    "\n",
    ">I only use XGBoost (Liberty Mutual Property Inspection, Winner's Interview: Qingchen Wang)\n",
    "\n",
    "> As the winner of an increasing amount of Kaggle competitions XGBoost showed us again to be a great all-around algorithm worith having in your toolbox (Dato Winner's Interview, 1st Place, Mad Professors)\n",
    "\n",
    "> The only supervised learning method I used was gradient boosting as implemented in the excellent xgboost package (Recruit Coupon Purchase Winner's Interview, 2nd place, Halla Yang)\n",
    "\n",
    "\n",
    "\n",
    "We are going to start our exploration of XGBoost using the Iris dataset, which we have used before.\n",
    "\n",
    "This reminds me of a section of the *Hitchhiker's Guide to the Galaxy* by Douglas Adams, where Marvin, the robot, is asked to bring two hitchhikers to the bridge and he says:\n",
    "\n",
    "> Here I am, brain the size of a planet, and they ask me to take you to the bridge. Call that job satisfaction, 'cause I don't\n",
    "\n",
    "XGBoost is an extremely powerful state-of-the-art algorithm and we are using it on a toy example. Oh well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "ma21zshP4umI",
    "outputId": "3bffbc58-3da7-4af4-900c-f7f61aa3f765"
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('1jLIRJwfZhg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8XREkjWB5fS"
   },
   "source": [
    "\n",
    "\n",
    "### GPU!\n",
    "We are going to be running this code on a Graphics Processing Unit, GPU, a graphics card.\n",
    "\n",
    "To do so, under the runtime menu above, select **Change Runtime Type** and select **GPU**\n",
    "\n",
    "That's it! Now let's check out what GPU we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83Bp2hvE6bcg",
    "outputId": "c0685533-b6fe-4c97-8a0a-9b1f85cbfe28"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZmJUO_2CGez"
   },
   "source": [
    "In my case it looked like \n",
    "\n",
    "```\n",
    "Sat Aug 13 02:47:01 2022       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "\n",
    "It is a Tesla T4, which has 320 tensor cores.\n",
    "\n",
    "\n",
    "\n",
    "Now let's load the database\n",
    "\n",
    "## The Iris Data Set\n",
    "\n",
    "### Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwUq3mqwB9EL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/iris.csv')\n",
    "iris_X = iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]\n",
    "iris_Y = iris['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the unique values for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_Y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the label values are strings. However, XGBoost requires the values to be integers starting at 0.  We can use sklearn's `LabelEncoder` to do the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "iris_Y = le.fit_transform(iris_Y)\n",
    "iris_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can divide the data into training and testing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(iris_X, iris_Y, test_size=0.2)\n",
    "test_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmHyfTQeCUQs"
   },
   "source": [
    "### Create an instance of the XGBoost classifier\n",
    "We are going to create an XGBoost classifier with gpu support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acYGT2PzCRJV",
    "outputId": "70f82e37-deaf-42d3-8c75-e5f45374abf1"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "params = { \"n_estimators\": 400, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor' }\n",
    "\n",
    "model = XGBClassifier(**params)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX_yoP5kDKjl"
   },
   "source": [
    "Let's take a look at those parameters.\n",
    "\n",
    "* **n_estimators** the number of classifiers in the boost ensemble. The default is 100.\n",
    "* **tree_method** the tree construction algorithm that is used. `gpu_hist` is a distributed histogram approach (see the [original paper](https://arxiv.org/pdf/1603.02754.pdf))\n",
    "* **predictor** the prediction algorithm to use. `gpu_predictor` means use the gpu!\n",
    "* **max_depth** the depth of the decision trees. The default of 3 is used here. The trees for any ensemble method are typically very shallow. \n",
    "\n",
    "### Fitting model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBxXaHTVCbt0",
    "outputId": "9f1b50d8-9906-4af3-94da-86a354a4ef6e"
   },
   "outputs": [],
   "source": [
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xekQss6bFp65"
   },
   "source": [
    "### evaluate model\n",
    "Finally let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SO-caKnjFlG-",
    "outputId": "8d5e0092-69bd-4e29-d729-d8f182fd7d92"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris_predictions = model.predict(test_X)\n",
    "accuracy_score(test_Y, iris_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoX-aaylF8He"
   },
   "source": [
    "We ran a state-of-the-art algorithm on a GPU. Yay us!\n",
    "\n",
    "\n",
    "\n",
    "Now we are going to back up quite a bit.\n",
    "\n",
    "#### Bagging and Pasting\n",
    "With bagging and pasting we created a number of decision trees each of which was trained on different data. One tree did not influence the construction of another. Each classifier was independent of the others.\n",
    " \n",
    "\n",
    "#### Boosting\n",
    "Boosting is different. \n",
    "\n",
    "Imagine that we create one decision tree classifier. Let's call it Classifier 1. Classifier 1 doesn't perform with 100% accuracy. \n",
    "\n",
    "Next we create a second decision tree classifier and as part of its training data we will use the instances that Classifier got wrong. Now Classifier 2 isn't perfect either and there will be some instances that both Classifier 1 and Classifier 2 got wrong, and, you guessed it, we will use those instances as part of the training data for Classifier 3.\n",
    "\n",
    "#### 400 Classifiers\n",
    "Suppose we created 400 classifiers using the bagging algorithm. Since each classifier is independent of the others, we can run those 400 in parallel. \n",
    "\n",
    "Now think about boosting for a moment. Can we run those in parallel?\n",
    "\n",
    "Since one classifier is dependent on the errors of the others it seems like we couldn't run them in parallel and doing 400 classifiers in series seems impractical. Fortunately for us, XGBoost has parallelized training!\n",
    "\n",
    "\n",
    "# The task - The Adult Dataset\n",
    "\n",
    "Let's try a bit larger dataset, the [Adult Dataset](http://archive.ics.uci.edu/ml/datasets/Adult). The webpage describes the problem. We are trying to predict whether someone makes more that $50,000 year based on a number of features. The data folder contains both training data `adult.data` and test data `adult.test`. \n",
    "\n",
    "## Prepare the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6O1qfbyAFvgW",
    "outputId": "d5b5b757-b8fd-4f1d-88eb-dca5ce4748b6"
   },
   "outputs": [],
   "source": [
    "colNames = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "            'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'wage']\n",
    "adult = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=colNames)\n",
    "adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Iodm2UOrs7"
   },
   "source": [
    "## divide features and labels\n",
    "let's create 2 DataFrames, one for the features and one for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8I01XNJzPOI2",
    "outputId": "b8ba96e5-b4bd-4233-b86f-c39ee94de432"
   },
   "outputs": [],
   "source": [
    "adult_features = adult.drop('wage', axis=1)\n",
    "adult_labels = adult['wage']\n",
    "adult_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTPkOHzNITR"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>1. OneHotEncoder</font> \n",
    "\n",
    "Now let's one hot encode the features using sklearn's OneHotEncoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "JCItEsf8K2TZ",
    "outputId": "6aadb4c7-1c2e-4724-b644-7d0aeda6c936"
   },
   "outputs": [],
   "source": [
    "#TODO  \n",
    "\n",
    "\n",
    "adultSparse = \"TO DO\"\n",
    "adultSparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZyAn3WLZMqV"
   },
   "source": [
    "Fantastic!  \n",
    "\n",
    "Let's go ahead divide this up into training and test sets (Notice that this is a bit different than we have been doing it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "pc-AXr9FZGhW",
    "outputId": "c786231e-9439-4054-ddb3-882dc077dbce"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "adult_train_features, adult_test_features, adult_train_labels, adult_test_labels = train_test_split(adultSparse, adult_labels, test_size = 0.7)\n",
    "adult_train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRDTurCqa9nq"
   },
   "source": [
    "You may have noticed that we put a whopping 70% of the data in the test set. We did this because when we are just playing with things to gain an understanding we don't want to wait hours for a result.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Create an XGBoost classifier</font> \n",
    "Create an XGBoost classifier called model with the parameters:\n",
    "\n",
    "* `tree_method: gpu_hist`\n",
    "* `predictor: gpu_predictor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Py0OOb5aBy3"
   },
   "outputs": [],
   "source": [
    "## TO DO call it model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eks7EiOebgkj"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Parameter Grid</font> \n",
    "\n",
    "Now let's say we want to find the best hyperparameter values for \n",
    "\n",
    "* n_estimators -- let's try 50, 100, 150, 200\n",
    "* max_depth -- let's try 2, 4, 6, 8\n",
    "\n",
    "\n",
    "Go ahead and create the `param_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkqwXndrbfhP"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4GoFt4hc4zt"
   },
   "source": [
    "\n",
    "\n",
    "### Time Constraint\n",
    "\n",
    "Even with a GPU it is going to take a long time to do an exhaustive search of which parameters are best. There are 16 possible combinations. We may want 5 fold cross validation. That is 80 fits, each of which is creating on average 100 classifiers. And we have around 20,000 instances in our training data. Let's pick a random smaller set of combinations to test.  Let's say we want the search algorithm to select 5 combinations of hyperparameters `param_comb` at random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZkjYTZHc1iP"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_comb = 5\n",
    "folds=5\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=param_comb,  n_jobs=-1, \n",
    "                                   cv=skf.split(adult_train_features,adult_train_labels), verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3syNvBizdMYj"
   },
   "source": [
    "Let's fit the model (this will take awhile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "BVxaKf8OdK9n",
    "outputId": "2e668137-b516-4d8e-994a-af1871714084"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "grid_result = random_search.fit(adult_train_features, adult_train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhJmfE6gM6IJ"
   },
   "source": [
    "Now let's see what the best parameters are, make predictions on our test data, and check accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CFPTUpq_dT0s",
    "outputId": "c2a87777-7ebe-4e22-c72d-305b1c4b593d"
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTboDXG8xsRV"
   },
   "outputs": [],
   "source": [
    "predictions = random_search.best_estimator_.predict(adult_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dA0tfa2n7t2w",
    "outputId": "deb9a6ff-6726-4902-aae7-74149e956454"
   },
   "outputs": [],
   "source": [
    "accuracy_score(adult_test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmC50ToeLn7S"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font> \n",
    "## <font color='#EE4C2C'>Predicting musical genres from audio file attributes</font> \n",
    "\n",
    "\n",
    "We played with this dataset in the bagging and pasting lab.\n",
    "\n",
    "### <font color='#EE4C2C'>Does XGBoost outperform the bagging and pasting algorithms we used in that lab?</font> \n",
    "\n",
    "Show your work in detail. Also include a short written summary.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bluesClassical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTPwWCmh7y8k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Working version of xgboostLab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
