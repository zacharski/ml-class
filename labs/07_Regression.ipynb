{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4cnBhHe-dlE"
   },
   "source": [
    "# Regression\n",
    "\n",
    "So far, in our exploration of machine learning, we have build sysems that predict discrete values: \n",
    "\n",
    "* whether a particular plant is Iris Setosa, Iris Versicolour, or Iris Virginica\n",
    "* whether a person has diabetes or not\n",
    "* whether a tumor is malignant or benign \n",
    "\n",
    "\n",
    "Even deciding whether an image is of a particular digit:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/MnistExamples.png)\n",
    "\n",
    "or which of 1,000 categories does a picture represent.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodleSmall.jpg)\n",
    "\n",
    "\n",
    "This lab looks at how we can build classifiers that predict **continuous** values and such classifiers are called regression classifiers.\n",
    "\n",
    "First, let's take a small detour into correlation.\n",
    "\n",
    "## Correlation\n",
    "A correlation is the degree of association between two variables. One of my favorite books on this topic is: \n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/mg_statistics_big.png)\n",
    "\n",
    "\n",
    "\n",
    "and they illustrate it by looking at\n",
    "## Ladies expenditures on clothes and makeup\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/mangaCorrelation1.png)\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/mangeCorrelation2.png)\n",
    "\n",
    "\n",
    "\n",
    "So let's go ahead and create that data in Pandas and show the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "bqpDwvW3-dlF",
    "outputId": "2aa782ae-aad9-4c17-9753-2cf9d3b1879c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "makeup =  [3000, 5000, 12000, 2000, 7000,  15000,  5000,  6000, 8000,  10000]\n",
    "clothes = [7000, 8000, 25000, 5000, 12000, 30000, 10000, 15000, 20000, 18000]\n",
    "ladies = ['Ms A','Ms B','Ms C','Ms D','Ms E','Ms F','Ms G','Ms H','Ms I','Ms J',]\n",
    "monthly = DataFrame({'makeup': makeup, 'clothes': clothes}, index= ladies)\n",
    "monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Yyce16P-dlJ"
   },
   "source": [
    "and let's show the scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "f5QIDuDg-dlJ",
    "outputId": "14862ecd-81e2-4d93-f321-01e7ff285667"
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "output_notebook()\n",
    "x = figure(title=\"Montly Expenditures on Makeup and Clothes\", x_axis_label=\"Money spent on makeup\", y_axis_label=\"Money spent on clothes\")\n",
    "x.circle(monthly['makeup'], monthly['clothes'], size=10, color=\"navy\", alpha=0.5)\n",
    "output_file(\"stuff.html\")\n",
    "\n",
    "show(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBH5fNxT-dlN"
   },
   "source": [
    "When the data points are close to a straight line going up, we say that there is a positive correlation between the two variables. So in the case of the plot above, it visually looks like a postive correlation. Let's look at a few more examples:\n",
    "\n",
    "## Weight and calories consumed in 1-3 yr/old children\n",
    "This small but real dataset examines whether young children who weigh more, consume more calories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "bscoNATT-dlN",
    "outputId": "f873b239-dc01-4050-eae3-451a05544017"
   },
   "outputs": [],
   "source": [
    "weight = [7.7, 7.8, 8.6, 8.5, 8.6, 9, 10.1, 11.5, 11, 10.2, 11.9, 10.4, 9.3, 9.1, 8.5, 11]\n",
    "calories = [360, 400, 500, 370, 525, 800, 900, 1200, 1000, 1400, 1600, 850, 575, 425, 950, 800]\n",
    "kids = DataFrame({'weight': weight, 'calories': calories})\n",
    "kids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "Ngx0m8sN-dlT",
    "outputId": "7e460c55-6937-4ce2-b78d-4692c7b72c37"
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Weight and calories in 1-3 yr.old children\", \n",
    "           x_axis_label=\"weight (kg)\", y_axis_label='weekly calories')\n",
    "p.circle(kids['weight'], kids['calories'], size=10, color='navy', alpha=0.5)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfU5QT1G-dlW"
   },
   "source": [
    "And again, there appears to be a positive correlation.\n",
    "\n",
    "## The stronger the correlation the closer to a straight line\n",
    "The closer the data points are to a straight line, the higher the correlation. A rising straight line (rising going left to right) would be perfect positive correlation. Here we are comparing the heights in inches of some NHL players with their heights in cm. Obviously, those are perfectly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "SJJ5k9Iu-dlX",
    "outputId": "0bedd7c3-0b27-4ccd-941e-e9e49cbccdfa"
   },
   "outputs": [],
   "source": [
    "inches =[68, 73, 69,72,71,77]\n",
    "cm = [173, 185, 175, 183, 180, 196]\n",
    "nhlHeights = DataFrame({'heightInches': inches, 'heightCM': cm})\n",
    "nhlHeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "MWppQKt4-dla",
    "outputId": "95f0522a-5c58-46d9-9d5b-4142b8bd1be6"
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Comparison of Height in Inches and Height in CM\", \n",
    "           x_axis_label=\"Height in Inches\", \n",
    "           y_axis_label=\"Height in centimeters\")\n",
    "p.circle(nhlHeights['heightInches'], nhlHeights['heightCM'],\n",
    "         size=10, color='navy', alpha=0.5)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0vwF4yr-dld"
   },
   "source": [
    "## No correlation = far from straight line\n",
    "On the opposite extreme, if the datapoints are scattered and no line is discernable, there is no correlation.\n",
    "\n",
    "Here we are comparing length of the player's hometown name to his height in inches. We are checking whether a player whose hometown name has more letters, tends to be taller. For example, maybe someone from Medicine Hat is taller than someone from Ledue. Obviously there should be no correlation. \n",
    "\n",
    "\n",
    "(Again, a small but real dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "YYa58G0_-dle",
    "outputId": "77c3fec8-9460-4498-e965-051942a9ee56"
   },
   "outputs": [],
   "source": [
    "medicineHat = pd.read_csv('https://raw.githubusercontent.com/zacharski/machine-learning-notebooks/master/data/medicineHatTigers.csv')\n",
    "medicineHat['hometownLength'] = medicineHat['Hometown'].str.len()\n",
    "medicineHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "duMhFyDu-dlg",
    "outputId": "f1ab4fac-0f05-4b28-fc36-a630f2f27c4d"
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Correlation of the number of Letters in the Hometown to Height\", \n",
    "           x_axis_label=\"Player's Height\", y_axis_label=\"Hometown Name Length\")\n",
    "p.circle(medicineHat['Height'], medicineHat['hometownLength'], size=10, color='navy', alpha=0.5)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMDBBcyM-dlj"
   },
   "source": [
    "And that does not look at all like a straight line. \n",
    "\n",
    "## negative correlation has a line going downhill\n",
    "When the slope goes up, we say there is a positive correlation and when it goes down there is a negative correlation.\n",
    "\n",
    "#### the relationship of hair length to a person's height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "fF6MndKp-dlj",
    "outputId": "a05948ec-b2b3-48c8-97a2-672110c09e31"
   },
   "outputs": [],
   "source": [
    "height =[62, 64, 65, 68, 69, 70, 67, 65, 72, 73, 74]\n",
    "hairLength = [7, 10, 6, 4, 5, 4, 5, 8, 1, 1, 3]\n",
    "cm = [173, 185, 175, 183, 180, 196]\n",
    "people = DataFrame({'height': height, 'hairLength': hairLength})\n",
    "p = figure(title=\"Correlation of hair length to a person's height\", \n",
    "           x_axis_label=\"Person's Height\", y_axis_label=\"Hair Length\")\n",
    "p.circle(people['height'], people['hairLength'], size=10, color='navy', alpha=0.5)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdRZxka9-dlm"
   },
   "source": [
    "There is a strong negative correlation between the length of someone's hair and how tall they are. Why do you think that is?\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "# Numeric Representation of the Strength of the Correlation\n",
    "\n",
    "So far, we've seen a visual representation of the correlation, but we can also represent the degree of correlation numerically.\n",
    "\n",
    "## Pearson Correlation Coefficient\n",
    "\n",
    "This ranges from -1 to 1.\n",
    "1 is perfect positive correlation, -1 is perfect negative.\n",
    "\n",
    "$$r=\\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})}  \\sqrt{\\sum_{i=1}^n(y_i - \\bar{y})}}$$\n",
    "\n",
    "In Pandas it is very easy to compute.\n",
    "\n",
    "### Japanese ladies expenses on makeup and clothes\n",
    "Let's go back to our first example.\n",
    "First here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "sIowAZ8L-dln",
    "outputId": "164ca291-7e0f-4457-a917-919f460d1031"
   },
   "outputs": [],
   "source": [
    "monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "3t5jGS5f-dlp",
    "outputId": "dc0bd637-5a04-4389-aedf-07c1cb17b3b3"
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Montly Expenditures on Makeup and Clothes\", \n",
    "           x_axis_label=\"Money spent on makeup\", y_axis_label=\"Money spent on clothes\")\n",
    "p.circle(monthly['makeup'], monthly['clothes'], size=10, color='navy', alpha=0.5)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxUIU2lE-dls"
   },
   "source": [
    "So that looks like a pretty strong positive correlation. To compute Pearson on this data we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "WvYUkvHD-dls",
    "outputId": "e412bb80-b306-4357-fa8b-7dab2e04c333"
   },
   "outputs": [],
   "source": [
    "monthly.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cdda0-9-dlv"
   },
   "source": [
    "There is no surprise that makeup is perfectly correlated with makeup and clothes with clothes (those are the 1.000 on the diagonal). The interesting bit is that the Pearson for makeup to clothes is 0.968. That is a pretty strong correlation. \n",
    "\n",
    "If you are interesting you can compute the Pearson values for the datasets above, but let's now move to ...\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "## Regression\n",
    "\n",
    "Let's say we know a young lady who spends about  ¥22,500 per month on clothes (that's about $200/month). What do you think she spends on makeup, based on the chart below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "tGwP03jC-dlw",
    "outputId": "1fddcd70-93ad-4f09-e615-46b6c0983aaf"
   },
   "outputs": [],
   "source": [
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l89HC2LH-dlz"
   },
   "source": [
    "I'm guessing you would predict she spends somewhere around  ¥10,000 a month on makeup (almost $100/month). And how we do this is when looking at the graph  we mentally draw an imaginary straight line through the datapoints and use that line for predictions. We are performing human linear regression.  And as humans, we have the training set--the dots representing data points on the graph. and we **fit** our human classifier by mentally drawing that straight line. That straight line is our model. Once we have it, we can throw away the data points. When we want to make a prediction, we see where the money spent on clothes falls on that line. \n",
    "\n",
    "We just predicted a continuous value (money spent on makeup) from one factor (money spent on clothes).\n",
    "\n",
    "## Predicting MPG\n",
    "\n",
    "What happens when we want to predict a continuous value from 2 factors? Suppose we want to predict MPG based on the weight of a car and its horsepower.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/regression.png)\n",
    "from [Mathworks](https://www.mathworks.com/help/stats/regress.html)\n",
    "\n",
    "Now instead of a line representing the relationship we have a plane.\n",
    "\n",
    "Let's create a linear regression classifier and try this out!\n",
    "\n",
    "First, let's get the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "nS-VZrlZ-dlz",
    "outputId": "4b202843-3167-4a19-ca7f-c92e33b0f1ab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columnNames = ['mpg', 'cylinders', 'displacement', 'HP', 'weight', 'acceleration', 'year', 'origin', 'model']\n",
    "cars = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/auto-mpg.csv',\n",
    "                   na_values=['?'], names=columnNames)\n",
    "cars = cars.set_index('model')\n",
    "cars = cars.dropna()\n",
    "cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0rh87Zf-dl2"
   },
   "source": [
    "Now divide the dataset into training and testing. And let's only use the horsepower and weight columns as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "R_JBLlrW-dl2",
    "outputId": "18703952-1771-4563-a14d-ae823513169e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "cars_train, cars_test = train_test_split(cars, test_size = 0.2)\n",
    "cars_train_features = cars_train[['HP', 'weight']]\n",
    "# cars_train_features['HP'] = cars_train_features.HP.astype(float)\n",
    "cars_train_labels = cars_train['mpg']\n",
    "cars_test_features = cars_test[['HP', 'weight']]\n",
    "# cars_test_features['HP'] = cars_test_features.HP.astype(float)\n",
    "cars_test_labels = cars_test['mpg']\n",
    "\n",
    "cars_test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f62sESxd-dl6"
   },
   "source": [
    "### SKLearn Linear Regression\n",
    "\n",
    "Now let's create a Linear Regression classifier and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJ5-BzSU-dl6",
    "outputId": "c6806847-ed86-4190-9058-491d5ba566a2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linclf = LinearRegression()\n",
    "linclf.fit(cars_train_features, cars_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_VNdhI-dl9"
   },
   "source": [
    "and finally use the trained classifier to make predictions on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtNyLgia-dl9"
   },
   "outputs": [],
   "source": [
    "predictions = linclf.predict(cars_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYsJvAiR-dl_"
   },
   "source": [
    "Let's take an informal look at how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "9AnurGcX-dmC",
    "outputId": "b6359cdc-d17c-4a2a-8fd2-e2c2799ba1c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = cars_test_labels.to_frame()\n",
    "results['Predicted']= predictions\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjOo1BLN-dmF"
   },
   "source": [
    "Here is what my output looked like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/predictMPG.png)\n",
    "\n",
    "as you can see the first two predictions were pretty close as were a few others.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "### Determining how well the classifier performed\n",
    "\n",
    "With categorical classifiers we used sklearn's accuracy_score:\n",
    "\n",
    "```\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "Consider a task of predicting whether an image is of a dog or a cat. We have 10 instances in our test set. After our classifier makes predictions, for each image we have the actual (true) value, and the value our classifier predicted:\n",
    "\n",
    "actual | predicted\n",
    ":-- | :---\n",
    "dog | dog\n",
    "**dog** | **cat**\n",
    "cat | cat\n",
    "dog | dog\n",
    "cat | cat\n",
    "**cat** | **dog**\n",
    "dog | dog\n",
    "cat | cat\n",
    "cat | cat\n",
    "dog | dog\n",
    "\n",
    "sklearn's accuracy score just counts how many predicted values matched the actual values and then divides by the total number of test instances. In this case the accuracy score would be .8000. The classifier was correct 80% of the time. \n",
    "\n",
    "We can't use this method with a regression classifier. In the image above, the actual MPG of the Peugeot 304 was 30 and our classifier predicted 30.038619. Does that count as a match or not? The actual mpg of a Pontiac Sunbird Coupe was 24.5 and we predicted 25.57. Does that count as a match? Instead of accuracy_score, there are different evaluation metrics we can use. \n",
    "\n",
    "#### Mean Squared Error and Root Mean Square Error\n",
    "\n",
    "A common metric is Mean Squared Error or MSE. MSE is a measure of the quality of a regression classifier. The closer MSE is to zero, the better the classifier. Let's look at some made up data to see how this works:\n",
    "\n",
    "vehicle | Actual MPG | Predicted MPG\n",
    ":---: | ---: | ---: \n",
    "Ram Promaster 3500 | 18.0 | 20.0\n",
    "Ford F150 | 20 | 19\n",
    "Fiat 128 | 33 | 33\n",
    "\n",
    "First we compute the error (the difference between the predicted and actual values)\n",
    "\n",
    "vehicle | Actual MPG | Predicted MPG | Error\n",
    ":---: | ---: | ---: | --:\n",
    "Ram Promaster 3500 | 18.0 | 20.0 | -2\n",
    "Ford F150 | 20 | 19 | 1\n",
    "Fiat 128 | 33 | 33 | 0\n",
    "\n",
    "Next we square the error and compute the average:\n",
    "\n",
    "vehicle | Actual MPG | Predicted MPG | Error | Error^2\n",
    ":---: | ---: | ---: | --: | ---:\n",
    "Ram Promaster 3500 | 18.0 | 20.0 | -2 | 4\n",
    "Ford F150 | 20 | 19 | 1 | 1\n",
    "Fiat 128 | 33 | 33 | 0 | 0\n",
    "MSE | - | - | - | 1.667\n",
    "\n",
    "**Root Mean Squared Error** is simply the square root of MSE. The advantage of RMSE is that it has the same units as what we are trying to predict. Let's take a look ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8DX6nRJ-dmF",
    "outputId": "5247e95c-4a6e-4563-d441-aa41496d4547"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE  = mean_squared_error(cars_test_labels, predictions)\n",
    "RMSE = mean_squared_error(cars_test_labels, predictions, squared=False)\n",
    "print(\"MSE: %5.3f.   RMSE: %5.3f\" %(MSE, RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ZgOu53-dmH"
   },
   "source": [
    "That RMSE tells us on average how many mpg we were off.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ... </font> \n",
    "## <font color='#EE4C2C'>Predicting the range of electric vehicles</font> \n",
    "\n",
    "We are wondering whether we can predict the range of an EV from its battery capacity and efficiency. It seems like it might. If our battery holds 60 kWh (kilowatt hours), and it it takes 200 wh (watt hours) to go 1 kilometer, you might predict that we could go 60,000 / 200 = 300 kilometers on a charge. This is equivalent to if you know the capacity of your gas tank and how efficient (MPG) your car is you can roughly predict the range of your car. For example, if the tank capacity is 10 gallons and it gets 30 MPG, you should be able to go 300 miles.\n",
    "\n",
    "The data is in the file \n",
    "\n",
    "[https://raw.githubusercontent.com/zacharski/ml-class/master/data/ElectricCarData_w_battery.csv](https://raw.githubusercontent.com/zacharski/ml-class/master/data/ElectricCarData_w_battery.csv)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>1. Load that file into a Pandas DataFrame</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "vkuf_K0g7BRK",
    "outputId": "5bb242b6-dfb3-4fc3-a907-47f4dd580836"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHin7iEO7BZw"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Divide into training and test datasets</font> \n",
    "Now divide the dataset into training and testing. And let's only use the `Efficiency_WhKm`, `battery_kWh` columns as features and the column `Range_Km` is what we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "0oYPJC_s7uZa",
    "outputId": "c2ac1340-33b3-4138-8dad-e302e6bec299"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uG356hQ8lfs"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>3. SKLearn Linear Regression</font> \n",
    "\n",
    "Now let's create a Linear Regression classifier and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAqgum4Q8luZ",
    "outputId": "a4bfb70f-b6f8-490f-d788-7a5068b39f44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enX0lXdh80-d"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>4. Predictions</font>\n",
    "Let's use the classifier to make predictions on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFGQ31B681G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6FkH6F-89KJ"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>5. RMSE</font>\n",
    "What is the Root Mean Square Error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVZH4UDJ89e8",
    "outputId": "2c8844bd-ccf1-4d7e-f03a-a6c5ef085f13"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOQJsBB__EHB"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>6. How well does the classifier work?</font>\n",
    "Just double click this text cell and type in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTV5eu8b7Bbh"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "\n",
    "## So what kind of model does a linear regression classifier build?\n",
    "\n",
    "You probably know this if you reflect on grade school math classes you took. \n",
    "\n",
    "Let's go back and look at the young ladies expenditures on clothes and makeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "hpZ8csEm-dmI",
    "outputId": "061c6ce2-892b-4f8d-fe58-97e3042dc287",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Montly Expenditures on Makeup and Clothes\", \n",
    "           x_axis_label=\"Money spent on makeup\", y_axis_label=\"Money spent on clothes\")\n",
    "p.circle(monthly['makeup'], monthly['clothes'], size=10, color='navy', alpha=0.5)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd0Kp1b0-dmL"
   },
   "source": [
    "When we talked about this example above, I mentioned that when we do this, we imagine a line. Let's see if we can use sklearns linear regression classifier to draw that line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "iUPQmLI9-dmL",
    "outputId": "8e9f9aea-d328-4221-cc24-3b0509c3cb78"
   },
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(monthly[['clothes']], monthly['makeup'])\n",
    "pred = regr.predict(monthly[['clothes']])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot outputs\n",
    "plt.scatter(monthly['clothes'], monthly['makeup'],  color='black')\n",
    "plt.plot(monthly['clothes'], pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lwOOz55-dmO"
   },
   "source": [
    "Hopefully that matches your imaginary line!\n",
    "\n",
    "The formula for the line is\n",
    "\n",
    "$$makeup=w_0clothes + y.intercept$$\n",
    "\n",
    "We can query our classifier for those values ($w_0$, and $y.intercept$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_Qu6YBQa-dmO",
    "outputId": "5622fea3-934c-40e0-bf2d-867ec0ff3a9e"
   },
   "outputs": [],
   "source": [
    "print('w0 = %5.3f' % regr.coef_)\n",
    "print('y intercept = %5.3f' % regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W7hfaH1-dmR"
   },
   "source": [
    "So the formula for this particular example is\n",
    "\n",
    "$$ makeup = 0.479 * clothes + 121.782$$\n",
    "\n",
    "So if a young lady spent ¥22,500 on clothes we would predict she spent the following on makeup:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8H6PpL3-dmR",
    "outputId": "4bb21c34-cdb5-405e-d910-ebb70f29b025"
   },
   "outputs": [],
   "source": [
    "makeup = regr.coef_[0] * 22500 + regr.intercept_\n",
    "makeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWU_PMNW-dmT"
   },
   "source": [
    "The formula for regression in general is\n",
    "\n",
    "$$\\hat{y}=\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... \\theta_nx_n$$\n",
    "\n",
    "where $\\theta_0$ is the y intercept.  When you fit your classifier it is learning all those $\\theta$'s. That is the model your classifier learns. \n",
    "\n",
    "It is important to understand this as it applies to other classifiers as well!\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "## Overfitting\n",
    "Consider two models for our makeup predictor. One is the straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "QDX1lstU-dmU",
    "outputId": "f0fc4e4d-f1d7-493c-a99a-bfd3825faca3"
   },
   "outputs": [],
   "source": [
    "plt.scatter(monthly['clothes'], monthly['makeup'],  color='black')\n",
    "plt.plot(monthly['clothes'], pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9pH023O-dmW"
   },
   "source": [
    "And the other looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "G6yZNWN--dmX",
    "outputId": "43444bb6-1e55-4a2c-be77-2f273fd3ac73"
   },
   "outputs": [],
   "source": [
    "monthly2 = monthly.sort_values(by='clothes')\n",
    "plt.scatter(monthly2['clothes'], monthly2['makeup'],  color='black')\n",
    "plt.plot(monthly2['clothes'], monthly2['makeup'], color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txLtJJZc-dma"
   },
   "source": [
    "The second model fits the training data perfectly. Is it better than the first?  Here is what could happen.  \n",
    "\n",
    "Let's say we have been tuning our model using our validation data set. Our error rates look like \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/overfitter.png)\n",
    "\n",
    "As you can see our training error rate keeps going down, but at the very end our validation error increases. This is called **overfitting** the data. The model is highly tuned to the nuances of the training data. So much so, that it hurts the performance on new data--in this case, the validation data. This, obviously, is not a good thing.\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "#### An aside \n",
    "\n",
    "Imagine preparing for a job interview for a position you really, really, want. Since we are working on machine learning, let's say it is a machine learning job. In their job ad they list a number of things they want the candidate to know:\n",
    "\n",
    "* Convolutional Neural Networks\n",
    "* Long Short Term Memory models\n",
    "* Recurrent Neural Networks\n",
    "* Generative Deep Learning\n",
    "\n",
    "And you spend all your waking hours laser focused on these topics. You barely get any sleep and you read articles on these topics while you eat. You know the tiniest intricacies of these topics. You are more than 100% ready.\n",
    "\n",
    "The day of the interview arrives. After of easy morning of chatting with various people, you are now in a conference room for the technical interview, standing at a whiteboard, ready to hopefully wow them with your wisdom. The first question they ask is for you to write the solution to the fizz buzz problem:\n",
    "\n",
    "> Write a program that prints the numbers from 1 to 100. But for multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers which are multiples of both three and five print “FizzBuzz”\n",
    "\n",
    "And you freeze. This is a simple request and a very common interview question. In fact, to prepare you for this job interview possibility, write it now:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMdU0j3ALXgU"
   },
   "outputs": [],
   "source": [
    "def fizzbuzz():\n",
    "  print(\"TO DO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bDG7M1WGLgEj",
    "outputId": "1017545f-4b24-4bd6-f0b7-0b6b1011b7a4"
   },
   "outputs": [],
   "source": [
    "fizzbuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7Zt-eCZLgZe"
   },
   "source": [
    "Back to the job candidate freezing, this is an example of overfitting. You overfitting to the skills mentioned in the job posting.\n",
    "\n",
    "At dissertation defenses often faculty will ask the candidate questions outside of the candidate's dissertation. I heard of one case in a physics PhD defense where a faculty member asked \"Why is the sky blue?\" and the candidate couldn't answer. \n",
    "\n",
    "Anyway, back to machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "There are a number of ways to reduce the likelihood of overfitting including\n",
    "\n",
    "* We can reduce the complexity of the model. Instead of going with the model of the jagged line immediately above we can go with the simpler straight line model. We have seen this in decision trees where we limit the depth of the tree.\n",
    "\n",
    "* Another method is to increase the amount of training data.\n",
    "\n",
    "Let's examine the first. The process of reducing the complexity of a model is called regularization.\n",
    "\n",
    "The linear regression model we have just used tends to overfit the data and there are some variants that are better and these are called regularized linear models. These include\n",
    "\n",
    "* Ridge Regression\n",
    "* Lasso Regression\n",
    "* Elastic Net - a combination of Ridge and Lasso\n",
    "\n",
    "Let's explore Elastic Net. And let's use all the columns of the car mpg dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nxItuCZ-dma"
   },
   "outputs": [],
   "source": [
    "newTrain_features = cars_train.drop('mpg', axis=1)\n",
    "newTrain_labels = cars_train['mpg']\n",
    "newTest_features = cars_test.drop('mpg', axis=1)\n",
    "newTest_labels = cars_test['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6GUbt83-dmc"
   },
   "source": [
    "First, let's try with our standard Linear Regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwsP5LBo-dmd"
   },
   "outputs": [],
   "source": [
    "linclf = LinearRegression()\n",
    "linclf.fit(newTrain_features, newTrain_labels)\n",
    "predictions = linclf.predict(newTest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3iFVEwh_-dmf",
    "outputId": "61f6547e-0152-4d4f-bcb8-041593b01e4b"
   },
   "outputs": [],
   "source": [
    "MSE  = mean_squared_error(newTest_labels, predictions)\n",
    "RMSE = mean_squared_error(newTest_labels, predictions, squared=False)\n",
    "print(\"MSE: %5.3f.   RMSE: %5.3f\" %(MSE, RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7lJGcXQN1iI"
   },
   "source": [
    "Now let's try with [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0eTqpNS-dmk"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(newTrain_features, newTrain_labels)\n",
    "ePredictions = elastic_net.predict(newTest_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "PFjWu3Nx-dmm",
    "outputId": "ebb5f7d1-c936-469f-acd6-b298451bbe6c"
   },
   "outputs": [],
   "source": [
    "MSE  = mean_squared_error(newTest_labels, ePredictions)\n",
    "RMSE = mean_squared_error(newTest_labels, ePredictions, squared=False)\n",
    "print(\"MSE: %5.3f.   RMSE: %5.3f\" %(MSE, RMSE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oasWKk30OCui"
   },
   "source": [
    "I've run this a number of times. Sometimes linear regression is slightly better and sometimes ElasticNet is.  Here are the results of one run:\n",
    "\n",
    "##### RMSE\n",
    "\n",
    "Linear Regression | Elastic Net\n",
    ":---: | :---:\n",
    "2.864 | 2.812\n",
    "\n",
    "So this is not the most convincing example.\n",
    "\n",
    "However, in general, it is always better to have some regularization, so (mostly) you should avoid the generic linear regression classifier.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ... Happiness</font> \n",
    "## <font color='#EE4C2C'>Better Life Index</font> \n",
    "\n",
    "\n",
    "What better way to explore regression then to look at happiness.\n",
    "\n",
    "From a Zen perspective, happiness is being fully present in the current moment. \n",
    "\n",
    "But, ignoring that advice, let's see if we can predict happiness, or life satisfaction.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/happy2.png)\n",
    "\n",
    "We are going to be investigating the [Better Life Index](https://stats.oecd.org/index.aspx?DataSetCode=BLI). You can download a csv file of that data from that site. (under the *Export* menu select *Text File (CSV)*)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/better2.png)\n",
    "\n",
    "Now that you have the CSV data file on your laptop, you can upload it to Colab.\n",
    "\n",
    "In Colab, you will see a list of icons on the left. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/uploadFile4.png)\n",
    "\n",
    "Select the file folder icon. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/uploadFiles5b.png)\n",
    "\n",
    "Next, select the upload icon (the page with an arrow icon). And upload the file.\n",
    "\n",
    "Next, let's execute the Linux command `ls`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkxsnifT-dmp",
    "outputId": "659b57fa-5644-4d69-e0bb-c192b5198c4f"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzw8N699SrPP"
   },
   "source": [
    "You will notice that the BLI file is now uploaded to Google Colab.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>1. Load that file into a Pandas DataFrame</font> \n",
    "\n",
    "We will load the file into Pandas Dataframe called `bli` for better life index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3tYTVthdMAPO",
    "outputId": "c71ccb40-306d-4e2a-9fbd-539055a7761b"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "\n",
    "bli = \"TO DO\"\n",
    "bli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7T8rpt-YS3h"
   },
   "source": [
    "When examining the DataFrame we can see it has an interesting structure. So the first row we can parse:\n",
    "\n",
    "* The country is Australia\n",
    "* The feature is Labour market insecurity\n",
    "* The Inequality column tells us it is the **total** Labour market insecurity value.\n",
    "* The unit column tells the us the number is a percentage.\n",
    "* And the value is 3.1 (this may be different if you have a newer version of the file than the one I used)\n",
    "\n",
    "So, in English, the column is The total labor market insecurity for Australia is 5.40%.\n",
    "\n",
    "I am curious as to what values other than Total are in the Inequality column:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sdjs0SLKlE5O",
    "outputId": "ea00f0d0-9977-4ed2-f94d-e4392d22dc94"
   },
   "outputs": [],
   "source": [
    "bli.Inequality.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7VnlbDZlJnm"
   },
   "source": [
    "Cool. So in addition to the total for each feature, we can get values for just men, just women, and the high and low. \n",
    "\n",
    "Let's get just the totals and then pivot the DataFrame so it is in a more usable format. \n",
    "\n",
    "In addition, there are a lot of NaN values in the data, let's replace them with the mean value of the column.\n",
    "\n",
    "We are just learning about regression and this is a very small dataset, so let's divide training and testing by hand ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EQ2YbF3pMHhn",
    "outputId": "49e1cdc1-9a01-42ca-de84-977869bb392e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bli = bli[bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "bli = bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "bli.fillna(bli.mean(), inplace=True)\n",
    "bliTest = bli.loc['Greece':'Italy', :]\n",
    "bliTrain = pd.concat([bli.loc[:'Germany' , :], bli.loc['Japan':, :]])\n",
    "bliTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zru6GsUZRiwp"
   },
   "source": [
    "We are trying to predict Life satisfaction, where *life satisfaction* is defined as:\n",
    "\n",
    ">The indicator considers people's evaluation of their life as a whole. It is a weighted-sum of different response categories based on people's rates of their current life relative to the best and worst possible lives for them on a scale from 0 to 10, using the Cantril Ladder (known also as the \"Self-Anchoring Striving Scale\").\n",
    "“High”/ “Low” refer to values for people with tertiary/ below upper secondary education.\n",
    "\n",
    "Let's get some basic information on life satisfaction from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPqFU_I9SEP7",
    "outputId": "66f744e2-5c9d-4522-9f34-e8801a9329a1"
   },
   "outputs": [],
   "source": [
    "bli['Life satisfaction'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uwI17DVr5SM"
   },
   "source": [
    "So the indicator is a scale from 0 to 10, but the minimum satisfaction in our dataset was 4.9 and the happiest (i.e. life satisfaction) was 7.9. Let's find out which countries they were: {again you might have a newer version of the data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m75mWw4jSxgh",
    "outputId": "dd8f9eac-07a9-495a-9f9c-83207bbc6f76"
   },
   "outputs": [],
   "source": [
    "print(\"The least happy country is \" + bli['Life satisfaction'].idxmin())\n",
    "print(\"The happiest country is \" + bli['Life satisfaction'].idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0xgGLvfUn--"
   },
   "source": [
    "#### Now we need to divide both the training and test sets into features and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XnUfMadMdO1",
    "outputId": "832d488e-f7e4-4abb-d64b-2f6861157813"
   },
   "outputs": [],
   "source": [
    "feature_names   = bliTrain.columns.values.tolist()\n",
    "feature_names.remove('Life satisfaction')\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1_Oc_04Mvqc"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Dividing the datasets into features and labels</font> \n",
    "\n",
    "Go ahead and construct:\n",
    "\n",
    "* `bliTrain_features`\n",
    "* `bliTrain_labels`\n",
    "* `bliTest_features`\n",
    "* `bliTest_labels`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mqP1PbycaXTu",
    "outputId": "1ca92223-2173-412e-a247-9e877d36331b"
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeUlBUDRsHN_"
   },
   "source": [
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>3. Create and Train an elastic net model</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "LXMzYK-9moqH",
    "outputId": "7ad04a73-02d2-41bd-f80d-166dd5041a99"
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PoRRcvhNQkit",
    "outputId": "13f80734-52dc-4ec5-fe64-a50dd7d92ed1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGBEijsSsOgO"
   },
   "source": [
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>4. Use the trained model to make predictions on our tiny test set</font> \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy79gKKGnhH5"
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu2ZTJ6sxQAo"
   },
   "source": [
    "Now let's visually compare the differences between the predictions and the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "eAWEt8rNoj9A",
    "outputId": "0985916e-f8bb-455b-ece6-6e3221eb3ee2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KScOb25yRCnb"
   },
   "source": [
    "How did you do? For me Iceland was a lot happier than what was predicted.\n",
    "\n",
    "Now we will compute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0Vt06M2U0tQ",
    "outputId": "76953c67-1fa1-4647-bd15-5ad54e334240"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu5NtK7ltNrr"
   },
   "source": [
    "\n",
    "\n",
    "# Prediction Housing Prices\n",
    "\n",
    "## But first some wonkiness\n",
    "When doing one hot encoding, sometimes the original datafile has the same type of data in multiple columns. For example...\n",
    "\n",
    "Title | Genre 1 | Genre 2\n",
    " :--: | :---: | :---: \n",
    " Mission: Impossible - Fallout | Action | Drama\n",
    " Mama Mia: Here We Go Again | Comedy | Musical\n",
    " Ant-Man and The Wasp | Action | Comedy\n",
    " BlacKkKlansman | Drama | Comedy\n",
    " \n",
    " \n",
    " When we one-hot encode this we get something like\n",
    " \n",
    " Title | Genre1 Action | Genre1 Comedy | Genre1 Drama | Genre2 Drama | Genre2 Musical | Genre2 Comedy\n",
    "  :--: | :--: | :--: | :--: | :--: | :--: | :--: \n",
    "  Mission: Impossible - Fallout | 1 | 0 | 0 | 1 | 0 | 0\n",
    "  Mama Mia: Here We Go Again  | 0 | 1 | 0 | 0 | 1 | 0\n",
    "  Ant-Man and The Wasp | 1 | 0 | 0 | 0 | 0 | 1\n",
    "  BlacKkKlansman | 0 | 0 | 1 | 0 | 0 | 1\n",
    "  \n",
    "  But this isn't what we probably want. Instead this would be a better representation:\n",
    "  \n",
    "  Title | Action | Comedy | Drama | Musical\n",
    "  :---: | :---: | :---: |  :---: | :---: | \n",
    "  Mission: Impossible - Fallout | 1 | 0 | 1 | 0\n",
    "  Mama Mia: Here We Go Again  | 0 | 1 | 0 | 1\n",
    "  Ant-Man and The Wasp | 1 | 1 | 0 | 0\n",
    "  BlacKkKlansman | 0 | 1 | 1 | 0\n",
    "  \n",
    "  Let's see how we might do this in code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "G_Y7mmEKo9Fg",
    "outputId": "78125e7b-324f-4b8c-f71c-0f85277220ba"
   },
   "outputs": [],
   "source": [
    "df   = pd.DataFrame({'Title': ['Mission: Impossible - Fallout', 'Mama Mia: Here We Go Again', \n",
    "                               'Ant-Man and The Wasp', 'BlacKkKlansman' ],\n",
    "                    'Genre1': ['Action', 'Comedy', 'Action', 'Drama'],\n",
    "                    'Genre2': ['Drama', 'Musical', 'Comedy', 'Comedy']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDyEYXhHpEo7"
   },
   "outputs": [],
   "source": [
    "one_hot_1 = pd.get_dummies(df['Genre1'])\n",
    "one_hot_2 = pd.get_dummies(df['Genre2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "aObvufy0vYwZ",
    "outputId": "d81366fd-a64c-4b7a-95b1-952571258687"
   },
   "outputs": [],
   "source": [
    "# now get the intersection of the column names\n",
    "s1 = set(one_hot_1.columns.values)\n",
    "s2 = set(one_hot_2.columns.values)\n",
    "intersect = s1 & s2\n",
    "only_s1 = s1 - intersect\n",
    "only_s2 = s2 - intersect\n",
    "# now logically or the intersect\n",
    "logical_or = one_hot_1[list(intersect)] | one_hot_2[list(intersect)]\n",
    "# then combine everything\n",
    "combined = pd.concat([one_hot_1[list(only_s1)], logical_or, one_hot_2[list(only_s2)]], axis=1)\n",
    "combined\n",
    "\n",
    "### Now drop the two original columns and add the one hot encoded columns\n",
    "df= df.drop('Genre1', axis=1)\n",
    "df= df.drop('Genre2', axis=1)\n",
    "df = df.join(combined)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxM-Rw_ZwCMU"
   },
   "source": [
    "That looks more like it!!!\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "# <font color='#EE4C2C'>The task: Predict Housing Prices</font> \n",
    "\n",
    "\n",
    "\n",
    "Your task is to create a regession classifier that predicts house prices. The data and a description of\n",
    "\n",
    "* [The description of the data](https://raw.githubusercontent.com/zacharski/ml-class/master/data/housePrices/data_description.txt)\n",
    "* [The CSV file](https://raw.githubusercontent.com/zacharski/ml-class/master/data/housePrices/data.csv)\n",
    "\n",
    "\n",
    "Minimally, your classifier should be trained on the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RfTLx11vicC"
   },
   "outputs": [],
   "source": [
    "numericColumns = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n",
    "                 'FullBath', 'HalfBath', 'Bedroom', 'Kitchen']\n",
    "categoryColumns = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', \n",
    "                   'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'BldgType', \n",
    "                   'HouseStyle', 'RoofStyle', 'RoofMatl', '' ]\n",
    "\n",
    "# Using multicolumns is optional\n",
    "multicolumns = [['Condition1', 'Condition2'], ['Exterior1st', 'Exterior2nd']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWcqhPixyfZI"
   },
   "source": [
    "You are free to use more columns than these. Also, you may need to process some of the columns. \n",
    " Here are the requirements:\n",
    " \n",
    " ### 1. Drop any data rows that contain Nan in a column. \n",
    " Once you do this you should have around 1200 rows.\n",
    " ### 2. Use the following train_test_split parameters\n",
    "  ```\n",
    "train_test_split( originalData, test_size=0.20, random_state=42)\n",
    "```\n",
    "\n",
    " ### 3. You are to compare Linear Regression and Elastic Net\n",
    " ### 4. You should use 10 fold cross validation (it is fine to use grid search)\n",
    " ### 5. When finished tuning your model, determine the accuracy on the test data using RMSE.\n",
    "\n",
    "# Performance Bonus\n",
    "You are free to adjust any hyperparameters but do so before you evaluate the test data. You may get up to 15xp bonus for improved accuracy. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBCp4tvCyCDr"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY9EdczzwAxT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nzw8N699SrPP",
    "GeUlBUDRsHN_",
    "YGBEijsSsOgO"
   ],
   "name": "Working version of regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
