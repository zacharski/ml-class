{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvk_0biOrDxJ"
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/decisionTreez2.png)\n",
    "\n",
    "#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n",
    "\n",
    "\n",
    "In this notebook we are going to look at decision trees. Decision trees are robust machine learning algorithms in their own right, and also are the core component of random forest techniques and XGBoost.\n",
    "\n",
    "Let's get some important stuff out of the way. Here are the advantages and disadvantages of decision trees:\n",
    "\n",
    "### advantages\n",
    "\n",
    "1. When it classifies an instance you can ask *why*. Most machine learning techniques are called **black box models** which means that when it makes a decision you can't go inside and see how that decision was made.  For example, consider a task where we have 100 attributes and we are trying to predict whether someone has a particular kind of cancer. With an algorithm like Naive Bayes, which relies on probabilities, we can't ask \"How did you come to the conclusion that this person will have cancer?\" In contrast, decision trees can provide an answer to that question. \n",
    "2. It requires minimal data wrangling. We don't need to scale columns. It can handle missing data (although, not with the sklearn implementation)\n",
    "4. Once trained, it is fairly fast at making classifications. The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "\n",
    "### disadvantages\n",
    "\n",
    "1. Susceptible to overfitting the data. The training set is only a sample from some huge real world set (for example, even a training set of 1000 people who do and do not have diabetes, is tiny compared to all the people who do or do not have diabetes). The decision tree algorithm may add branches to the tree that improve performance for the instances in the dataset but decrease performance on the instances outside the dataset. This is the big wart on decision trees.\n",
    "3. Learning the optimal decision tree is NP-complete. What this means is that the algorithm can create a pretty good classifier in a reasonable amount of time, but to create the very best classifier would take more time than there are grains of sand in the Ganges.\n",
    "\n",
    "\n",
    "Let's get to a simple example. \n",
    "\n",
    "## Iris Dataset yet again\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/irises.png))\n",
    "\n",
    "Here is a very simple basic example. We are going to use the Iris Dataset we used before.  As a reminder, the data set contains 3 classes of instances:\n",
    "\n",
    "1. Iris Setosa \n",
    "2. Iris Versicolour \n",
    "3. Iris Virginica (the picture above)\n",
    "\n",
    "There are only 4 attributes or features:\n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "Here is an example of the data:\n",
    "\n",
    "Sepal Length|Sepal Width|Petal Length|Petal Width|Class\n",
    ":--: | :--: |:--: |:--: |:--: \n",
    "5.3|3.7|1.5|0.2|Iris-setosa\n",
    "5.0|3.3|1.4|0.2|Iris-setosa\n",
    "5.0|2.0|3.5|1.0|Iris-versicolor\n",
    "5.9|3.0|4.2|1.5|Iris-versicolor\n",
    "6.3|3.4|5.6|2.4|Iris-virginica\n",
    "6.4|3.1|5.5|1.8|Iris-virginica\n",
    "\n",
    "The job of the classifier is to determine the class of an instance from the values of the attributes.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>Your Work ...</font> \n",
    "Just for practice you can help do the following...\n",
    "\n",
    "## <font color='#EE4C2C'>1. Load the data</font> \n",
    "\n",
    "\n",
    "\n",
    "This dataset is so common that it is build into sklearn, but just for practice let's load our own csv file from\n",
    "\n",
    "    https://raw.githubusercontent.com/zacharski/ml-class/master/data/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ODOlog2trDxM",
    "outputId": "55f8aa75-d439-4787-e963-9cb3f8583fae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPtte7w4LybQ"
   },
   "source": [
    "## <font color='#EE4C2C'>2. Divide into features and labels</font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "XbIStIHIL4uy",
    "outputId": "77424c6d-5289-4911-f6a6-18a6d26b800e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdWlGxc2rDxT"
   },
   "source": [
    "## <font color='#EE4C2C'>3. Splitting data into a training set and test set</font> \n",
    "\n",
    "\n",
    "\n",
    "### Splitting data into a training set and test set\n",
    "\n",
    "Let's say we want to 80% of the data in the training set and 20% in the test set. And let us set a random state of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bin2aoLZrDxV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGnU9ldarDxd"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "# Decision trees\n",
    "### Step 1: Create a decision tree classifier\n",
    "Now it is time to create a decision tree classifier using entropy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFoXZc-WrDxe"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkqLmPUtrDxl"
   },
   "source": [
    "### Step 2: Train the Classifier on the Training Data:\n",
    "(the arguments are the same as the knn classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGkygkC1rDxm",
    "outputId": "24b4f017-6380-458a-b89c-eeaf56c2fde0"
   },
   "outputs": [],
   "source": [
    "clf.fit(iris_features_train, iris_label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKIU1A92rDxv"
   },
   "source": [
    "Parsing\n",
    "\n",
    "```\n",
    "clf.fit(iris_features_train, iris_label_train)\n",
    "```\n",
    "\n",
    "`clf` is our decision tree classifier. `fit` means to train the classifier on the dataset. `iris_features_train` is the DataFrame containing all the feature columns of the training set and `iris_label_train` are the labels of the training set.\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "### Viewing our decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "_VyZ_T6UrDxw",
    "outputId": "d9bea110-0c89-416e-e75f-102cad30da5b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "dot_data = tree.export_graphviz(clf, out_file=\"iris.dot\", \n",
    "                         feature_names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width'],  \n",
    "                         class_names=['setosa', 'versicolor', 'virginica'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graphviz.graph_from_dot_file(\"iris.dot\")\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAfF82KOrDx2"
   },
   "source": [
    "Ok. that is cool.\n",
    "\n",
    "When I did this the first few nodes of the tree looked like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/tree3.png)\n",
    "\n",
    "Let's parse that out a bit. The first node looks like:\n",
    "\n",
    "```\n",
    "Petal Length <= 2.45\n",
    "entropy = 1.583\n",
    "samples=120\n",
    "values = [37, 41, 42]\n",
    "class = virginica\n",
    "```\n",
    "Let's skip the first line for a moment and look at the lines:\n",
    "\n",
    "```\n",
    "samples=120\n",
    "values = [37, 41, 42]\n",
    "```\n",
    "The number of samples is 120. 37 of them are setosa, 41 are versicolor, and 42 are virginica. So if this is all the information you had to guess with, you would select virginica since most of the samples are virginica. That is how we get\n",
    "\n",
    "```\n",
    "class = virginica\n",
    "```\n",
    "From the samples and values lines we can calculate entropy, or info:\n",
    "\n",
    "$$\n",
    "info([37, 41, 42])=-\\sum{37\\over120}log_2{({37\\over120})}+{41\\over120}log_2{({41\\over120})}+{42\\over120}log_2{({42\\over120})} = 1.583\n",
    "$$\n",
    "\n",
    "So the entropy of the node is 1.583.  \n",
    "\n",
    "To recap, we examined all the lines of\n",
    "\n",
    "\n",
    "```\n",
    "Petal Length <= 2.45\n",
    "entropy = 1.583\n",
    "samples=120\n",
    "values = [37, 41, 42]\n",
    "class = virginica\n",
    "```\n",
    "except the first.\n",
    "\n",
    "The first line contains the first question: *Is the petal length smaller or equal to 2.45*. If that is true we follow the left link to the node:\n",
    "\n",
    "```\n",
    "entropy=0.0\n",
    "samples=37\n",
    "values=[37,0,0]\n",
    "class = setosa\n",
    "```\n",
    "\n",
    "It turns out that 37 of the samples have a petal length less than or equal to 2.45 and all of them are setosa. If we get a new sample and its petal length is 2.37, we would classify it as setosa. \n",
    "Let's examine the node on the right:\n",
    "\n",
    "```\n",
    "Petal Length <= 1.75\n",
    "entropy= 1.0\n",
    "samples= 83\n",
    "values=[0, 41, 42]\n",
    "class = virginica\n",
    "```\n",
    "It turns out that 83 of the samples have a petal length greater than 2.45 (we are following the 'no' link from the root node) \n",
    "\n",
    "Look over the nodes of the tree you constructed to gain a good understanding of what a decision tree looks like.\n",
    "\n",
    "That was a bit of an diversion (although, an important one).\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "## A Recap\n",
    "#### step 1. we created a classifier\n",
    "\n",
    "```\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "```\n",
    "#### step 2. we trained the classifier on the data\n",
    "```\n",
    "clf.fit(iris_features_train, iris_label_train)\n",
    "```\n",
    "We are fitting a classifier that uses the columns Sepal Length, Sepal Width, Petal Length, and Petal Width to predict Class.\n",
    "\n",
    "#### step 3. use the classifier to classifier instances from out test set.\n",
    "Once we fit our classifier to the data (i.e. we trained it) we can use the classifier to classifier samples. let's try it on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmMeUs-NrDx3",
    "outputId": "a4f51f0e-1e6b-4789-da2e-6d39931c402e"
   },
   "outputs": [],
   "source": [
    "iris_predictions = clf.predict(iris_features_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(iris_label_test, iris_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnO-IXN2rDyA"
   },
   "source": [
    "Great! You should get 100% or near 100% accuracy on that small dataset.\n",
    "\n",
    "### Adjusting Max Depth\n",
    "\n",
    "The depth of a decision tree is how many rows of nodes there are excluding the root node. In the case of the tree above, the depth was likely to be 5. \n",
    "\n",
    "We can limit depth by using the `max_depth` parameter when we construct our DecisionTreeClassifier.\n",
    "\n",
    "Let's try it with a max depth of 4 (notice in the first line of code we added `max_depth=4`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cldD6J3rDyC",
    "outputId": "776c4be3-a06f-4c14-9c50-4896dd6155e0"
   },
   "outputs": [],
   "source": [
    "clf_maxdepth4 = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
    "clf_maxdepth4.fit(iris_features_train, iris_label_train)\n",
    "iris_predictions = clf_maxdepth4.predict(iris_features_test)\n",
    "accuracy_score(iris_label_test, iris_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "aX1xrzL5rDyJ",
    "outputId": "7bfe52f7-033f-4851-89af-57359a0781fa"
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf_maxdepth4, out_file=\"iris4.dot\", \n",
    "                         feature_names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width'],  \n",
    "                         class_names=['setosa', 'versicolor', 'virginica'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graphviz.graph_from_dot_file(\"iris4.dot\")\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GH37ciTlrDyV"
   },
   "source": [
    "I already mentioned that a negative of decision trees are that they may overfit the training data.  If we allow for unbounded depth the risk of overfitting greatly increases. We can reduce the risk of overfitting by limiting the depth as we just did.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font> \n",
    "\n",
    "## <font color='#EE4C2C'>1. Pima Indian Diabetes Dataset</font> \n",
    "\n",
    "\n",
    "The Pima Indians Diabetes Data Set was developed by the\n",
    "United States National Institute of Diabetes and Digestive and Kidney Diseases. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/6/65/Pinal_County_Arizona_Incorporated_and_Unincorporated_areas_GRIC_highlighted.svg)\n",
    "\n",
    "Astonishingly, over 30% of Pima people develop diabetes. In contrast, the diabetes rate in\n",
    "the United States is 8.3% and in China it is 4.2%.\n",
    "\n",
    "Each instance in the dataset represents information about a Pima woman over the age of 21\n",
    "and belonged to one of two classes: a person who developed diabetes within five years, or a\n",
    "person that did not. There are eight attributes in addition to the column representing whether or not they developed diabetes:\n",
    "\n",
    "\n",
    "1.  The number of times the woman was pregnant\n",
    "2.  Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3.  Diastolic blood pressure (mm Hg)\n",
    "4.  Triceps skin fold thickness (mm) \n",
    "5.  2-Hour serum insulin (mu U/ml) \n",
    "6.  Body mass index (weight in kg/(height in m)^2) \n",
    "7.  Diabetes pedigree function \n",
    "8.  Age\n",
    "9.  Whether they got diabetes or not (0 = no, 1 = yes)\n",
    "\n",
    "We are trying to predict whether they got diabetes or not based on the  features.\n",
    "\n",
    "The csv file at  is at\n",
    "\n",
    "    https://raw.githubusercontent.com/yew1eb/machine-learning/master/Naive-bayes/pima-indians-diabetes.data.csv\n",
    "    \n",
    "<span style=\"color:red\">This file does not have a header row</span>\n",
    "\n",
    "You will need to \n",
    "\n",
    "1. load the file into a dataframe\n",
    "2. divide the data into training and test sets. (an 80-20 split sounds good)\n",
    "3. train a decision tree classifier on the training data \n",
    "3. display the tree\n",
    "3. run the classifier on the test data\n",
    "4. compute the accuracy\n",
    "5. Have a small paragraph describing the results.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0gb3RfhrDyX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP1aDZfkrDyf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p53iBa8MrDym"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twXdcNGgrSYq"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. The Wisconsin Cancer Datasett</font> \n",
    "\n",
    "The task is to predict whether a tumor is malignant or benign (the second column of the  dataset based on 30 real values. \n",
    "\n",
    "The data file is\n",
    "\n",
    "\n",
    "https://raw.githubusercontent.com/zacharski/ml-class/master/data/wdbc.data\n",
    "\n",
    "\n",
    "And a writeup about the data is at:\n",
    "\n",
    "https://raw.githubusercontent.com/zacharski/ml-class/master/data/wdbc.names\n",
    "\n",
    "Follow the same steps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBT9FXTYrDys"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Working version of decision_trees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
