{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xgboostLab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L5KyC6KP_yY",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost Lab\n",
        "\n",
        "## Reflections\n",
        "Let's go back to thinking about a few algorithms we worked on.\n",
        "\n",
        "\n",
        "### Decisions trees\n",
        "We began our exploration of decision trees with a mountain bike example:\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dtree77.png)\n",
        "\n",
        "Here's is roughly what we did by hand.\n",
        "\n",
        "1. We determined that if we couldn't ask any questions, we would say the person mountain biked since they mountained bike 9 times and didn't 5 times. So our error rate was 5 out of 14 or roughly 36%\n",
        "2. Next, if we could ask one question we determined that the question should be about Outlook. Now our error rate was 4 out of 14 or 29%\n",
        "3. Then we determined the next question to ask and reduced the error rate more. And then the next question ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In some sense, the algorithm is additive. We start with zero questions with whatever error rate. Add a question and reduce the error rate. Add another question and reduce the rate. And so on.\n",
        "\n",
        "**Additive** is the key word. Let's look at an example, from [Gradient boosting: Distance to target](https://explained.ai/gradient-boosting/L2-loss.html)  by Terence Parr and Jeremy Howard. They ask us to imagine writing the formula for *y* that matches this plot:\n",
        "\n",
        "![](https://explained.ai/gradient-boosting/images/L2-loss/L2-loss_additive_2.svg)\n",
        "\n",
        "Like the decision tree example above, our first approximation might be simple, perhaps just the y-intercept:\n",
        "\n",
        "$$y = 30$$\n",
        "\n",
        "as shown in the leftmost picture below. \n",
        "\n",
        "![](https://explained.ai/gradient-boosting/images/L2-loss/L2-loss_additive_3.svg)\n",
        "\n",
        "Next, we may want to add in the slope of the line and get\n",
        "\n",
        "$$y = 30 + x$$\n",
        "\n",
        "and get the middle graph above.  Finally, we add in the squiggle:\n",
        "\n",
        "$$y = 30 + x + sin(x)$$\n",
        "\n",
        "We have decomposed a complex task into subtasks, each refining the previous approximation. So, again, we have an additive algorithm.\n",
        "\n",
        "This approach shouldn't be surprising to us since this is how we typically develop programs. We get some skeleton code working and then incrementally add to it.\n",
        "\n",
        "\n",
        "## Boosting\n",
        "\n",
        "Boosting algorithms work in a similar additive fashion. We first develop a simple model that roughly classifies the data. Next, we add another simple model that is focused on ameliorating the errors of the first. And then we add another and another.\n",
        "\n",
        "$$boosting=model_1 + model_2 + model_3 + ... + model_n$$\n",
        "\n",
        "\n",
        "### How boosting differs from bagging and pasting\n",
        "\n",
        "With bagging and pasting we created a number of decision trees each of which was trained on different data. **One tree did not influence the construction of another.** Thus, each classifier was independent of the others.\n",
        " \n",
        "\n",
        "#### Boosting\n",
        "Boosting is different. \n",
        "\n",
        "Imagine that we create one decision tree classifier. Let's call it Classifier 1. Classifier 1 doesn't perform with 100% accuracy. \n",
        "\n",
        "Next we create a second decision tree classifier and as part of its training data we will use the instances that Classifier got wrong. Now Classifier 2 isn't perfect either and there will be some instances that both Classifier 1 and Classifier 2 got wrong, and, you guessed it, we will use those instances as part of the training data for Classifier 3.\n",
        "\n",
        "#### 400 Classifiers\n",
        "Suppose we created 400 classifiers using the bagging algorithm. Since each classifier is independent of the others, we can run those 400 in parallel. \n",
        "\n",
        "Now think about boosting for a moment. Can we run those in parallel? Think about it for \n",
        "\n",
        "1. second\n",
        "2. seconds\n",
        "3. seconds\n",
        "4. seconds\n",
        "5. seconds\n",
        "\n",
        "\n",
        "\n",
        "Since one classifier is dependent on the errors of the others it seems like we couldn't run them in parallel and training 400 classifiers  sequentially seems impractical. This is true in general with boosting algorithms but as we will see XGBoost is different.\n",
        "\n",
        "### Gradient Boosting\n",
        "Suppose I am interested in taking my camper van \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/travato2.png)\n",
        "\n",
        "\n",
        "to White Horse Road Dispersed Camping in Utah.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/wildHorse.png)\n",
        "\n",
        "And to get there from my home in Santa Fe, I am using an old school paper map.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/map.png)\n",
        "\n",
        "A route will be something like.\n",
        "\n",
        "$$route = road_0 + road_1 + road_2 + ... + road_n$$\n",
        "\n",
        "To get to White Horse Road, it looks like my best bet is to start by taking I25 to Albuquerque.\n",
        "\n",
        "\n",
        "$$route = i25 $$\n",
        "\n",
        "Now the difference between where I am and where I want to go is Albuquerque to White Horse. So I performed an action and now my new problem is dealing with this new problem of getting from Albuquerque to White Horse\n",
        "\n",
        "From Albuquerque I can take 550 to Farmington\n",
        "\n",
        "$$route = i25  + US550$$\n",
        "\n",
        "and from there take 491 to Monticello Utah\n",
        "\n",
        "\n",
        "$$route = i25  + US550+ US491$$\n",
        "\n",
        "and so on.\n",
        "\n",
        "There are some similarities between this old school mapping and gradient boosting. In gradient boosting we start with a poor model (in our case, we decided to go to Albuquerque). Then we are going to look at the difference between what we want and where we are-- and then take the next step, the delta $\\Delta$. \n",
        "\n",
        "\n",
        "Let's look at a simple example of classification of one feature *x* to predict a label *y*. We will label our prediction $\\hat{y}$. For gradient boosting our formula is\n",
        "\n",
        "$$\\hat{y}=f_0(x) + \\Delta_1(x) + \\Delta_2(x) + ... + \\Delta_m(x)$$\n",
        "\n",
        "Where $\\Delta_1$ is the first improvement, $\\Delta_2$ the second and so on.\n",
        "\n",
        "Gradient Boosting is an ensemble method, meaning that it is built with a number of sub-classifiers. So perhaps a better Utah analogy is that I hitchhike from here to Albuquerque with one person (one 'classifier'), then go to Framington with another and so on.\n",
        "\n",
        "\n",
        "This is the rough intuition of gradient boosting. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In any gradient algorithm there is a parameter called *learning rate* and in a sense it is how big of steps we can take. \n",
        "\n",
        "Suppose we are hiking on a mountain in Utah and suddenly we are fogged in and can't see a thing. We want to get back to our van in the valley.\n",
        "\n",
        "In my 2D Utah it looks like this:\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/gradient1.png)\n",
        "\n",
        "The purple dot is us near the top of the mountain and the burnt orange dot is our van. So our algorithm is\n",
        "\n",
        "\n",
        "```\n",
        "WHILE NOT AT VAN OR NOT MOVING:\n",
        "  take one step to the left.\n",
        "  IF we are lower than when we started:\n",
        "     stay here at the new location\n",
        "  ELSE\n",
        "     go back to starting point and go one step to the right\n",
        "     IF we are lower than when we started:\n",
        "        stay here at the new location\n",
        "     ELSE\n",
        "        go back to starting point\n",
        "```\n",
        "\n",
        "We repeat the above procedure and get to the state shown on the right above. If we take a step to the right or left we go uphill so we are stuck. We hit what is called a local minima and local minima are a problem with all gradient descent algorithms.\n",
        "\n",
        "Perhaps the one step was too small an increment. So let's say we have a rope. You stay where we are and hold one end of the rope and I walk until I reach the end of the rope. Based on the angle of the rope, we see if I am lower or not and we move accordingly. Now we jump over that local minima and reach a state that looks like the following image on the left. We don't know it, but we are almost to the van!\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/gradient2.png)\n",
        "\n",
        "We use the rope technique again but this time I jump over the location of our van since I am not at the end of the rope yet and am in the position shown on the right. The learning rate was too large. (Now I am sounding like the three bears tale!)\n",
        "\n",
        "The one step was our learning rate as was our rope technique and you can see that selecting a good one is crucial. \n",
        "\n",
        "#### Loss Function\n",
        "For both these examples, one thing we needed was a measure for how far away are we from our goal. Are we better or worse? For the fog on a mountain example, the loss function was our altitude and we are trying to reduce the loss -- the altitude. \n",
        "\n",
        "\n",
        "### Two more examples\n",
        "\n",
        "#### One Dimensional Team Frisbee Golf\n",
        "Here is my representation of our 1D golf game. The hole is the green circle on the right and our frisbee's location is shown with the lovely pink circle on the left. Let $y$ be the actual distance between the two and $x$ what I see standing by the frisbee--off in the one dimensional distance I see the hole. \n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf1.png)\n",
        "\n",
        "\n",
        "It is player zero's turn and she estimates the distance to be 70 yards.\n",
        "\n",
        "$$f_0(x) = 70$$ \n",
        "\n",
        "She flings the frisbee and ...\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf2.png)\n",
        "\n",
        "Now it is player two's turn. He is only concerned with the  difference, the $\\Delta_1$ --the current position of the frisbee and the location of the hole. He estimates it to be 20 yards\n",
        "\n",
        "$$\\Delta_1(x) = 20$$\n",
        "\n",
        "So far we have flung the frisbee\n",
        "\n",
        "$$\\hat{y}= f+0(x) + \\Delta_1(x) = 70 + 20 = 90$$\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf3.png)\n",
        "\n",
        "Now it is player two's turn. She estimates the distance remaining ($\\Delta_2$) to be 15 yards...\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf4.png)\n",
        "\n",
        "And she overshot. \n",
        "\n",
        "Player three estimates the remaining distance to be -5 yards and ...\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/golf5.png)\n",
        "\n",
        "Notice that each player is not concerned with the original problem. She is just concerned with the **residual** --- meaning what is remaining based on the previous players' results.\n",
        "\n",
        "The formula is \n",
        "\n",
        "$$\\hat{y} = f_0(x) + \\Delta_1(x) + \\Delta_2(x) + ... + \\Delta_m(x)$$\n",
        "$$=f_0(x) + \\sum_{m=1}^M{\\Delta_m(x)}$$\n",
        "\n",
        "So the first classifier works on the original problem but all the rest work on the residual.\n",
        "\n",
        "#### Expenditures on Makeup and Clothes\n",
        "Ok, I have exhausted my creativity, so even though I am not keen on this example, let's go back to predicting a young lady's expenditure on makeup based on what she spends on clothes.  And just for readability I am going to make the feature clothes to be represented by *x* and what we want to predict, the makeup, *y*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3c2iMbCP9aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3ce5a9e6-5028-482b-9c09-e74fad433992"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "makeup =  [3000, 5000, 12000, 2000, 7000,  15000,  5000,  6000, 8000,  10000]\n",
        "clothes = [7000, 8000, 25000, 5000, 12000, 30000, 10000, 15000, 20000, 18000]\n",
        "ladies = ['Ms A','Ms B','Ms C','Ms D','Ms E','Ms F','Ms G','Ms H','Ms I','Ms J',]\n",
        "monthly = DataFrame({'x': clothes, 'y': makeup}, index= ladies)\n",
        "monthly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ms A</th>\n",
              "      <td>7000</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms B</th>\n",
              "      <td>8000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms C</th>\n",
              "      <td>25000</td>\n",
              "      <td>12000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms D</th>\n",
              "      <td>5000</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms E</th>\n",
              "      <td>12000</td>\n",
              "      <td>7000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms F</th>\n",
              "      <td>30000</td>\n",
              "      <td>15000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms G</th>\n",
              "      <td>10000</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms H</th>\n",
              "      <td>15000</td>\n",
              "      <td>6000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms I</th>\n",
              "      <td>20000</td>\n",
              "      <td>8000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms J</th>\n",
              "      <td>18000</td>\n",
              "      <td>10000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x      y\n",
              "Ms A   7000   3000\n",
              "Ms B   8000   5000\n",
              "Ms C  25000  12000\n",
              "Ms D   5000   2000\n",
              "Ms E  12000   7000\n",
              "Ms F  30000  15000\n",
              "Ms G  10000   5000\n",
              "Ms H  15000   6000\n",
              "Ms I  20000   8000\n",
              "Ms J  18000  10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8hWOON-0qEU",
        "colab_type": "text"
      },
      "source": [
        "And for our first prediction $f_0$ let's predict just the average value:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddG9_pVPTwDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4785b730-d243-469f-f83c-5343ad2a723b"
      },
      "source": [
        "monthly['f0'] = monthly.y.mean()\n",
        "monthly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>f0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ms A</th>\n",
              "      <td>7000</td>\n",
              "      <td>3000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms B</th>\n",
              "      <td>8000</td>\n",
              "      <td>5000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms C</th>\n",
              "      <td>25000</td>\n",
              "      <td>12000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms D</th>\n",
              "      <td>5000</td>\n",
              "      <td>2000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms E</th>\n",
              "      <td>12000</td>\n",
              "      <td>7000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms F</th>\n",
              "      <td>30000</td>\n",
              "      <td>15000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms G</th>\n",
              "      <td>10000</td>\n",
              "      <td>5000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms H</th>\n",
              "      <td>15000</td>\n",
              "      <td>6000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms I</th>\n",
              "      <td>20000</td>\n",
              "      <td>8000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms J</th>\n",
              "      <td>18000</td>\n",
              "      <td>10000</td>\n",
              "      <td>7300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x      y      f0\n",
              "Ms A   7000   3000  7300.0\n",
              "Ms B   8000   5000  7300.0\n",
              "Ms C  25000  12000  7300.0\n",
              "Ms D   5000   2000  7300.0\n",
              "Ms E  12000   7000  7300.0\n",
              "Ms F  30000  15000  7300.0\n",
              "Ms G  10000   5000  7300.0\n",
              "Ms H  15000   6000  7300.0\n",
              "Ms I  20000   8000  7300.0\n",
              "Ms J  18000  10000  7300.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijAYdpZd2fzx",
        "colab_type": "text"
      },
      "source": [
        "and the differences between our predictions and the actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKG3OvcB1RWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "df6b2101-e01b-46e9-a87b-f3969382014d"
      },
      "source": [
        "monthly['y-f0'] = monthly.y - monthly.f0\n",
        "monthly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>f0</th>\n",
              "      <th>y-f0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ms A</th>\n",
              "      <td>7000</td>\n",
              "      <td>3000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-4300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms B</th>\n",
              "      <td>8000</td>\n",
              "      <td>5000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-2300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms C</th>\n",
              "      <td>25000</td>\n",
              "      <td>12000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>4700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms D</th>\n",
              "      <td>5000</td>\n",
              "      <td>2000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-5300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms E</th>\n",
              "      <td>12000</td>\n",
              "      <td>7000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms F</th>\n",
              "      <td>30000</td>\n",
              "      <td>15000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>7700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms G</th>\n",
              "      <td>10000</td>\n",
              "      <td>5000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-2300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms H</th>\n",
              "      <td>15000</td>\n",
              "      <td>6000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-1300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms I</th>\n",
              "      <td>20000</td>\n",
              "      <td>8000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms J</th>\n",
              "      <td>18000</td>\n",
              "      <td>10000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>2700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x      y      f0    y-f0\n",
              "Ms A   7000   3000  7300.0 -4300.0\n",
              "Ms B   8000   5000  7300.0 -2300.0\n",
              "Ms C  25000  12000  7300.0  4700.0\n",
              "Ms D   5000   2000  7300.0 -5300.0\n",
              "Ms E  12000   7000  7300.0  -300.0\n",
              "Ms F  30000  15000  7300.0  7700.0\n",
              "Ms G  10000   5000  7300.0 -2300.0\n",
              "Ms H  15000   6000  7300.0 -1300.0\n",
              "Ms I  20000   8000  7300.0   700.0\n",
              "Ms J  18000  10000  7300.0  2700.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYYABnmp2niq",
        "colab_type": "text"
      },
      "source": [
        "That $y-f_0$ is the residual. What is left, or how far the first classifier was off. The residual is what the second classifier is trying to predict.\n",
        "\n",
        "Next, we are going to create a classifier Δ1 that predicts $y - f_0$ from the x. Let's say my next classifier has the wacky\n",
        "\n",
        "$$(x - 10000)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVqyVkYF2XSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "881981c7-1bbf-477e-c139-2ebe8e2b315d"
      },
      "source": [
        "monthly['Δ1'] = (monthly['x'] - 10000)\n",
        "monthly['y-f1'] = monthly['y-f0'] - monthly['Δ1']\n",
        "monthly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>f0</th>\n",
              "      <th>y-f0</th>\n",
              "      <th>Δ1</th>\n",
              "      <th>y-f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ms A</th>\n",
              "      <td>7000</td>\n",
              "      <td>3000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-4300.0</td>\n",
              "      <td>-3000</td>\n",
              "      <td>-1300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms B</th>\n",
              "      <td>8000</td>\n",
              "      <td>5000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-2300.0</td>\n",
              "      <td>-2000</td>\n",
              "      <td>-300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms C</th>\n",
              "      <td>25000</td>\n",
              "      <td>12000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>4700.0</td>\n",
              "      <td>15000</td>\n",
              "      <td>-10300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms D</th>\n",
              "      <td>5000</td>\n",
              "      <td>2000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-5300.0</td>\n",
              "      <td>-5000</td>\n",
              "      <td>-300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms E</th>\n",
              "      <td>12000</td>\n",
              "      <td>7000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-300.0</td>\n",
              "      <td>2000</td>\n",
              "      <td>-2300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms F</th>\n",
              "      <td>30000</td>\n",
              "      <td>15000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>7700.0</td>\n",
              "      <td>20000</td>\n",
              "      <td>-12300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms G</th>\n",
              "      <td>10000</td>\n",
              "      <td>5000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-2300.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms H</th>\n",
              "      <td>15000</td>\n",
              "      <td>6000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>-1300.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>-6300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms I</th>\n",
              "      <td>20000</td>\n",
              "      <td>8000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>700.0</td>\n",
              "      <td>10000</td>\n",
              "      <td>-9300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ms J</th>\n",
              "      <td>18000</td>\n",
              "      <td>10000</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>2700.0</td>\n",
              "      <td>8000</td>\n",
              "      <td>-5300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          x      y      f0    y-f0     Δ1     y-f1\n",
              "Ms A   7000   3000  7300.0 -4300.0  -3000  -1300.0\n",
              "Ms B   8000   5000  7300.0 -2300.0  -2000   -300.0\n",
              "Ms C  25000  12000  7300.0  4700.0  15000 -10300.0\n",
              "Ms D   5000   2000  7300.0 -5300.0  -5000   -300.0\n",
              "Ms E  12000   7000  7300.0  -300.0   2000  -2300.0\n",
              "Ms F  30000  15000  7300.0  7700.0  20000 -12300.0\n",
              "Ms G  10000   5000  7300.0 -2300.0      0  -2300.0\n",
              "Ms H  15000   6000  7300.0 -1300.0   5000  -6300.0\n",
              "Ms I  20000   8000  7300.0   700.0  10000  -9300.0\n",
              "Ms J  18000  10000  7300.0  2700.0   8000  -5300.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T72unS4a7HJP",
        "colab_type": "text"
      },
      "source": [
        "And the next classifier will try to predict $y-f_1$ based on x.\n",
        "\n",
        "If you understand all these examples, from Utah to Makeup, you have a pretty good intuition on how Gradient Boosting works.\n",
        "\n",
        "# XGBoost\n",
        "You may recall that in the first few videos, we mentioned that XGBoost was one of the state-of-the-art algorithms. The Kaggle competition winners are dominated by deep learning and XGBoost solutions.\n",
        "\n",
        ">I only use XGBoost (Liberty Mutual Property Inspection, Winner's Interview: Qingchen Wang)\n",
        "\n",
        "> As the winner of an increasing amount of Kaggle competitions XGBoost showed us again to be a great all-around algorithm worith having in your toolbox (Dato Winner's Interview, 1st Place, Mad Professors)\n",
        "\n",
        "> The only supervised learning method I used was gradient boosting as implemented in the excellent xgboost package (Recruit Coupon Purchase Winner's Interview, 2nd place, Halla Yang)\n",
        "\n",
        "\n",
        "\n",
        "We are going to start our exploration of XGBoost using the Iris dataset, which we have used before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma21zshP4umI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "3bffbc58-3da7-4af4-900c-f7f61aa3f765"
      },
      "source": [
        "\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('1jLIRJwfZhg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/1jLIRJwfZhg\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f953e2809e8>"
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8XREkjWB5fS",
        "colab_type": "text"
      },
      "source": [
        "This reminds me of a section of the *Hitchhiker's Guide to the Galaxy* by Douglas Adams, where Marvin, the robot, is asked to bring two hitchhikers to the bridge and he says:\n",
        "\n",
        "> Here I am, brain the size of a planet, and they ask me to take you to the bridge. Call that job satisfaction, 'cause I don't\n",
        "\n",
        "XGBoost is an extremely powerful state-of-the-art algorithm and we are using it on a toy example. Oh well.\n",
        "\n",
        "### GPU!\n",
        "We are going to be running this code on a Graphics Processing Unit, GPU, a graphics card.\n",
        "\n",
        "To do so, under the runtime menu above, select **Change Runtime Type** and select **GPU**\n",
        "\n",
        "That's it! Now let's check out what GPU we are using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Bp2hvE6bcg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "58ce67fe-8cd9-41bf-b6f2-446a242087e0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Sep 16 21:30:48 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    20W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZmJUO_2CGez",
        "colab_type": "text"
      },
      "source": [
        "It is a Tesla T4, which has 320 tensor cores.\n",
        "\n",
        "\n",
        "\n",
        "Now let's load the database\n",
        "\n",
        "## The Iris Data Set\n",
        "\n",
        "### Load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwUq3mqwB9EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/iris.csv')\n",
        "\n",
        "iris_train, iris_test = train_test_split(iris, test_size = 0.2)\n",
        "train_X = iris_train[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]\n",
        "train_y = iris_train['Class']\n",
        "test_X = iris_test[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]\n",
        "test_y = iris_test['Class']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmHyfTQeCUQs",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the XGBoost classifier\n",
        "We are going to create an XGBoost classifier with gpu support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acYGT2PzCRJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "3353e01d-faeb-4355-f0b3-1a282c30281e"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "params = { \"n_estimators\": 400, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor' }\n",
        "\n",
        "model = XGBClassifier(**params)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=400, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic',\n",
              "              predictor='gpu_predictor', random_state=0, reg_alpha=0,\n",
              "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
              "              subsample=1, tree_method='gpu_hist', verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX_yoP5kDKjl",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at those parameters.\n",
        "\n",
        "* **n_estimators** the number of classifiers in the boost ensemble. The default is 100.\n",
        "* **tree_method** the tree construction algorithm that is used. `gpu_hist` is a distributed histogram approach (see the [original paper](https://arxiv.org/pdf/1603.02754.pdf))\n",
        "* **predictor** the prediction algorithm to use. `gpu_predictor` means use the gpu!\n",
        "* **max_depth** the depth of the decision trees. The default of 3 is used here. The trees for any ensemble method are typically very shallow. \n",
        "\n",
        "### Fitting model to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxXaHTVCbt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "24f257b2-86e2-45bf-ebac-4ae59948efab"
      },
      "source": [
        "model.fit(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=400, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob',\n",
              "              predictor='gpu_predictor', random_state=0, reg_alpha=0,\n",
              "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
              "              subsample=1, tree_method='gpu_hist', verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xekQss6bFp65",
        "colab_type": "text"
      },
      "source": [
        "### evaluate model\n",
        "Finally let's evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO-caKnjFlG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcd0a5df-dbce-4399-a16c-3fdc45c49621"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris_predictions = model.predict(test_X)\n",
        "accuracy_score(test_y, iris_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoX-aaylF8He",
        "colab_type": "text"
      },
      "source": [
        "We ran a state-of-the-art algorithm on a GPU. Yay us!\n",
        "\n",
        "\n",
        "\n",
        "Now we are going to back up quite a bit.\n",
        "\n",
        "#### Bagging and Pasting\n",
        "With bagging and pasting we created a number of decision trees each of which was trained on different data. One tree did not influence the construction of another. Each classifier was independent of the others.\n",
        " \n",
        "\n",
        "#### Boosting\n",
        "Boosting is different. \n",
        "\n",
        "Imagine that we create one decision tree classifier. Let's call it Classifier 1. Classifier 1 doesn't perform with 100% accuracy. \n",
        "\n",
        "Next we create a second decision tree classifier and as part of its training data we will use the instances that Classifier got wrong. Now Classifier 2 isn't perfect either and there will be some instances that both Classifier 1 and Classifier 2 got wrong, and, you guessed it, we will use those instances as part of the training data for Classifier 3.\n",
        "\n",
        "#### 400 Classifiers\n",
        "Suppose we created 400 classifiers using the bagging algorithm. Since each classifier is independent of the others, we can run those 400 in parallel. \n",
        "\n",
        "Now think about boosting for a moment. Can we run those in parallel?\n",
        "\n",
        "Since one classifier is dependent on the errors of the others it seems like we couldn't run them in parallel and doing 400 classifiers in series seems impractical. Fortunately for us, XGBoost has parallelized training!\n",
        "\n",
        "\n",
        "# The task - The Adult Dataset\n",
        "\n",
        "Let's try a bit larger dataset, the [Adult Dataset](http://archive.ics.uci.edu/ml/datasets/Adult). The webpage describes the problem. We are trying to predict whether someone makes more that $50,000 year based on a number of features. The data folder contains both training data `adult.data` and test data `adult.test`. \n",
        "\n",
        "## Prepare the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O1qfbyAFvgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "b1443fa2-03ea-4dd1-e911-d3af83bb828d"
      },
      "source": [
        "colNames = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
        "            'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'wage']\n",
        "adult = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=colNames)\n",
        "adult"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>wage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  fnlwgt  ... hours-per-week  native-country    wage\n",
              "0       39          State-gov   77516  ...             40   United-States   <=50K\n",
              "1       50   Self-emp-not-inc   83311  ...             13   United-States   <=50K\n",
              "2       38            Private  215646  ...             40   United-States   <=50K\n",
              "3       53            Private  234721  ...             40   United-States   <=50K\n",
              "4       28            Private  338409  ...             40            Cuba   <=50K\n",
              "...    ...                ...     ...  ...            ...             ...     ...\n",
              "32556   27            Private  257302  ...             38   United-States   <=50K\n",
              "32557   40            Private  154374  ...             40   United-States    >50K\n",
              "32558   58            Private  151910  ...             40   United-States   <=50K\n",
              "32559   22            Private  201490  ...             20   United-States   <=50K\n",
              "32560   52       Self-emp-inc  287927  ...             40   United-States    >50K\n",
              "\n",
              "[32561 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Iodm2UOrs7",
        "colab_type": "text"
      },
      "source": [
        "## divide features and labels\n",
        "let's create 2 DataFrames, one for the features and one for the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I01XNJzPOI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "2303a972-0edb-403d-a5a2-900dcbbf50e4"
      },
      "source": [
        "adult_features = adult.drop('wage', axis=1)\n",
        "adult_labels = adult['wage']\n",
        "adult_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  ...  hours-per-week  native-country\n",
              "0       39          State-gov  ...              40   United-States\n",
              "1       50   Self-emp-not-inc  ...              13   United-States\n",
              "2       38            Private  ...              40   United-States\n",
              "3       53            Private  ...              40   United-States\n",
              "4       28            Private  ...              40            Cuba\n",
              "...    ...                ...  ...             ...             ...\n",
              "32556   27            Private  ...              38   United-States\n",
              "32557   40            Private  ...              40   United-States\n",
              "32558   58            Private  ...              40   United-States\n",
              "32559   22            Private  ...              20   United-States\n",
              "32560   52       Self-emp-inc  ...              40   United-States\n",
              "\n",
              "[32561 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTPkOHzNITR",
        "colab_type": "text"
      },
      "source": [
        "Now let's one hot encode the features using sklearn's OneHotEncoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCItEsf8K2TZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6aadb4c7-1c2e-4724-b644-7d0aeda6c936"
      },
      "source": [
        "#TODO  \n",
        "\n",
        "\n",
        "adultSparse = \"TO DO\"\n",
        "adultSparse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<32561x22144 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 455854 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZyAn3WLZMqV",
        "colab_type": "text"
      },
      "source": [
        "Fantastic!  \n",
        "\n",
        "Let's go ahead divide this up into training and test sets (Notice that this is a bit different than we have been doing it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc-AXr9FZGhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c786231e-9439-4054-ddb3-882dc077dbce"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "adult_train_features, adult_test_features, adult_train_labels, adult_test_labels = train_test_split(adultSparse, adult_labels, test_size = 0.7)\n",
        "adult_train_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<9768x22144 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 136752 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRDTurCqa9nq",
        "colab_type": "text"
      },
      "source": [
        "You may have noticed that we put a whopping 70% of the data in the test set. We did this because when we are just playing with things to gain an understanding we don't want to wait hours for a result.\n",
        "\n",
        "Create an XGBoost classifier called model with the parameters:\n",
        "\n",
        "* `tree_method: gpu_hist`\n",
        "* `predictor: gpu_predictor`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Py0OOb5aBy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TO DO call it model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eks7EiOebgkj",
        "colab_type": "text"
      },
      "source": [
        "Now let's say we want to find the best hyperparameter values for \n",
        "\n",
        "* n_estimators -- let's try 50, 100, 150, 200\n",
        "* max_depth -- let's try 2, 4, 6, 8\n",
        "\n",
        "\n",
        "Go ahead and create the `param_grid`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkqwXndrbfhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4GoFt4hc4zt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Time Constraint\n",
        "\n",
        "Even with a GPU it is going to take a long time to do an exhaustive search of which parameters are best. There are 16 possible combinations. We may want 5 fold cross validation. That is 80 fits, each of which is creating on average 100 classifiers. And we have around 20,000 instances in our training data. Let's pick a random smaller set of combinations to test.  Let's say we want the search algorithm to select 5 combinations of hyperparameters `param_comb` at random.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZkjYTZHc1iP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "param_comb = 5\n",
        "folds=5\n",
        "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=param_comb,  n_jobs=-1, \n",
        "                                   cv=skf.split(adult_train_features,adult_train_labels), verbose=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3syNvBizdMYj",
        "colab_type": "text"
      },
      "source": [
        "Let's fit the model (this will take awhile)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVxaKf8OdK9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "2e668137-b516-4d8e-994a-af1871714084"
      },
      "source": [
        "%%time \n",
        "grid_result = random_search.fit(adult_train_features, adult_train_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  8.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 24 s, sys: 19.3 s, total: 43.3 s\n",
            "Wall time: 8min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhJmfE6gM6IJ",
        "colab_type": "text"
      },
      "source": [
        "Now let's see what the best parameters are, make predictions on our test data, and check accuracy..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFPTUpq_dT0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2a87777-7ebe-4e22-c72d-305b1c4b593d"
      },
      "source": [
        "random_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 8, 'n_estimators': 200}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTboDXG8xsRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = random_search.best_estimator_.predict(adult_test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA0tfa2n7t2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "deb9a6ff-6726-4902-aae7-74149e956454"
      },
      "source": [
        "accuracy_score(adult_test_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8616680559820997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmC50ToeLn7S",
        "colab_type": "text"
      },
      "source": [
        "### This ends our first look at XGBoost\n",
        "T\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTPwWCmh7y8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}