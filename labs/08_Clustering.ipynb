{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xvgavwf4r5H"
   },
   "source": [
    "# Introduction to Clustering\n",
    "\n",
    "\n",
    "## Supervised vs Unsupervised learning\n",
    "\n",
    "Up until now we have been focusing on supervised learning. In supervised learning our training set consists of labeled data. For example, we have images and each image has an associated  label: dog, cat, elephant. And from this labeled data our model is trying to learn how to predict the label from the features. \n",
    "\n",
    "Unsupervised learning is trying to learn patterns from unlabeled data, and one set of models has to do with segmenting a dataset into clusters or groups of related data.\n",
    "\n",
    "![](https://cambridge-intelligence.com/wp-content/uploads/2016/01/clustering-animated.gif)\n",
    "\n",
    "\n",
    "In clustering the system divides a set of instances into clusters or groups\n",
    "based on some measure of similarity. There are two main types of clustering algorithms.\n",
    "\n",
    "### k-means clustering\n",
    "For one type, we tell the algorithm how many clusters to make. Please cluster these 1,000\n",
    "people into 5 groups. Please classify these web pages into 15 groups. These methods go by\n",
    "the name of k-means clustering algorithms and we will discuss those a bit later.\n",
    "\n",
    "\n",
    "### hierarchical clustering\n",
    "For the other approach we don’t specify how many clusters to make. Instead the algorithm\n",
    "starts with each instance in its own cluster. At each iteration of the algorithm it combines the\n",
    "two most similar clusters into one. It repeatedly does this until there is only one cluster. This is called hierarchical clustering and its name makes sense. The running of the algorithm\n",
    "results in one cluster, which consists of two sub-clusters. Each of those two sub-clusters in\n",
    "turn, consist of 2 sub-sub clusters and so on. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/hierarchicalClustering1.png)\n",
    "\n",
    "\n",
    "Again, at each iteration of the algorithm we join the two closest clusters. To determine the\n",
    "‘closest clusters’ we use a distance formula. But we have some choices in how we compute\n",
    "the distance between two clusters, which leads to different clustering methods. Consider the\n",
    "three clusters (A, B, and C) illustrated below each containing two members. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/hierarchicalClustering2.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Which pair of\n",
    "clusters should we join? Cluster A with B, or cluster C with B?\n",
    "\n",
    "### Single-linkage clustering\n",
    "In single-linkage clustering we define the distance between two clusters as the shortest\n",
    "distance between any member of one cluster to any member of the other. With this\n",
    "definition, the distance between Cluster A and Cluster B is the distance between A1 and B1,\n",
    "since that is shorter than the distances between A1 and B2, A2 and B1, and A2 and B2. With\n",
    "single-linkage clustering, Cluster A is closer to Cluster B than C is to B, so we would combine\n",
    "A and B into a new cluster.\n",
    "\n",
    "### Complete-linkage clustering\n",
    "In complete-linkage clustering we define the distance between two clusters as the greatest\n",
    "distance between any member of one cluster to any member of the other. With this\n",
    "definition, the distance between Cluster A and Cluster B is the distance between A2 and B2.\n",
    "With complete-linkage clustering, Cluster C is closer to Cluster B than A is to B, so we would\n",
    "combine B and C into a new cluster.\n",
    "\n",
    "### Average-linkage clustering\n",
    "In average-linkage clustering we define the distance between two clusters as the average\n",
    "distance between any member of one cluster to any member of the other. In the diagram\n",
    "above, it appears that the average distance between Clusters C and B would be less than the\n",
    "average between A and B and we would combine B and C into a new cluster. \n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "## Dog Breed Dataset\n",
    "\n",
    "Let's explore this with a small dog breed dataset.\n",
    "\n",
    "First, we will load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAQv0rnv4r5N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dog_data = pd.read_csv('https://raw.githubusercontent.com/zacharski/machine-learning-notebooks/master/data/dogbreeds.csv')\n",
    "dog_data = dog_data.set_index('breed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "mPnF_WW44r5N",
    "outputId": "eb111d01-2ce4-4f5d-fda2-2195aa65c5c2"
   },
   "outputs": [],
   "source": [
    "dog_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JI_WDmv4r5O"
   },
   "source": [
    "Looking at the values in the height and weight columns it looks like we should normalize the data.\n",
    "\n",
    "<img src=\"http://animalfair.com/wp-content/uploads/2014/08/chihuahua-and-great-dane.jpg\" width=\"700\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font> \n",
    "\n",
    "<font color='#EE4C2C'>1. Normalize the data</font> \n",
    "\n",
    "Normalize the data. Name the normalized dataset `dog_data`. HINT: This was covered in the sklearn lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "kBV9waEe4r5P",
    "outputId": "072287c2-2654-4e68-d51a-d90768090444"
   },
   "outputs": [],
   "source": [
    "\n",
    "## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVe08rKB4r5P"
   },
   "source": [
    "And let's visualize that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mbnPGxOU0jJd",
    "outputId": "bf76db54-419b-4a20-9c7f-1c7c7c26ec20"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set the figure size\n",
    "plt.rcParams[\"figure.figsize\"] = [14.00, 14]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# Scatter plot\n",
    "ax = dog_data.plot.scatter(x='Height', y='Weight', alpha=0.5)\n",
    "\n",
    "# Annotate each data point\n",
    "for i, txt in enumerate(dog_data.index.values.tolist()):\n",
    "   ax.annotate(txt, (dog_data.Height.iat[i]+0.02, dog_data.Weight.iat[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNd0rh6B4r5Q"
   },
   "source": [
    "Gazing at the scatter plot, it looks like we could group the data into three clusters. There are the 2 data points on the bottom left (*Chihuahua* and *Yorkshire Terrier*) The top right group of two (*Bull Mastiff* and *Great Dane*) and the middle group with all the other breeds. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "##  Now let's compute the Euclidean distance between all the breeds:\n",
    "\n",
    "This is just for pedagogical reasons --- no need to do this when using sklearn's clustering algorithms!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "6taLP4UMaQn2",
    "outputId": "b3d760f6-68dd-43a1-9ca2-00f75a090e8c"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "pdist(dog_data)\n",
    "\n",
    "pairwise = pd.DataFrame(\n",
    "    squareform(pdist(dog_data)),\n",
    "    columns = dog_data.index,\n",
    "    index = dog_data.index\n",
    ")\n",
    "\n",
    "pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iepQufLbwG0"
   },
   "source": [
    "### Hierarchical Clustering step by step\n",
    "\n",
    "### Step 1.\n",
    "Initially, each breed is in its own cluster. We find the two closest clusters and combine them\n",
    "into one cluster. From the table above we see that the closest clusters are the\n",
    "Border Collie and the Portuguese Water Dog (distance of 0.050470) so we combine them\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dendro1.png)\n",
    "\n",
    "### Step 2. \n",
    "We find the two closest clusters and combine them into one cluster. From the table on the\n",
    "preceding page we see that these are the Chihuahua and the Yorkshire Terrier (distance of\n",
    "0.077200) so we combine them. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dendro2.png)\n",
    "\n",
    "### Step 3.\n",
    "We repeat the process again. This time combining the German Shepherd and the Golden\n",
    "Retriever.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dendro3.png)\n",
    "\n",
    "### Step 4.\n",
    "\n",
    "We repeat the process yet again. From the table we see that the next closest pair is the Border\n",
    "Collie and the Brittany Spaniel. The Border Collie is already in a cluster with the Portuguese\n",
    "Water Dog which we created in Step 1. So in this step we are going to combine that cluster\n",
    "with the Brittany Spaniel.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dendro4.png)\n",
    "\n",
    "### Step 5.\n",
    "And we continue ...\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dendro5.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/borderCollie.png)\n",
    "\n",
    "\n",
    "## Hierarchical Clustering in sklearn\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/doodle.png)\n",
    "\n",
    "Here is how to do hierarchical clustering in sklearn ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEq7UkwuemzW",
    "outputId": "87ce58c1-b38c-4bc5-fe34-f25143a036a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clusterer = AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters=3)\n",
    "clusterer.fit_predict(dog_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbSTtrPjgrrC"
   },
   "source": [
    "And let's draw the dendrogram ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mVxaIA9xgsAL",
    "outputId": "8d16acd5-d066-4f59-87a6-47489323339b"
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(dog_data, method='single'), labels = dog_data.index, orientation = 'right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHxunl0swNUM"
   },
   "source": [
    "### Let's disect this\n",
    "\n",
    "Let's take a closer look at the parameters for AgglomerativeClustering `AgglomerativeClustering(affinity='euclidean', linkage='ward', n_clusters=3)`\n",
    "\n",
    "`affinity='euclidean'` simply means to use the Euclidean distance. The options for affinity are `manhattan` and `cosine` among others. The default is euclidean.\n",
    "\n",
    "`linkage='ward'` describes how we decide what clusters to join. The options are:\n",
    "\n",
    "\n",
    "* `ward` minimizes the variance of the clusters being merged. (the default)\n",
    "* `average` uses the average of the distances of each observation of the two sets.\n",
    "* `complete` or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
    "* `single` uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "You'll notice that when we drew the dendrogram we used single linkage because it matched the explanation above. In general, it is good to use the default ward linkage.\n",
    "\n",
    "`n_clusters=3` specifies how many clusters to create. In this case we instruct the algorithm to create 3 clusters. Let's do a scatter plot of those clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p36BJemn6Jo8",
    "outputId": "b3a32a7d-7092-4465-c723-976718a56f90"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20.00, 16]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "#ax = plt.scatter(x=dog_data['Height'], y=dog_data['Weight'], c= clusterer.labels_, cmap='rainbow' )\n",
    "ax = dog_data.plot.scatter(x='Height', y='Weight', alpha=0.5, c= clusterer.labels_, cmap='rainbow', s=60)\n",
    "for i, txt in enumerate(dog_data.index.values.tolist()):\n",
    "   ax.annotate(txt, (dog_data.Height.iat[i]+0.02, dog_data.Weight.iat[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmiGEBBHGrsq"
   },
   "source": [
    "So the Yorkshire Terrier and Chihuahua are in a cluster by themselves as is the Bullmastiff and the Great Dane. The remainder of the dog breeds are in the third cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xe0SPnRbMZS"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "# k means clustering\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/kmeans1.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/kmeans2.png)\n",
    "\n",
    "The basic k-means algorithm is \n",
    "\n",
    "1. select k random instances to be the\n",
    "initial centroids\n",
    "2. REPEAT\n",
    "3. assign each instance to the nearest\n",
    "centroid. (forming k clusters)\n",
    "4. update centroids by computing mean\n",
    "of each cluster\n",
    "5. UNTIL centroids don’t change (much).\n",
    "\n",
    "Let’s go through an example. Consider the following points (x and y coordinates):\n",
    "\n",
    "Let's divide our dog dataset into 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "XiJq1OD0Ix49",
    "outputId": "00dd0aa3-6e2a-4230-a49f-a4e0a886be60"
   },
   "outputs": [],
   "source": [
    "points = pd.DataFrame({'x':[1,1, 2, 2, 4, 4, 5, 5], 'y':[2,4,2,3,2,4,1,3]})\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-00Z7DdJRVw"
   },
   "source": [
    "Say we want to cluster these into 2 groups.\n",
    "\n",
    "**step 1 of above algorithm: select k random instances to be initial centroids.**\n",
    "\n",
    "Suppose we randomly select (1, 4) as centroid 1 and (4, 2) as centroid 2.\n",
    "\n",
    "**step 3: assign each instance to the nearest centroid**\n",
    "To assign each instance to the nearest centroid we can use any of the distance measures we\n",
    "have previously discussed. To keep things simple, for this example let’s use Manhattan\n",
    "Distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "D78lZqhgJnYt",
    "outputId": "6a3f928f-9b78-444e-d3eb-c51278af79ee"
   },
   "outputs": [],
   "source": [
    "points['CentOneDist'] = np.abs(points['x'] - 1) + np.abs(points['y'] - 4)\n",
    "points['CentTwoDist'] = np.abs(points['x'] - 4) + np.abs(points['y'] - 2)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RGrY66bMFW2"
   },
   "source": [
    "Based on these distances we assign the points to the following clusters:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cluster1.png)\n",
    "\n",
    "step 4: update centroids\n",
    "We compute the new centroids by computing the mean of each cluster. The mean x\n",
    "coordinate of cluster 1 is:\n",
    "(1 + 1 + 2) / 3 = 4/3 = 1.33\n",
    "and the mean y is\n",
    "(2 + 4 + 3) / 3 = 9/3 = 3\n",
    "So the new cluster 1 centroid is (1.33, 3).\n",
    "The new centroid for cluster 2 is (4, 2.4)\n",
    "\n",
    "**step 5: until centroids don’t change**\n",
    "\n",
    "The old centroids were (1, 4) and (4, 2) and the new ones are (1.33, 3) and (4, 2.4). The\n",
    "centroids changed so we repeat.\n",
    "\n",
    "**step 3: assign each instance to the nearest centroid**\n",
    "\n",
    "Again we compute Manhattan Distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HdouCFZPNlpv",
    "outputId": "bfa10372-0c5e-4823-d9d6-07b3950f1855"
   },
   "outputs": [],
   "source": [
    "points['CentOneDist'] = np.abs(points['x'] - 1.33) + np.abs(points['y'] - 3)\n",
    "points['CentTwoDist'] = np.abs(points['x'] - 4) + np.abs(points['y'] - 2.4)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYCC59YsNxPe"
   },
   "source": [
    "and based on these distances assign the points to clusters:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cluster2.png)\n",
    "\n",
    "**step 4: update centroids**\n",
    "\n",
    "We compute the new centroids by computing the mean of each cluster.\n",
    "* Cluster 1 centroid: (1.5, 2.75)\n",
    "* Cluster 2 centroid: (4.5, 2.5)\n",
    "\n",
    "**step 5: until centroids don’t change**\n",
    "The centroids changed so we repeat.\n",
    "\n",
    "**step 3: assign each instance to the nearest centroid**\n",
    "Again we compute Manhattan Distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "TcwplNmvOKd2",
    "outputId": "c4542f3c-8dd7-4a5b-c294-f71b53feabf0"
   },
   "outputs": [],
   "source": [
    "points['CentOneDist'] = np.abs(points['x'] - 1.5) + np.abs(points['y'] - 2.75)\n",
    "points['CentTwoDist'] = np.abs(points['x'] - 4.5) + np.abs(points['y'] - 2.5)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfhTn0TLOw0j"
   },
   "source": [
    "and based on these distances assign the points to clusters:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cluster3.png)\n",
    "\n",
    "**step 4: update centroids**\n",
    "\n",
    "We compute the new centroids by computing the mean of each cluster.\n",
    "Cluster 1 centroid: (1.5, 2.75)\n",
    "Cluster 2 centroid: (4.5, 2.5)\n",
    "\n",
    "**step 5: until centroids don’t change**\n",
    "\n",
    "The updated centroids are identical to the previous ones so the algorithm converged on a\n",
    "solution and we can stop. The final clusters are\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/cluster4.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/centroidText.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/centroidGood.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/CentroidSimple.png)\n",
    "\n",
    "# Hill Climbing\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Hill1.png)\n",
    "\n",
    "I would like to briefly interrupt our\n",
    "discussion of K-means clustering to talk\n",
    "about hill climbing algorithms. Suppose our\n",
    "goal is to reach the peak of some mountain\n",
    "and we come up with the following\n",
    "algorithm:\n",
    "\n",
    "```\n",
    "start at some random location on the mountain.\n",
    "REPEAT\n",
    " take a step in the direction that will take you higher.\n",
    "UNTIL there is no direction that will take you higher.\n",
    "```\n",
    "\n",
    "This seems like a reasonable algorithm.\n",
    "Consider using it with the mountain shown here\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Hill2.png)\n",
    "\n",
    "You can see that regardless of where we are plopped\n",
    "down on the mountain, we will reach the peak if we\n",
    "follow the algorithm.\n",
    "And if we think of this as a graph, we will reach the\n",
    "peak value regardless of where we start on the graph.\n",
    "\n",
    "Now let’s consider using the algorithm with the following graph\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Hill3.png)\n",
    "\n",
    "Thus, this simple version of the hill-climbing algorithm is not guaranteed to reach the\n",
    "optimal solution.\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Hill4b.png)\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/Hill5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnGWodF6d7l_"
   },
   "source": [
    "## SSE or Scatter\n",
    "\n",
    "To determine the quality of a set of clusters we can use the sum of the squared error\n",
    "(SSE). This is also called scatter. Here is how to compute it: for each point we will square\n",
    "the distance from that point to its centroid, then add all those squared distances together.\n",
    "More formally, \n",
    "\n",
    "$$SSE=\\sum_{i=1}^{k}\\sum_{x∈C_i}{dist(c_i,x)^2}$$\n",
    "\n",
    "Let’s dissect that. In the first summation sign we are iterating over the clusters. So initially i\n",
    "equals cluster 1, then i equals cluster 2, up to i equals cluster k. The next summation sign\n",
    "iterates over the points in that cluster—something like, for each point x in cluster i. Dist is\n",
    "whatever distance formula we are using (for example, Manhattan, or Euclidean). So we\n",
    "compute the distance between that point, x, and the centroid for the cluster ci, square that\n",
    "distance and add it to our total. \n",
    "\n",
    "Let’s say we run our k-means algorithm twice on the same data and for each run we pick a\n",
    "different set of random initial centroids. Is the set of clusters that were computed during the\n",
    "first run worse or better than the set computed during the second run? To answer that\n",
    "question we compute the SSE for both sets of clusters. The set with the smaller SSE is the\n",
    "better of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0vw4t-lOTm_"
   },
   "source": [
    "# k-means in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CpGewPEOe0R"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2skLMP5X4r5Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, ).fit(dog_data)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qTILIqf4r5R"
   },
   "source": [
    "The variable `labels` is an array the specifies which group each dog belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77rM5cBM4r5R",
    "outputId": "10858e22-f986-47de-8875-aae66a6592a7"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSlFYCBr4r5R"
   },
   "source": [
    "My results were:\n",
    "\n",
    "    array([0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 2], dtype=int32)\n",
    "\n",
    "which indicates that the first, second, and third dogs are in group 0, the next one in group 1 and so on  That may be helpful for future computational tasks but is not the helpful if we are trying to visualize the data. Let me munge that a bit into a slightly more useful form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DHfIQVP4r5R",
    "outputId": "33840b4e-1832-48d9-d4e3-35255f0bc33d"
   },
   "outputs": [],
   "source": [
    "groups = {0: [], 1: [], 2: []}\n",
    "i = 0\n",
    "for index, row in dog_data.iterrows():\n",
    "    groups[labels[i]].append(index)\n",
    "    i += 1\n",
    "## Now I will print it in a nice way:\n",
    "\n",
    "for key, value in groups.items():\n",
    "    print ('CLUSTER %i' % key)\n",
    "    for breed in value:\n",
    "        print(\"    %s\" % breed)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_OMZ5HL4r5S"
   },
   "source": [
    "keep in mind that since they initial centroids are selected somewhat randomly it is possible that you get a different answer than I do. The answer I got was:\n",
    "\n",
    "    CLUSTER 0\n",
    "        Border Collie\n",
    "        Boston Terrier\n",
    "        Brittany Spaniel\n",
    "        German Shepherd\n",
    "        Golden Retreiver\n",
    "        Portuguese Water Dog\n",
    "        Standard Poodle\n",
    "    \n",
    "\n",
    "    CLUSTER 1\n",
    "        Bullmastiff\n",
    "        Great Dane\n",
    "\n",
    "\n",
    "    CLUSTER 2\n",
    "        Chihuahua\n",
    "        Yorkshire Terrier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVSLZcCN4r5V"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font> \n",
    "## <font color='#EE4C2C'>1. Breakfast Cereals</font> \n",
    "\n",
    "I would like you to create 4 clusters of the data in:\n",
    "\n",
    "    https://raw.githubusercontent.com/zacharski/pg2dm-python/master/data/ch8/cereal.csv\n",
    "    \n",
    "For clustering use the features calories, sugar, protein, and fiber.\n",
    "\n",
    "Print out the results as we did for the dog breed data:\n",
    "\n",
    "\n",
    "    CLUSTER 0\n",
    "    Bullmastiff\n",
    "    Great Dane\n",
    "    \n",
    "\n",
    "    CLUSTER 1\n",
    "        Chihuahua\n",
    "        Yorkshire Terrier\n",
    "    \n",
    "\n",
    "    CLUSTER 2\n",
    "        Border Collie\n",
    "        Boston Terrier\n",
    "        Brittany Spaniel\n",
    "        German Shepherd\n",
    "        Golden Retreiver\n",
    "        Portuguese Water Dog\n",
    "        Standard Poodle\n",
    "        \n",
    "Because the initial centroids are random, by default the sklearn kmeans agorithm runs the algorithm 10 times and picks the best results (based on some of squares error). I would like you to change that parameter so it runs the algorithm 100 times.   Just google `sklearn kmeans` to get documentation on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2RR8kwx4r5V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkQoS43x4r5W"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Hierarchical</font> \n",
    "\n",
    "I would like you to use the hierarchical clustering algorithm on the cereal data. Plot the dendrogram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9Ye84l-4r5W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnvJ32j5FCxf"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxfXTsvzgPYi"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>Fighting Poverty</font> \n",
    "## <font color='#EE4C2C'>(bonus possible based on quality of work)</font> \n",
    "\n",
    "\n",
    "\n",
    "### Objective:\n",
    "To categorise the countries using socio-economic and health factors that determine the overall development of the country.\n",
    "\n",
    "### Problem Statement:\n",
    "[HELP International's](https://help-international.org/) hmission is to fight poverty and empower people. Even though it has millions of dollars of funds, it is a tiny amount compared to the needs of the countries of the world. HELP needs to decide how to use this money strategically and effectively by selecting countries that are in the direst need of aid. Hence, your Job as a Data scientist is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.\n",
    "\n",
    "\n",
    "Specifically, you need to\n",
    "\n",
    "* determine what columns are relevant for this task \n",
    "* cluster the countries (you decide how many clusters)\n",
    "* create a choropleth map showing the clusters\n",
    "* determine the cluster representing countries of greatest need.\n",
    "* list the countries with the greatest need.\n",
    "\n",
    "Use whatever methods and hyperparameters you want. \n",
    "\n",
    "The [dataset](https://raw.githubusercontent.com/zacharski/ml-class/master/data/Country-data.csv) countains 10 measures for each of 167 countries:\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "country|Name of the country\n",
    "child_mort|Death of children under 5 years of age per 1000 live births\n",
    "exports|Exports of goods and services per capita. Given as %age of the GDP per capita\n",
    "health|Total health spending per capita. Given as %age of GDP per capita\n",
    "imports|Imports of goods and services per capita. Given as %age of the GDP per capita\n",
    "Income|Net income per person\n",
    "Inflation|The measurement of the annual growth rate of the Total GDP\n",
    "life_expec|The average number of years a new born child would live if the current mortality patterns are to remain the same\n",
    "total_fer|The number of children that would be born to each woman if the current age-fertility rates remain the same.\n",
    "gdpp|The GDP per capita. Calculated as the Total GDP divided by the total population.\n",
    "\n",
    "### Choropleth Map\n",
    "A choropleth map allows us to visualize how a variable varies across a geographic area. There is a python library that helps us create one. \n",
    "\n",
    "Here is a quick example,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "TcctBi3bRLTa",
    "outputId": "e19f59ea-f32e-4db7-fd58-a89a889e140d"
   },
   "outputs": [],
   "source": [
    "countries = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/Country-data.csv')\n",
    "countries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOGF8tRpRLVG"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "b6pGSwweRMcX",
    "outputId": "19bb7e40-892b-413b-b3f9-ffe7376fba46"
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth(countries,\n",
    "                    locationmode='country names',\n",
    "                    locations='country',\n",
    "                    color='life_expec',\n",
    "                    title='Coutries by Life Expectancy'\n",
    "                   )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKTxAfW7SFZn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Working version of clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
