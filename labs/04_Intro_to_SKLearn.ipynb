{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGqlHFimEx7-"
   },
   "source": [
    "## Introduction to SciKitLearn (sklearn)\n",
    "#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg)\n",
    "\n",
    "\n",
    "Scikit-learn (aka `sklearn`)\n",
    "sklearn is a Python library that implements many machine learning algorithms. It uses Numpy for high performance matrix operations. \n",
    "\n",
    "The best bet, as with probably everything else, is just to google what you want. For example, if you want to learn how to use kNN (k nearest neighbors) with sklearn, google **sklearn knn**  and you will find the information. The good thing about sklearn documentation, is that they provide a short example of how to use each algorithm.\n",
    "\n",
    "That said, let's get started.\n",
    "\n",
    "\n",
    "### Women Athletes\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/rass.png)\n",
    "<font size='0.85em'>(Credit: Aly Raisman/Instagram)</font> \n",
    "\n",
    "(Left to right: Liz Cambage (Australian Olympic Basksetball Team, Aly Raisman (U.S. Olympic Gymnast)\n",
    "\n",
    "We are going to start with something that people find very easy: Based on height and weight, guessing what sport (basketball or gymnastics) an athlete plays at a world class level. Starting with a task that is super easy for us will allow us to focus more on the algorithm.\n",
    "\n",
    "The dataset has contains three classes of 50 instances each:\n",
    "\n",
    "1. Female Olympic Basketball Players\n",
    "2. Female Olympic Gymnasts.\n",
    "3. Female Professional Rugby Players.\n",
    "\n",
    "We will restrict our initial exploration to Basketball Players and Gymnasts. \n",
    "\n",
    "\n",
    "There are only 2 attributes or features:\n",
    "\n",
    "1. height in inches \n",
    "2. weight in pounds\n",
    "\n",
    "\n",
    "\n",
    "The job of the classifier is to determine the class of an instance (the sport) based on the values of the attributes.\n",
    "\n",
    "The data file is\n",
    "\n",
    "\n",
    "```\n",
    "https://raw.githubusercontent.com/zacharski/datamining-guide/main/data/womens_sports.csv\n",
    "```\n",
    " Let's load it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "tNVeMkqjEx7_",
    "outputId": "46e362b0-7238-4ef0-c962-761b554c0236",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "athletes = pd.read_csv('https://raw.githubusercontent.com/zacharski/datamining-guide/main/data/womens_sports.csv')\n",
    "athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmPJ1QWjmJqT"
   },
   "source": [
    "Before we do anything else, let us graph out the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "S-I_iMSSmOIf",
    "outputId": "80cbf404-42b5-4588-a212-7800c331d41b"
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bpl\n",
    "import bokeh.models as bmo\n",
    "from bokeh.palettes import d3, Set2_3,brewer\n",
    "bpl.output_notebook()\n",
    "source = bpl.ColumnDataSource(athletes)\n",
    "\n",
    "# use whatever palette you want...\n",
    "palette = brewer['Dark2'][len(athletes['sport'].unique())]\n",
    "\n",
    "color_map = bmo.CategoricalColorMapper(factors=athletes['sport'].unique(),\n",
    "                                   palette=palette)\n",
    "\n",
    "# create figure and plot\n",
    "p = bpl.figure(title=\"The Height and Weight of Different Female Athletes\", x_axis_label=\"Height (in.)\", y_axis_label=\"Weight (lb.)\")\n",
    "p.scatter(x='height_in', y='weight_lb',\n",
    "          color={'field': 'sport', 'transform': color_map},\n",
    "          legend_field='sport', source=source)\n",
    "bpl.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B954u8DEx8D"
   },
   "source": [
    "From that chart it looks pretty easy to distinguish gymnasts from basketball players and a bit more challenging to distinguish basketball players from rugby players. Again, to start with we will restrict the dataset to basketball and gymnastics. So our steps for processing the data file are:\n",
    "\n",
    "### 1. Remove the rugby entries from the dataset.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/RugbyDiscard.png)\n",
    "\n",
    "### 2. Divide the dataset columns into features and labels.\n",
    "For `sklearn` algorithms the data needs to be divided into an array of features (in our case height_in and weight_lb) and a Pandas series for the labels (basketball or gymnastics). We do not need the column for athete's name \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/FeaturesLabels.png)\n",
    "\n",
    "\n",
    "### 3. Divide the dataset into training and test sets\n",
    "Eventually we will divide our datasets into three parts. In this introductory worksheet we will divide our data into two:\n",
    "\n",
    "* **training** This is the data we will be using for training our machine learning model.\n",
    "* **test** After we train our model we want to know how well the model performs. We will use the test data for this purpose.\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/TrainTest.png)\n",
    "\n",
    "\n",
    "Let's implement this plan.\n",
    "\n",
    "### 1.  Just get the data for basketball and gymnastics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "soNAR2_SniaJ",
    "outputId": "8352e349-2801-49de-d20f-c22d091cecb0"
   },
   "outputs": [],
   "source": [
    "athletes2 = athletes.loc[(athletes['sport'] == 'gymnastics') | (athletes['sport'] == 'basketball')]\n",
    "athletes2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWiOYgunQaqm"
   },
   "source": [
    "### 2. Divide the dataset into features and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "ZdXFyi5QQZd4",
    "outputId": "8a1b772c-c2c7-4730-81d0-2061508de460"
   },
   "outputs": [],
   "source": [
    "athlete_features = athletes2[['height_in', 'weight_lb']]\n",
    "athlete_labels = athletes2['sport']\n",
    "athlete_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr6NLJb9Ts93"
   },
   "source": [
    "### 3. Divide into training and test sets\n",
    "For this we are going to use `sklearn`'s `train_test_split`.\n",
    "\n",
    "The parameters for `train_test_split` are\n",
    "\n",
    "* the features of the dataset\n",
    "* the labels of the dataset\n",
    "* `random_state` - The data is shuffled before it is split. This parameter sets the same shuffle each time you run it. This helps when you are trying to compare runs and reproduce results.\n",
    "* `train_size` specifies what percent of the data should go into the training set. In our case we will have 80% of the data put into training and 20% into test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KH9c4aQUYZx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "athlete_features_train, athlete_features_test, athlete_label_train, athlete_label_test = train_test_split(athlete_features, athlete_labels, random_state=0, train_size = .8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VM0mpocGul7"
   },
   "source": [
    " \n",
    "\n",
    "## Classifying the sport based on height and weight\n",
    "\n",
    "### The k Nearest Neighbor Algorithm\n",
    "\n",
    "We have seen the germ of the k nearest neighbor algorithm in the Numpy worksheet. \n",
    "\n",
    "Here is a chart of the height and weight of athletes who were in the 2020 Olympics for gymnastics and basketball, along with 2 instances (shown as stars) we would like to classify. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/bokehSport.png)\n",
    "\n",
    "Let us look at the blue star, which we are trying to classify. The idea is that we will find its closest neighbor and classify the blue star as being the class of that nearest neighbor. Some common methods of determining *nearest* include Manhattan and Euclidean distance, which we have seen before. Since its nearest neighbor is an instance of basketball, we can classify the blue star as a basketball player. \n",
    "\n",
    "Things get more interesting when we classify the red star. The closest neighbor to the red star is a gymnast, but the next two nearest neighbors are basketball players. This is where the *k* comes in from k Nearest Neighbors. *k* is just a number. If *k* is 1 we will just use the closest nearest neighbor and classify the red star as gymnast. If *k* is 3 we will use the three closest neighbors. Those are one gymnastics and two basketball. The majority vote is basketball so we classify the red star as that. \n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/dudes.png)\n",
    "\n",
    "### Minkowski\n",
    "\n",
    "So far we have seen Manhattan Distance \n",
    "\n",
    "$$d(x, y) = \\sum_{k=1}^{n}{|x_k-y_k|}$$\n",
    "\n",
    "and Euclidean Distance\n",
    "\n",
    "$$d(x,y) = \\sqrt{\\sum_{k=1}^{n}{|x_k-y_k|}^2}$$\n",
    "\n",
    "We can generalize Manhattan Distance and Euclidean Distance to what is called the\n",
    "Minkowski Distance Metric:\n",
    "\n",
    "$$d(x,y) =(\\sum_{k=1}^{n}{|x_k-y_k|^{p})^{1\\over{p}}}$$\n",
    "\n",
    "When\n",
    "* p = 1: The formula is Manhattan Distance.\n",
    "* p = 2: The formula is Euclidean Distance\n",
    "* p = ∞: Supremum Distance\n",
    "\n",
    "### Arghhhh Math! \n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/minkowski.png)\n",
    "When you see formulas like this you have\n",
    "several options. One option is to see the formula-->\n",
    "brain neurons fire that say math formula-->and then\n",
    "you quickly skip over it to the next English bit. I\n",
    "have to admit that I was once a skipper. The other\n",
    "option is to see the formula, pause, and dissect it. \n",
    "\n",
    "Many times you’ll find the formula quite understandable. Let’s dissect it now. When p = 1 the\n",
    "formula reduces to Manhattan Distance:\n",
    "\n",
    "$$d(x,y) =(\\sum_{k=1}^{n}{|x_k-y_k|^{1})^{1\\over{1}}} = \\sum_{k=1}^{n}{|x_k-y_k|} $$\n",
    "\n",
    "Let us say that *x* and *y* are athletes with the weight and height:\n",
    "\n",
    "```\n",
    "x = [148, 68]\n",
    "y = [144, 71]\n",
    "```\n",
    "\n",
    "Then the Manhattan distance is\n",
    "\n",
    "$$d(x, y) = \\sum_{k=1}^{n}{|x_k-y_k|} = |148 - 144| + |68 - 71| = 4 + 3 = 7$$\n",
    "\n",
    "The Euclidean distance is \n",
    "\n",
    "$$d(x,y) =(\\sum_{k=1}^{n}{|x_k-y_k|^{2})^{1\\over{2}}} = {(|148 - 144|^2 + |68-71|^2)}^{1\\over2} = {(4^2+3^2)}^{1\\over2} = {(16+9)}^{1\\over2} = 25^{1\\over2} = 5$$\n",
    "\n",
    "\n",
    "> Here is the scoop. The greater the r, the more a large difference in\n",
    "one dimension will influence the total difference.\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "### Ok, onward to coding!\n",
    "\n",
    "First, Let us import the kNN algorithm and make an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya90oNiSEx8D",
    "outputId": "13878899-6383-44b7-ca2f-5fdb0d9dcbf1"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm7rrfppEx8G"
   },
   "source": [
    "The \n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "makes an instance of a k nearest neighbor classifier with k=3.\n",
    "\n",
    "As the name of the method suggests, the line\n",
    "\n",
    "    knn.get_params()\n",
    "    \n",
    "displays the parameters of the classifier. In this case:\n",
    "\n",
    "```\n",
    "{'algorithm': 'auto',\n",
    " 'leaf_size': 30,\n",
    " 'metric': 'minkowski',\n",
    " 'metric_params': None,\n",
    " 'n_jobs': None,\n",
    " 'n_neighbors': 3,\n",
    " 'p': 2,\n",
    " 'weights': 'uniform'}\n",
    " ```\n",
    " \n",
    "I won't explain all the parameters now, but notice that the metric is Minkowski, and the power or `p` of Minkowski is 2 making it Euclidean distance. The number of neighbors `n_neighbors` is 3. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Most classifiers in sklearn want the labels (the thing we are trying to predict) to be a separate parameter from the features. Fortunately, we already created these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZLwsJdSEx8M"
   },
   "source": [
    "Again, the features are a Pandas DataFrame and the labels are a Pandas Series.\n",
    "\n",
    "We train the knn classifier we created by using the `fit` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sItP_6OEx8M",
    "outputId": "c3766df6-928f-49a3-dbfb-9cffe5c873ec"
   },
   "outputs": [],
   "source": [
    "knn.fit(athlete_features_train, athlete_label_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGXZQ_HzEx8P"
   },
   "source": [
    "Most machine learning (ML) algorithms build a model. From the examples in the training set, they build an internal representation, a model, of the relationship between the features and the labels. Once the algorithm builds this model it 'forgets' the individual data instances. Such ML algorithms are called eager learners. On the other hand, a lazy learner does not build a model beforehand. It remembers all the training instances and when it needs to make a prediction on a new instance it then processes all the training data. \n",
    "\n",
    "kNN is a lazy learner. During the learning phase, during `fit` it simply remembers the data (it remembers that the first athlete has a height of 72\tand a weight of 165 and the second has a height of 72 and a weight of 168).\n",
    "In the traditional kNN that we might through by hand, when we want to make a prediction, we calculate the distance between that new instance and every instance in our training set (how close is this new instance to the first example? To the second? And to the 98 other athletes in our small dataset. As you can imagine, this takes some time. So fitting is fast because it doesn't actually build a model but predicting is slow.\n",
    "\n",
    "### Three kNN algorithms\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/kdtree.png)\n",
    "There are three basic kNN algorithms: \n",
    "\n",
    "* **brute** which uses a brute force method we described above.\n",
    "* **[kd_tree](https://en.wikipedia.org/wiki/K-d_tree)** which, when you are fitting, creates a binary tree. This makes prediction faster. \n",
    "* **[ball_tree](https://en.wikipedia.org/wiki/Ball_tree)** which also creates a binary tree during training (fitting)\n",
    "\n",
    "Since this is our first machine learning algorithm, we won't get too bogged down in learning about kd trees and ball trees, but let's cover a few things. First, since the algorithm needs to construct a binary tree when it fits the data, fitting takes longer but with that binary tree, prediction is faster. That is the trade off. For both trees, a binary tree is constructed that divides the training data into a number of sets that have a specific size limit known as leaf size, which is a hyperparameter of the algorithm. When we specify a large leaf size, the depth of the constructed binary tree will be shallow and the amount of time constructing the tree will be reduced. When we specify a smaller leaf size, training time will be increased but predictions will be faster. \n",
    "\n",
    "When we did `knn.get_params()` we saw\n",
    "\n",
    "```\n",
    "{'algorithm': 'auto',\n",
    " 'leaf_size': 30,\n",
    " 'metric': 'minkowski',\n",
    " 'metric_params': None,\n",
    " 'n_jobs': None,\n",
    " 'n_neighbors': 3,\n",
    " 'p': 2,\n",
    " 'weights': 'uniform'}\n",
    "```\n",
    "\n",
    "The first line:\n",
    "\n",
    "```\n",
    "'algorithm': 'auto',\n",
    "```\n",
    "\n",
    "as you can guess, displays the algorithm used. The options are:\n",
    "\n",
    "* `brute`\n",
    "* `kd_tree`\n",
    "* `ball_tree`\n",
    "* `auto`\n",
    "\n",
    "The default is `auto` which simply allows the algorithm itself to determine the best algorithm to use based on the training data.\n",
    "\n",
    "We can specify the algorithm by using the `algorithm` hyperparameter:\n",
    "\n",
    "```\n",
    "knn = KNeighborsClassifier(algorithm='brute', n_neighbors=3)\n",
    "```\n",
    "\n",
    "The next line\n",
    "\n",
    "```\n",
    "'leaf_size': 30,\n",
    "```\n",
    "\n",
    "displays the leaf size which we just talked about. 30 is the default value. The optimal value is dependent on the problem. You can set it by\n",
    "\n",
    "\n",
    "```\n",
    "knn = KNeighborsClassifier(algorithm='brute', leaf_size=10, n_neighbors=3)\n",
    "```\n",
    "The next line\n",
    "```\n",
    "'metric': 'minkowski',\n",
    "```\n",
    "\n",
    "specifies the distance metric we are using. Minkowski is the default.  Its strength is in working with real values. If you are dealing with boolean or integer values other metrics may be a better fit.\n",
    "\n",
    "Finally,\n",
    "```\n",
    "'weights': 'uniform'}\n",
    "```\n",
    "\n",
    "As we know, when we use k=3, the three closest neighbors get a vote in determining the classification of the new instance. If 2 say 'Basketball' and one 'Gymnastics'  we classify the new instance as Basketball. In this case, all three neighbors have an equal, or uniform, vote. That's what this `'weights': 'uniform'` specifies. But suppose we want the closest neighbor to have more weight in the vote than the others. In that case we could use `weights='distance'`, where the weight of each vote is the inverse of the distance.\n",
    "\n",
    "\n",
    " This long description was intended to peel away some of the mystery, and not to bore or confuse you. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "\n",
    "\n",
    "### let's classify something\n",
    "\n",
    "Nneka Ogwumike is 6'2\" or 74 inches and weighs 174 and Leklani Mitchell is 5'5\" and 132 pounds. Let's see what our classifier predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8uRDJu9Ex8P",
    "outputId": "c3b10186-1963-4fe7-ae47-f208cb89eb2a"
   },
   "outputs": [],
   "source": [
    "sample_data = DataFrame({'height_in': [74, 65], 'weight_lb': [174,132]}, index=['Nneka Ogwumike', 'Leilani Mitchell'])\n",
    "\n",
    "sample_data.loc['Nneka Ogwumike']\n",
    "print(knn.predict(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjxAChNWEx8S"
   },
   "source": [
    "Ok, the first prediction is correct. Nneka Ogwumike is the 2016 MVP for the WNBA. Leilani is a professional basketball player but unfortunately our knn model classifies her as a gymnast.\n",
    "\n",
    "We can also ask the classifier the probability of these classifications: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7Pp-5cXEx8S",
    "outputId": "1e3f1de5-8666-438b-aeb5-f4eb30087f49"
   },
   "outputs": [],
   "source": [
    "print(knn.predict_proba(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDQ3jVmjEx8V"
   },
   "source": [
    "Ok. The probability that Nneka is a basketball player is 1.0 and the probabity that she is a gymnast is 0.0. How did we get that probability? Well `k` was 3 so we used the three nearest neighbors and all of them were basketball players. If 2 were basketball players and 1 a gymnast the probability would be 66.6%. There is no magic here.\n",
    "\n",
    "Cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks6xqdbmEx8X"
   },
   "source": [
    "Ok. For Leilani our classifer predicts gymnastics. But is only .66 confident. Why?  Because of Leilani's three closest neighbors, only 1 (33%) are basketball players and two were gymnasts.\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv_dcL4Qa8Hn"
   },
   "source": [
    "###Testing & accuracy\n",
    "Once we create and train a model, we want to know how good that model is. For that we use data the model hasn't seen during training (our test data). And the metric we often use is accuracy---what percentage of the test data did it classify correctly.\n",
    "\n",
    "Let's give it a try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9C590gUbXTy",
    "outputId": "1f3c03c5-8c28-40ab-acc7-ab5ee1fbeb3a"
   },
   "outputs": [],
   "source": [
    "# first make predictions on the test data\n",
    "predictions = knn.predict(athlete_features_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTbsItwgb9AE"
   },
   "source": [
    "What is our accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eB1uwMlwcBiK",
    "outputId": "392e7f6c-e93f-48a5-aaea-2f7c35e56f99"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(athlete_label_test, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTgUjQRIEx8Z"
   },
   "source": [
    "It is not surprising that with this simple task the algorithm is 100% accurate or at least close to that.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "###  Parameters\n",
    "The **parameters** of a model are things we can adjust. \n",
    "  \n",
    "For example, when we created the classifier and then used the `get_params` method, the classifier returned:\n",
    "\n",
    "```\n",
    "{'algorithm': 'auto',\n",
    " 'leaf_size': 30,\n",
    " 'metric': 'minkowski',\n",
    " 'metric_params': None,\n",
    " 'n_jobs': None,\n",
    " 'n_neighbors': 3,\n",
    " 'p': 2,\n",
    " 'weights': 'uniform'}           \n",
    "```\n",
    "\n",
    "These are all things we can adjust to try to improve performance. \n",
    "\n",
    "\n",
    "You see that it uses the Minkowski distance. The `p` parameter is the power parameter for Minkowski and you see the default value is 2. When p=1 Minkowski distance is the Manhattan Distance, when p=2 it is the Euclidean distance. And when we created the classifier we set `k`, the number of nearest neightbors for kNN to 3 (the default is 5).  So the three nearest neighbors 'vote' on the label to give that example. \n",
    "\n",
    "As we just discussed, for knn we have a number of parameters we can use to modify the classifier. For example, to build a Manhattan Distance kNN classifier with a k of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9XgA9EBEx8a",
    "outputId": "9a05bf2a-c4eb-4ab8-a9cc-d249fd96e051"
   },
   "outputs": [],
   "source": [
    "knnOne = KNeighborsClassifier(n_neighbors=1, p= 1)\n",
    "knnOne.fit(athlete_features_train, athlete_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lf33NjwfBJS",
    "outputId": "f75b5aaa-c467-479c-c7fb-0f4ddb102022"
   },
   "outputs": [],
   "source": [
    "predictions = knnOne.predict(athlete_features_test)\n",
    "accuracy_score(athlete_label_test, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUVy_lixEx8c"
   },
   "source": [
    "And using `sklearn`, it is quite easy to build and use a variety of classifiers. For example, although you probably know nothing about Guassian Naive Bayes classifiers, you can build one of those without even knowing much about the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfkdOOE6Ex8d",
    "outputId": "99a26873-3d56-4e0e-cbaa-2014276305c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(athlete_features_train, athlete_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwJD6_-XEx8f",
    "outputId": "3ea528ba-a22a-48f6-fa39-2dea0a99c7ae"
   },
   "outputs": [],
   "source": [
    "clf.predict(athlete_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isG_XoZ6Ex8h"
   },
   "source": [
    "So regardless of algorithm, the steps were\n",
    "\n",
    "1. create the classifier. For ex., `knn = KNeighborsClassifier()`\n",
    "2. fit the classifier. For ex., `knn.fit(athletes_features, athletes_labels)`\n",
    "3. used the classifier to make predictions on new data. For ex., `knn.predict([70])`\n",
    "\n",
    "\n",
    "Ok. back to kNN.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhKL632MEx8q"
   },
   "source": [
    "\n",
    "\n",
    "## the non-coding part of the notebook\n",
    "\n",
    "\n",
    "Let's go back to thinking about height and weight as features. Allyson Michelle Felix is among the fastest women on the planet (she won 6 Olympic Gold Medals). Her height is 5'5 and she weighs 121. Courtney Williams is a guard for the Connecticut Suns WNBA team. She is 5'8\" and weighs 136.  Here is the chart so you can see those numbers:\n",
    "\n",
    "\n",
    " person | height | weight\n",
    " :---: |  :---: | :---: \n",
    "Allyson Michelle Felix | 65 | 121\n",
    "Courtney Williams | 68 | 136\n",
    "\n",
    "\n",
    "Now I want to classify an athlete who is 5'4 and weighs 130. What is your gut feeling? Do you think she is a track person or a WNBA player?\n",
    "\n",
    "My thinking is that she is track since she seems too short for a basketball player (and plus I know that those are the stats for Carmelita 'The Jet' Jeter, the fastest women on the planet. (Although, the shortest person in the WNBA, Shannon Denise Bobbitt, is only 5'2\").  But if we classify someone who is 5'4\" by using the Manhattan Distance:\n",
    "\n",
    "    distance(Carmelita, Allyson) = abs(64 - 65) + abs(130 - 120) = 1 + 10 = 11\n",
    "    distance(Carmelita, Courtney) = abs(64 - 68) + abs(130 - 136) = 4 + 6 = 10\n",
    "    \n",
    "We'd pick that she was a basketball player (and we would still pick basketball even if we used Euclidean Distance.\n",
    "    \n",
    "\n",
    "So this is sort of a bummer.  It's the same problem that I mentioned in the kNN video. If I had a match making site and had this misguided idea that the best relationships are those people who are about the same age and have the same salaries. And I have 2 guys:\n",
    "\n",
    " Guys | age | salary\n",
    " ---: | :---: | :---: \n",
    " Mr. Cool | 26 | 80,000\n",
    " Old Dude | 67 | 115,000\n",
    " \n",
    " And I am trying to match up Ann who is 28 and earns 100k. \n",
    " \n",
    " The Manhattan Distance between Ann and Mr. Cool is 2 + 20k = 20,002.\n",
    " \n",
    " The Manhattan Distance between Ann and Old Dude is 31 + 15k  = 15,031\n",
    " \n",
    " So, sadly, our algorithm would recommend the old dude to Ann. This is again a bummer. The problem in both examples is that the range of values in one column is far larger than the range in another column. In our case, the weight column values are much larger than the height values.\n",
    " \n",
    "### rescaling\n",
    "\n",
    "A solution to this problem is to rescale the values so the values in all columns range from 0 to 1. There are other (and possibly better) ways to rescale but let's start with this simple one for now.\n",
    "\n",
    "##### the formula for minmax rescaling:\n",
    "\n",
    "### $$x'= \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Let's look at our simple example of Ann, Mr. Cool and Old Dude:\n",
    "\n",
    "Person | Age | Salary\n",
    " ---: | :---: | :---: \n",
    " Mr. Cool | 26 | 80,000\n",
    " Old Dude | 67 | 115,000\n",
    " Ann   | 28 | 100,000\n",
    "\n",
    "so the minimum value of the age column is 26 and the max is 67 and let's say I want to normalize Mr. Cool's age:\n",
    "\n",
    "\n",
    "### $$x'_{Mr.Cool}= \\frac{x_{Mr.Cool}-x_{min}}{x_{max}-x_{min}} = \\frac{26-26}{67-26} = \\frac{0}{41} = 0$$\n",
    "\n",
    "Ann's normalized age:\n",
    "\n",
    "### $$x'_{Ann}= \\frac{x_{Ann}-x_{min}}{x_{max}-x_{min}} = \\frac{28-26}{67-26} = \\frac{2}{41} = 0.048$$\n",
    "\n",
    "Old Dude's normalized age:\n",
    "\n",
    "### $$x'_{OldDude}= \\frac{x_{OldDude}-x_{min}}{x_{max}-x_{min}} = \\frac{67-26}{67-26} = \\frac{41}{41} = 1$$\n",
    "\n",
    "\n",
    "### Normalize Salary\n",
    "Can you normalize the values in the salary column?\n",
    "\n",
    "Double click this cell, enter the data, and shift-enter to render this markdown cell\n",
    "\n",
    "Person | Age | Salary\n",
    " ---: | :---: | :---: \n",
    " Mr. Cool | 0 | 80,000\n",
    "Old Dude | 0.048 | 115,000\n",
    " Ann   | 1 | 100,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iam-CVSkEx8q"
   },
   "source": [
    "## It's pretty easy to do this in straight Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10H_1MmEEx8q",
    "outputId": "a7cb9134-58c2-4f32-e55e-b072a378bcb0"
   },
   "outputs": [],
   "source": [
    "age = [26, 67, 28]\n",
    "\n",
    "def scale(arr):\n",
    "    return [(x - min(arr))/ (max(arr) - min(arr)) for x in arr]\n",
    "\n",
    "scale(age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NbIYqsnEx8s"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "## Using the min-max scale method in sklearn\n",
    "It's even easier to do it for pandas DataFrames. \n",
    "First, let's make a dataframe from the data we have been using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "cmCvI5AoEx8s",
    "outputId": "b1c6e179-32b1-4d75-eb4a-9c0724367f90"
   },
   "outputs": [],
   "source": [
    "simple = DataFrame({'age': [26, 67, 28], 'salary': [80000, 115000, 100000]}, index=['Mr. Cool', 'Old Dude', 'Ann'])\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX-FKQQ-Ex8u"
   },
   "source": [
    "ok. and now let's scale those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "qIOT4l71Ex8v",
    "outputId": "fb50f2b1-733a-4e7c-88f4-2ae0aee4b0ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "simple[['age', 'salary']] =  scaler.fit_transform(simple[['age', 'salary']] )\n",
    "simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLeXXOTiho8M"
   },
   "source": [
    "Cool.  Now when I try to find the Manhattan distance from Ann to both Mr. Cool and Old Dude I get:\n",
    "\n",
    "###   $$distance_{Ann,Mr.Cool} = \\left|.048 - 0.0\\right| + \\left|0.57-0\\right| = .048 + 0.57 = 0.618$$\n",
    "\n",
    "###   $$distance_{Ann,OldDude} = \\left|.048 - 1.0\\right| + \\left|0.57-1\\right| = .952 + 0.57 = 1.32$$\n",
    "\n",
    "#### Now, fortunately, Ann is closer to Mr. Cool!\n",
    "\n",
    "## Rugby\n",
    "Let's go back to the original dataset with the Rugby players along with basketball and gymnastics. \n",
    "\n",
    "Just to remind ourselves, that DataFrame was called `athletes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "n16CLBSliXpx",
    "outputId": "dec776a2-4311-4685-ec16-522d072dade7"
   },
   "outputs": [],
   "source": [
    "athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eDbRBQars9k"
   },
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfmCGqV2ipWB"
   },
   "source": [
    "We can create a new DataFrame, `athletes_normalized` that contains the normalized height and weight columns. We will first create a copy of the `athletes` DataFrame using `copy()` and then normalize the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "jvqX0amQiyLU",
    "outputId": "6755cf59-491f-424b-d562-8a3711060149"
   },
   "outputs": [],
   "source": [
    "athletes_normalized = athletes.copy()\n",
    "scaler = MinMaxScaler()\n",
    "athletes_normalized[['height_in', 'weight_lb']] =  scaler.fit_transform(athletes_normalized[['height_in', 'weight_lb']] )\n",
    "athletes_normalized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcsGJ9YNjaS2"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font> \n",
    "\n",
    "## <font color='#EE4C2C'>1. Comparing normalized vs raw data</font> \n",
    "Using knns with Euclidean distance and 3 nearest neighbors, compare the performance of a knn trained with the raw data vs. a knn trained with the normalized data.\n",
    "\n",
    "First, we need to divide the DataFrames as we did before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "yqMBpk8BjyrD",
    "outputId": "d6089502-07a7-4e2f-e3c5-bd2319a76b25"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TItX-TcWtP-3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJmQZpV4tBCT"
   },
   "source": [
    "### Now the actual building, fitting and testing of a knn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvqLFLQKwYjM",
    "outputId": "cbb939df-d5d2-46b2-d891-f540df1325b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtiMXLeEsyga",
    "outputId": "d11fa16c-9450-4bdd-b5f5-eac9a693b8cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDO6ebLrxcR7"
   },
   "source": [
    "### Results\n",
    "Please describe the results of your experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_zMDcckEx8w"
   },
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. The Iris Dataset</font> \n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/irises.png)\n",
    "What is the accuracy of your new model with one epoch of training?\n",
    "\n",
    "We are going to use the Iris Dataset, one of the standard data mining data sets which has been around since 1988.  The data set contains 3 classes of 50 instances each\n",
    "\n",
    "1. Iris Setosa \n",
    "2. Iris Versicolour \n",
    "3. Iris Virginica \n",
    "\n",
    "There are only 4 attributes or features:\n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "Here is an example of the data:\n",
    "\n",
    "Sepal Length|Sepal Width|Petal Length|Petal Width|Class\n",
    ":--: | :--: |:--: |:--: |:--: \n",
    "5.3|3.7|1.5|0.2|Iris-setosa\n",
    "5.0|3.3|1.4|0.2|Iris-setosa\n",
    "5.0|2.0|3.5|1.0|Iris-versicolor\n",
    "5.9|3.0|4.2|1.5|Iris-versicolor\n",
    "6.3|3.4|5.6|2.4|Iris-virginica\n",
    "6.4|3.1|5.5|1.8|Iris-virginica\n",
    "\n",
    "The job of the classifier is to determine the class of an instance (the type of Iris) based on the values of the attributes.\n",
    "\n",
    "The dataset is available at\n",
    "\n",
    "    https://raw.githubusercontent.com/zacharski/ml-class/master/data/irisTrain.csv\n",
    "    \n",
    "When you divide into training and test sets please use `random_state=0` so we can compare results. \n",
    "\n",
    "You should include a short paragraph describing your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "R6fjnhmSEx8x",
    "outputId": "69474383-4f36-4f31-9d29-be723c17104d",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKKOm1Vf0eAf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6pOzXcV0eIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtVpZ-tB0ePB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08SwSbUc0emC"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Intro-to-sklearn-2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
