{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnUQs11DflMR"
   },
   "source": [
    "# A quickstart introduction\n",
    "\n",
    "## An example ...\n",
    "As part of a [conservation effort](http://burrowingowlconservation.org/sightings/), Ann would like to report sightings of Burrowing Owls as she is hiking. Unfortunately, Ann doesn't know what a Burrowing Owl looks like so she goes to the web to look at pictures. What she has then, is a set of images that are labeled, meaning that she knows what each image is a picture of. By examining these labeled images she is **training** herself to recognize Burrowing Owls.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/owls2.png)\n",
    "\n",
    "\n",
    "More generally, we can call this set of labeled images used for training, the **labeled training dataset**. Let's dive into this idea of a labeled training dataset a bit more. Suppose Clara is given the task of distinguishing between pictures of telecaster style guitars and stratocaster. But not to worry, because her boss has given her thousands of pictures of guitars. When looking at a picture, the only thing Clara knows is that it is of either a stratocaster or a telecaster. For example, here are some pictures of stratocasters and telecasters.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/guitars2.png)\n",
    "\n",
    "\n",
    "Again, when looking at a picture all she knows is that it is a picture of a stratocaster or telecaster but she doesn't know which of the two it is. How long will it take for Clara to learn how to distinguish between these two guitar styles? Would the time be significantly shorter if her boss gave her 10,000 pictures or 100,000? If this is all the information she gets, she will never learn. What she needs is a **labeled** dataset. When presented with a picture she needs to know whether it is a picture of a stratocaster or a telecaster.\n",
    "\n",
    "\n",
    "**Now back to Ann learning to recognize Burrowing Owls**\n",
    "\n",
    "When Ann is learning to recognize Burrowing Owls from her labeled training set, she is developing a model of what features make it a Burrowing Owl. One such feature is the bright yellow eyes. Once she is done learning she can be on a hike, see an animal, and classify it as a Burrowing Owl or something else. This is an **inference** process---based on the evidence of different features of the animal she can infer what it is. And to throw more jargon at you, this type of problem is called a **classification problem**. In classification problems the system is given the features of an object and it needs to classify that object. For example,\n",
    "\n",
    "* The features might be the words of a Twitter post (i.e., *Everything Everywhere All At Once was a f-ing masterpiece. I can't emphasize how great this movie is, it's just that great.*) and based on those features the system classifies the post as positve, negative, or neutral.\n",
    "* The features might be the pixels of an image and based on those pixels the system classifies the image into one of 1,000 categories (it's an image of an owl, a bicycle).\n",
    "\n",
    "\n",
    "\n",
    "## machine learning\n",
    "In machine learning, classification systems have a similar two step process. First is the training phase where the system uses a labeled training dataset to build a model. (We will learn about the architecture of these models and how they learn a bit later.)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/quickDiagram2.png)\n",
    "\n",
    "\n",
    "\n",
    "> **Supervised vs. Unsupervised learning**. When a machine learning system trains on labeled data this is called supervised learning. When a system learns with unlabeled data this is called unsupervised learning. A very common example of unsupervised learning is clustering where we might give our system a million unlabeled pictures and ask it to divide the images into 10 groups.\n",
    "\n",
    "Once we have a trained model, we can use that model for inference---we can give it pictures and the system can classify them as burrowing owl or something else. Again, the two phases are:\n",
    "\n",
    "1. training\n",
    "2. inference\n",
    "\n",
    "In this introduction we are going to ignore the training phase and learn a bit about inference. To do that we are going to use a pre-trained model. What *pre-trained model* simply means is that someone else designed the architecture of the model and trained it.\n",
    "\n",
    "### VGG16\n",
    "\n",
    "The pretrained model we will use is VGG16, which was designed by Kar√©n Simonyan & Andrew Zisserman. In 2014, it won a competition where the competing systems had to classify objects in images into one of 1,000 categories and then locate where that object is located in the image by drawing a bounding box around the object. As shown below.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/boundingbox.png)\n",
    "\n",
    "VGG16 beat out GoogLeNet in that competition.  Since then there are dozens of systems that perform better, but we will use VGG16 because it runs well on a Colab instance. The pretrained VGG16 was trained on ImageNet, a labeled training dataset of over one million images.\n",
    "\n",
    "## Let's get started\n",
    "\n",
    "#### First, a note ...\n",
    "The intent of this notebook is for you to learn a little bit about data mining and have a bit of fun. The idea is not for you to understand every line of code. That will come later.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "**First let's set the runtime to GPU (Graphics Processing Unit) -- click on 'runtime' in the menu above, select 'Change runtime type' and pick one of the 'GPU' options.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbSyRnWYflMY"
   },
   "source": [
    "### 0. TensorFlow and Keras\n",
    "TensorFlow and Keras are already available in Google Colab so there is nothing special to install.\n",
    "\n",
    "### 1. Load the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pm_bhhMjflMY",
    "outputId": "83d613ab-7087-4d0f-d2ea-96412c101e6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7vaBQx2flMa"
   },
   "source": [
    "The `import` command loads a library module into Python. For example,\n",
    "\n",
    "```import tensorflow```\n",
    "\n",
    "will import the Python tensorflow library, meaning that all the code in that module will be available to your program. Sometimes you only need part of that module. To import part of the code from a module one can use the `from ... import` command. For example,\n",
    "\n",
    "```from keras.models import Sequential```\n",
    "\n",
    "will import the code from the `Sequential` submodule from `models` which itself is a submodule of `keras`.\n",
    "\n",
    "Now back to our code above. The first line:\n",
    "\n",
    "```from keras.applications import VGG16```\n",
    "\n",
    "imports the code associated with VGG16.\n",
    "\n",
    "The next line:\n",
    "\n",
    "```model = VGG16(weights='imagenet', include_top=True)```\n",
    "\n",
    "creates an instance of the VGG16 model. We are going to use the model that has been pretrained on the ImageNet dataset (`weights= 'imagenet`) and we want to use the same types of object names used in ImageNet (`include_top=True`)\n",
    "\n",
    "\n",
    "\n",
    "#### Pretrained models\n",
    "There are many pretrained models available to use in Keras ([around 40 at the time of this writing](https://keras.io/api/applications/#available-models))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yvDAoqNflMc"
   },
   "source": [
    "Of course we could call this model anything we want:\n",
    "\n",
    "```\n",
    "vgg = VGG16(weights='imagenet', include_top=True)\n",
    "myModel = VGG16(weights='imagenet', include_top=True)\n",
    "```\n",
    "\n",
    "Now we have a pretrained model loaded into our system. We would like to use the model to classify our own images.\n",
    "\n",
    "## Inference\n",
    "\n",
    "We would like to give VGG16 an image like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/poodle.jpg)\n",
    "\n",
    "and have VGG16 classify it. First, let's download the image file from the web using the Unix command `curl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXZzt6a6flMc",
    "outputId": "a6ac88a2-ab3c-4f65-dd28-20eca28a8b18"
   },
   "outputs": [],
   "source": [
    "!curl http://zacharski.org/files/courses/dmpics/poodle.jpg -o poodle.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umQSlZbRflMd"
   },
   "source": [
    "The exclamation point (aka *bang*) at the beginning of the line instructs the system to interpret the rest of the line as a Unix command (something you might type in a Unix terminal).\n",
    "\n",
    "For example\n",
    "\n",
    "```!ls```\n",
    "\n",
    "will list the contents of the current directory. If you want more information about the `curl` command you can type\n",
    "\n",
    "```!man curl```\n",
    "\n",
    "Next, let's load that image into Python using PIL (Python Imaging Library). This step isn't necessary for our model to recognize the image, but it is useful for us to see the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nI-yVPYflMd"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"poodle.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OTUlDW-flMd"
   },
   "source": [
    "Now let's display that image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3Mny1F_BflMe",
    "outputId": "4c4c217d-b9ef-47da-8364-8808047424d4"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSdD1WZQflMe"
   },
   "source": [
    "(If that doesn't display an image change the code to `img.show`)\n",
    "\n",
    "\n",
    "The size of this particular image is 4032x3024 which is slightly over 12 million pixels. That is a lot of pixels! VGG16 was designed to work with an image size of 224x224. So we need to transform the original image to VGG16 specifications.  Since the image isn't square but bigger in width than height, if we just resized it to 224x224 the image would look squished:\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/ghost103.png)\n",
    "\n",
    "To avoid this let's do two things:\n",
    "\n",
    "First, let's resize the image so that the smaller dimension of the original image will be resized to 224.  For this image the smaller dimension is the height. That will look like:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/ghost101.png)\n",
    "\n",
    "Then we will crop the image, removing the left and right sections of the photo:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/ghost102.png)\n",
    "\n",
    "\n",
    "This all can be done by one Keras command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rmI2eXBflMe"
   },
   "outputs": [],
   "source": [
    "from keras.utils import load_img\n",
    "img = load_img(\"poodle.jpg\", target_size=(224, 224), keep_aspect_ratio=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3RIn53EU2Qe"
   },
   "source": [
    "The resulting image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "2LNx57T1Un7B",
    "outputId": "e8c5ec3f-2061-442f-a84f-26b09a55e2f0"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQQKbYRLVIWs"
   },
   "source": [
    "The resulting size of the image is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axqLYw-QVG29",
    "outputId": "7ba474ea-3dc2-40be-bcbf-305fe4b79052"
   },
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnfdmQzQXhMb"
   },
   "source": [
    "That looks fantastic!\n",
    "\n",
    "\n",
    "\n",
    "Finally let's convert the image to an array and preprocess the array into a form expected by VGG16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqOaj5mzXf4o",
    "outputId": "6a552dfe-7422-4f18-bd8e-6ed24683eec6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "input_img = image.img_to_array(img)             # convert the image into an array.\n",
    "                                                # The array represents a single image\n",
    "input_img = np.expand_dims(input_img, axis= 0)  # our model processes an array of images, not just\n",
    "                                                # a single one. So add a dimension to the array\n",
    "input_img = preprocess_input(input_img)         # finally, transform the values to those needed by vgg16\n",
    "input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUHn9foEflMf"
   },
   "source": [
    "\n",
    "Now we have prepared the image and are prepared to pass it to our vgg16 model for inference.\n",
    "\n",
    "### Model Inference\n",
    "\n",
    "In deep learning models we can classify an image using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07sjKJ6CflMf",
    "outputId": "1235b76f-d390-4353-ea87-9107675e7483"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(input_img)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQ3vQ0PFZw97",
    "outputId": "2cd921da-c534-4f88-c93a-645892a0bc66"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiH6cKiaflMh"
   },
   "source": [
    "As we see `predictions` is a two dimensional tensor with one row representing the single image we used as input and 1,000 different values for that image. We get 1,000 values because ImageNet contained 1,000 classes for the images. The larger the number the more likely the image is of that class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP-kvbkmflMh"
   },
   "source": [
    "As you can see, the image is not very likely to be one of the first 5 labels as they are very small numbers:\n",
    "\n",
    "```1.86004166e-14, 5.63008062e-14, 1.81835883e-13, 3.05401185e-14, 1.17410248e-11```\n",
    "\n",
    "Those represent the probabilities of\n",
    "\n",
    "```['tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead']```\n",
    "\n",
    " It is good to know our model didn't think our image was of a fish. What does our model think?\n",
    "\n",
    "\n",
    "Let's find out the top 5 predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cNIQ0UGol7L",
    "outputId": "c2c4614d-ad8c-46ab-d338-7e5f47599d98"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "top5  = decode_predictions(predictions, top=5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHH57CliflMj"
   },
   "source": [
    "VGG16 is over 86% certain that the image is of a standard poodle.\n",
    "\n",
    "Pretty impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dEP9tLjflMk"
   },
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's combine all the code above into several contiguous codeblocks without the intervening explanations.  \n",
    "\n",
    "First we will put all the import statements at the start of the code, much like we would do in other programming languages.\n",
    "\n",
    "Next we will create an instance of the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOHspUSIfPAG"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.utils import load_img\n",
    "import requests\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJsEF-mUfPhK"
   },
   "source": [
    "Let's turn what we learned into a function and try predicting the class of other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzSJzZ3ic5CI"
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform(img_path):\n",
    "  \"Transform the image to a 224 x 224 array\"\n",
    "  targetdim = 224\n",
    "  ## load image and crop\n",
    "  img = load_img(img_path, target_size=(targetdim, targetdim), keep_aspect_ratio=True)\n",
    "\n",
    "  # convert to array\n",
    "  input_img = image.img_to_array(img)\n",
    "  input_img = np.expand_dims(input_img, axis= 0)\n",
    "  input_img = preprocess_input(input_img)\n",
    "  return input_img\n",
    "\n",
    "\n",
    "def predict(url):\n",
    "    # first download the image from the web\n",
    "    r = requests.get(url)\n",
    "    with open('tmp.jpg', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    img_t = transform('tmp.jpg')\n",
    "    predictions = model.predict(img_t)\n",
    "    return (decode_predictions(predictions, top=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl1pLtjlpCCo"
   },
   "source": [
    "Let's see what if our model will correctly recognize a violin.\n",
    "\n",
    "![](https://cdn.shoplightspeed.com/shops/629006/files/23585410/430x510x3/yamaha-yamaha-4-4-braviol-av10-intermediate-violin.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Na60Wr3ieiex",
    "outputId": "e32d99b5-b69e-4f17-891d-1bf0f8a69470"
   },
   "outputs": [],
   "source": [
    "predict( 'https://cdn.shoplightspeed.com/shops/629006/files/23585410/430x510x3/yamaha-yamaha-4-4-braviol-av10-intermediate-violin.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbXBwIfxflMk"
   },
   "source": [
    "Around 93% certain it is a image of a violin.\n",
    "So far so good!\n",
    "\n",
    "Let's find out what the 1,000 labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCsSr4vjflMl",
    "outputId": "1bf2843c-6436-4137-a086-749ae3df34d2"
   },
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/imagenet_classes.txt -o imagenet_classes.txt\n",
    "\n",
    "with open('imagenet_classes.txt') as labelfile:\n",
    "  i = 0\n",
    "  line = ''\n",
    "  labels = labelfile.readlines()\n",
    "  for label in labels:\n",
    "\n",
    "    #print(\"HERE\")\n",
    "    line += '%-25s' %labels[i].strip()\n",
    "    if (i + 1) % 4 == 0:\n",
    "      print(line)\n",
    "      line = ''\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYkTXbujflMl"
   },
   "source": [
    "Electric Guitar is one of the labels. Let's see if our model correctly identifies a picture of one:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/telecaster.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUjOor_oflMl",
    "outputId": "5dd82168-61a5-46b8-d870-c28d37bc0f93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict('https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/telecaster.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8msErztflMl"
   },
   "source": [
    "Our system is nearly 75% sure that this image is an electric guitar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCLaql64flMm"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font>\n",
    "Ok, it is time for you to try out what you just learned.\n",
    "\n",
    "## <font color='#EE4C2C'>1. Your own images</font>\n",
    "Try this out on three images of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GanSdKhSflMn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoHaOAiZflMn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhck8yneflMn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbFQ20DUflMn"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Xception</font>\n",
    "\n",
    "Let's try a different pretrained model, Xception. [Here is the documentation page for it.](https://keras.io/api/applications/xception/) Load the model, construct a function that will make predictions based on the model and try it out on the images above that we provided plus your three.\n",
    "\n",
    "**Here are a few things to note:**\n",
    "\n",
    "1. VGG16 has an input size of 224x224x3.  Xception has a different input size and you will need to make that change to `transform`\n",
    "2. VGG16 used `keras.applications.vgg16.preprocess_input`. Xception uses a different `preprocess_input. You will need to make that change as well.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wxkMyuAflMo",
    "outputId": "3f5038de-e0a6-4ac5-f89b-72bb1272114b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oglJGX8flMo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05iaa3rhflMo",
    "outputId": "0476295f-b605-48e5-8728-ad53e031ff0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAgCjkvwflMo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqdx9MG8flMo"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "# <font color='SlateBlue'>Summary</font>\n",
    "We learned some critical terms in machine learning, including\n",
    "1. supervised and unsupervised learning systems\n",
    "2. labeled data\n",
    "3. inference\n",
    "4. pretrained models\n",
    "\n",
    "We alse gained some experience working with Colab, and Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0aK-Aw_flMo"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
