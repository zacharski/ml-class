{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHAxGRCXb7X_",
    "outputId": "80a54920-80c7-499b-fbc4-4c1c670614c5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omp4N9HXb-b4"
   },
   "source": [
    "\n",
    "# Deep Learning for Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) covers machine learning techniques dealing with text and includes\n",
    "\n",
    "* classification\n",
    "  * sentiment analyis (is this tweet a Pro-Biden or Anti-Biden one)\n",
    "  * stylometrics (was this typed suicide note really from the deceased or did the murderer write it [1](https://www.rosette.com/case-studies/alias/), [2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107011/)\n",
    "  * general classification (out of 40 topics, which is this article about)\n",
    "* question answering (building systems that can answer questions -- *What is the best treatment for hemangiosarcoma in dogs?*)\n",
    "* machine translation\n",
    "* speech recognition\n",
    "\n",
    "among many others. Deep Learning has led to tremendous improvements in all these areas of NLP. \n",
    "\n",
    "In this notebook, we are going to examine classification systems for textual information.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm5JKEGBvFZH"
   },
   "source": [
    "\n",
    "## Analyzing and Classifying Text\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/tiles.jpg\" width=\"500\"/>\n",
    "\n",
    "So far we have been dealing with **structured data**. Structured data is ... well ... structured. This means that an instance of our data has nice attributes that can be represented in a DataFrame or a table:\n",
    "\n",
    "make | mpg | cylinders | HP | 0-60 |\n",
    "---- | :---: | :---: | :---: | :---: |\n",
    "Fiat | 38 | 4 | 157   | 6.9 \n",
    "Ford F150 | 19 | 6 | 386 | 6.3 \n",
    "Mazda 3 | 37 | 4 | 155 |  7.5 \n",
    "Ford Escape | 27 | 4 | 245 | 7.1 \n",
    "Kia Soul | 31 | 4 | 164 | 8.5 \n",
    "\n",
    "The majority of data in the world is **unstructured**. Take text for example. Suppose I have a corpus of twitter posts from former president Donald Trump and the Dalai Lama and my goal is to create a classifier that takes a tweet and tells me if it was produced by Trump or the Dalai Lama:\n",
    "\n",
    "*The purpose of education is to build a happier society, we need a more holistic approach that promotes the practice of love and compassion.*\n",
    "\n",
    "*How low has President Obama gone to tapp my phones during the very sacred election*\n",
    "\n",
    "We might consider  the columns of a table to be things like *first word of the tweet*, *second word of the tweet* and so on:\n",
    "\n",
    "\n",
    "id | word 1 | word 2 | word 3 | word 4 |word 5 |word 6 | ... |\n",
    "---- | :---: | :---: | :---: | :---: | :---: |:---: |:---: |\n",
    "1 | The | purpose | of   | education |is | to | ...\n",
    "2 | How | low | has |President | Obama | gone | ...\n",
    "\n",
    "So we would be counting how many times the word *President* occurred as the fourth word of a tweet. **But that would be the wrong way to go**. First, the deep learning models we have developed so far require input of a specific length. For example, we resized our dog and cat images to a uniform 150x150 and because each image was a uniform size we could specify the input shape of our network to be ....\n",
    "\n",
    "```\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "```\n",
    "\n",
    "But what should the input length be for tweets? Sometimes there are short, one-word, tweets like *nice*. The average length of a tweet is 30 characters so something like the six word *Today, Toaster is 10 years old.*  And the limit of course is 280 characters so...\n",
    "\n",
    "> Like anything else, life has a beginning and in due course must end. In between those two events the important goal should be to live meaningfully, not to create trouble for others. If we can do that, when the end comes, we can go feeling at peace.\n",
    "\n",
    "That's 47 words so maybe we can limit our input to 50 words and for shorter tweets we can pad them with blank words.  But there is another possibility ...\n",
    "\n",
    "### Bag of Words (bow)\n",
    "\n",
    "A more common way to represent text is to treat the text as an unordered set of words, which is called the **bag of words** approach. \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/BagofWords.jpg\" width=\"350\"/>\n",
    "\n",
    "With the bag of words approach we count word occurrences and the features (what we might think of as columns) are the unique words. For example, we have a collection of Trump and Dalai Lama tweets and indicate whether the word occurred in the tweet or not. So we might get something like:\n",
    "\n",
    "id | a | the | compassion | love |sad |fake | rigged | ... |\n",
    "---- | :---: | :---: | :---: | :---: | :---: |:---: |:---: | :---:\n",
    "Trump_1 | 1 | 0 | 0   | 0 |1 | 1 | 1 |...\n",
    "Trump_2 | 1 | 1 | 0   | 0 | 0 | 1 | 1 |...\n",
    "DalaiLama_1 | 1 | 1 | 1 |1 | 0 | 0 | 0 | ...\n",
    "\n",
    "So, for example, in DalaiLama_1, in the text there was\n",
    "\n",
    "* an occurrence of *a*\n",
    "* an occurrence of *the*\n",
    "* an occurrence of *compassion*\n",
    "* an occurrence of *love*\n",
    "\n",
    "We don't know what order the words occurred in, we just know what words occurred in the tweet. This is the bag-of-words method. \n",
    "\n",
    "Instead of short text snippets like tweets, let's say we are analyzing speeches of Trump and the Dalai Lama. Maybe then we will count how many times they used each word. So something like:\n",
    "\n",
    "id | a | the | compassion | love |sad |fake | rigged | ... |\n",
    "---- | :---: | :---: | :---: | :---: | :---: |:---: |:---: | :---:\n",
    "Trump_1 | 52 | 25 | 0   | 0 |21 | 82 | 19 |...\n",
    "Trump_2 | 30 | 35 | 0   | 0 | 5 | 20 | 31 |...\n",
    "DalaiLama_1 | 60 | 271 | 27 |63 | 12 | 0 | 0 | ...\n",
    "\n",
    "Of course we are still faced with how many columns to make, representing the vocabulary size. We could limit it to the 10,000 most common words, for example.\n",
    "\n",
    "Once we have the text in this format we can use the standard deep learning classification techniques we used before.\n",
    "\n",
    "\n",
    "Converting **unstructured** text to something **structured** is a multistep process. Let's learn the bits before putting it together. And we will start with the last step first-- creating the bag of words.\n",
    "\n",
    "### Import Keras ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-igfitKjuYru",
    "outputId": "2d9a4b62-4e43-4400-d5d1-f09ad7b29e3b"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1153Guw8wB_"
   },
   "source": [
    "### Some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1mf095J8u_0",
    "outputId": "659e5cbf-8465-4597-90bd-6afd574e5ea6"
   },
   "outputs": [],
   "source": [
    "trump1 = \"How low has President Obama gone to tapp my phones during the very sacred election process. This is Nixon/Watergate. Obama bad (or sick) guy! Sad\"\n",
    "trump2 = \"Our wonderful new Healthcare Bill is now out for review and negotiation. ObamaCare is a complete and total disaster - is imploding fast! Sad\"\n",
    "trump3 = \"Don't let the FAKE NEWS tell you that there is big infighting in the Trump Admin. We are getting along great, and getting major things done!\"\n",
    "trump4 = \"Russia talk is FAKE NEWS put out by the Dems, and played up by the media, in order to mask the big election defeat and the illegal leaks! Sad\"\n",
    "dalaiLama1 = \"The purpose of education is to build a happier society, we need a more holistic approach that promotes the practice of love and compassion.\"\n",
    "dalaiLama2 = \"Be a kind and compassionate person. This is the inner beauty that is a key factor to making a better world.\"\n",
    "dalaiLama3 = \"If our goal is a happier, more peaceful world in the future, only education will bring change.\"\n",
    "dalaiLama4 = \"Love and compassion are important, because they strengthen us. This is a source of hope\"\n",
    "tinyCorpus = [trump1, trump2, trump3, trump4, dalaiLama1, dalaiLama2, dalaiLama3, dalaiLama4]\n",
    "tinyCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-y3dlin89du"
   },
   "source": [
    "### Create the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrWO8riu80Bm",
    "outputId": "dcad2a63-16c7-427b-8f6c-0c09878a63f7"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=200)\n",
    "tokenizer.fit_on_texts(tinyCorpus)\n",
    "\n",
    "# Directly get the one-hot binary representations.\n",
    "# Note that other vectorization modes than one-hot encoding are supported!\n",
    "one_hot_results = tokenizer.texts_to_matrix(tinyCorpus, mode='binary')\n",
    "# let's look at an example of an encoding ...\n",
    "print(one_hot_results[0])\n",
    "\n",
    "\n",
    "# This is how you can recover the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p13Daqp__dfB"
   },
   "source": [
    "That was pretty easy. And now we have the texts in a form we can use for deep learning.\n",
    "\n",
    "Instead of the binary choice (a 1 if the word is present and a 0 if not) -- `mode='binary'` we can count how many occurrences of each word there were in the text by using `mode='count'`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgOPTdZf_sBy",
    "outputId": "ef45dab4-2b8d-4ee6-ce58-833bbf5dbc25"
   },
   "outputs": [],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(tinyCorpus, mode='count')\n",
    "# let's look at an example of an encoding ...\n",
    "print(one_hot_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skKDJALg_-Is"
   },
   "source": [
    "So in the first tweet:\n",
    "\n",
    "> 'Our wonderful new Healthcare Bill is now out for review and negotiation. ObamaCare is a complete and total disaster - is imploding fast! Sad',\n",
    "\n",
    "There were 3 occurrences of the word *is*, 2 of *and*, and so on as indicated in the first row above.\n",
    "\n",
    "How do we know what columns are associated with which words? We can use the word_index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwaZVV8fAMnL",
    "outputId": "88bc5626-d6e0-487b-d137-a94c62d4a588"
   },
   "outputs": [],
   "source": [
    "# tokenizer.word_index is a python dictionary containing the word as a key and the column as its value\n",
    "[(k, v) for k, v in sorted(tokenizer.word_index.items(), key=lambda item: item[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-F8umxI-JnV"
   },
   "source": [
    "So, *the, is, and, a, to* are the words represented by the first five columns.\n",
    "\n",
    "\n",
    "# TF-IDF representation\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bigTFIDF.png)\n",
    "So far we looked at \n",
    "\n",
    "* a binary bag-of-words (whether or not the word was present in the text).\n",
    "* a raw count bag-of-words (counting how many occurrences of each word)\n",
    "\n",
    "There are several other approaches\n",
    "We could represent a document as a bag of words and their probabilities (`mode=\"freq\"`). For example, in *Tom Sawyer* 4.6% of the words are *the* and 0.95% are *Tom*. But the word *the* probably occurs in most novels with that frequency. So in some sense the word *the* is uninteresting. On the other hand *Tom* probably occurs much more frequently in *Tom Sawyer* than it does in *Moby Dick* and, in that way, it is a more interesting word. One way to discount words that occur evenly throughout our document collection is to use TF-IDF.  \n",
    "\n",
    "* TF: Term Frequency - each word uprated by how often the word occurs in the document.\n",
    "* IDF Inverse Document Frequency - how often the word appears in the entire corpus\n",
    "\n",
    "\n",
    "\n",
    "> TF-IDF was first proposed by Karen Sparck Jones as a heuristic--not having any theoretical foundation. Researchers since then have tried to justify this metric by relating it to probablistic theories and information theories. But these attempts have been problematic. Nevertheless, as we will see, this heuristic works quite well. \n",
    "\n",
    "The formula is\n",
    "\n",
    "### $$ tfidf(t, d) = tf(t,d) \\times idf(t) $$\n",
    "\n",
    "where *t* is the term (the word) and *d* is the document.\n",
    "\n",
    "To explain this I will use some made up data--the word counts of 5 emails (and for the sake of later computations let's assume that each email is 100 words long):\n",
    "\n",
    "id | the | sad | compassion |  \n",
    "----: | :---: | :---: | :---:\n",
    "1 | 3 | 0 | 1 \n",
    "2 | 3 | 0 | 0 \n",
    "3 | 4 | 0 | 0 \n",
    "4 | 3 | 2 | 0 \n",
    "5 | 3 | 0 | 2\n",
    "\n",
    "\n",
    "The intuition is this. Even though the word *the* occurs frequently in each email, it is unlikely to help us classify email because it occurs in **every** email. The words *sad* and *compassion* are more interesting as they don't occur uniformly in our collection. \n",
    "\n",
    "#### TF\n",
    "\n",
    "The TF part of TF-IDF refers to how often the word occurs in the document. There are a number of ways to define TF. The simplist is to use the raw count.  So for example, the TF of *the* in document 1 is 3 (the word *the* occurred 3 times in document 1). One problem with this approach is that the raw count is influenced by the length of the document. So if you in your 1,000 word essay on Tom Sachs use 50 occurrences of *the* and Jane in her 90,000 word Zen van life mystery novel use 4,5000 occurrences of *the*, it doesn't mean that Jane is a bigger fan of *the* than you are. Even though there is that disparity in the raw counts it is not a characteristic that will help us distinguish texts about Zen from those about Tom Sachs.  In both the 1,000 word essay and the 90,000 word book about 5% of the words are *the*. A popular measure of TF is to divide the number of occurrences of a word by the total words in the document. So the TF of the word *the* in both your 1,000 word essay and my novel would be .05\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### IDF\n",
    "\n",
    "IDF is defined as:\n",
    "\n",
    "### $$ idf(t)=\\log\\frac{1+n_d}{1+df(d,t)}+ 1 $$\n",
    "\n",
    "$n_d$ is the total number of documents and $df(d,t)$ is how many documents the term *t* occurred in. \n",
    "\n",
    "So:\n",
    "\n",
    "### $$ idf(the)=\\log\\frac{1+5}{1+5}+ 1 =  1.5 $$\n",
    "\n",
    "### $$ idf(compassion)=\\log\\frac{1+5}{1+2}+ 1 = \\log{2} + 1 =  2 $$\n",
    "\n",
    "So, *the* in document 1 has a tf-idf of $.03 \\times 1.5 = 0.045$ and *compassion* has a tf-idf of $.01 \\times \\ 2 = 0.02$\n",
    "\n",
    "This is a fairly important concept to understand. I was asked about tf-idf in my oral exam to become a certified instructor at the Deep Learning Institute and fortunately I knew about it. \n",
    "\n",
    "It is also important to know that while it works well as a heuristic and has been around since the 70s, there really is no theoretical foundation to it. \n",
    "\n",
    "With all that as background, it is easy to convert a document collection into an array of TFIDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KVXgJ3R9Pmf",
    "outputId": "e6a97d7a-e7ae-40ba-bf61-27bbe8b56390"
   },
   "outputs": [],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(tinyCorpus, mode='tfidf')\n",
    "# let's look at an example of an encoding ...\n",
    "print(one_hot_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTRonUNXFZRj"
   },
   "source": [
    "Again, once we have this representation we can use the deep learning methods we already used.\n",
    "\n",
    "\n",
    "## An initial example - IMDB\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/imdb.png)\n",
    "\n",
    "The Internet Movie DataBase consists of 50,000 movie reviews and contains an equal number of positive and negative reviews. The task of identifying the affect of a text (whether it is postive or negative, or how strongly someone feels about the topic) is called **sentiment analysis**. \n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLDR_eiY1YCL",
    "outputId": "2caece9f-0f53-4db9-923d-efdfe45b5d2c"
   },
   "outputs": [],
   "source": [
    "!wget http://zacharski.org/files/courses/cs419/imdb.zip\n",
    "!unzip imdb.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLkosiEaRc2W",
    "outputId": "acfdd43e-1201-4841-d10d-8a23e6e9bf53"
   },
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAnWmyR3rgK6"
   },
   "source": [
    "The zip file only contained one file so we could have read it directly.  Since we already unzipped it let's use the unzipped version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "T0tpCqv_BkWF",
    "outputId": "f60ae953-53a3-468f-f995-8d490ff0640d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('imdb.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg97QeqW2KoT"
   },
   "source": [
    "Now let's separate the texts from the labels. Also note that the labels are the strings *positive* and *negative* so let's convert those to 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ql-GmhdXLShN",
    "outputId": "841cf803-746a-4da7-86c3-2cfa1f018761"
   },
   "outputs": [],
   "source": [
    "data_text = data.review\n",
    "data_label =data.sentiment\n",
    "data_label =  data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "data_label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXn9kHUb2cn1"
   },
   "source": [
    "### TFIDF\n",
    "Now we are going to convert the text represented as strings to a tfidf representation. \n",
    "\n",
    "Let's use the 5,000 most common words in the documents (`Tokenizer(num_words=5000)`)\n",
    "\n",
    "This will take a bit of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOjTzQdOJz-Z",
    "outputId": "65e7384f-b9fb-4021-fd00-ddde1ef0f417"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "\n",
    "# Directly get the one-hot binary representations.\n",
    "# Note that other vectorization modes than one-hot encoding are supported!\n",
    "one_hot_results = tokenizer.texts_to_matrix(data_text, mode='tfidf')\n",
    "# let's look at an example of an encoding ...\n",
    "print(one_hot_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dC35HHYgbUqW"
   },
   "source": [
    "###  Divide into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbPLee2_MJEd",
    "outputId": "5c62a776-565a-48c1-8da3-2623bd1033df"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "imdb_train_text, imdb_test_text, imdb_train_labels, imdb_test_labels = train_test_split(one_hot_results, data_label, test_size = 0.2, random_state=42)\n",
    "imdb_test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04NGZqtVdOpF"
   },
   "source": [
    "### Build a deep learning model\n",
    "Let's go with a basic, no frills, model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Lj35GJladKJs",
    "outputId": "dbe610b9-8f1c-4d28-f6d2-57263f58f627"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3BxwR6u3YnG"
   },
   "source": [
    "In our tokenizer we specified a vocabulary size of 5,000 words, so that is our `input_shape`. We are trying to predict a binary 1,0 classification so we need \n",
    "\n",
    "```\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "at the very end of our network. \n",
    "\n",
    "We would like a network with\n",
    "\n",
    "1. a dense layer with 512 nodes and input shape (5000,)\n",
    "2. a dense layer of 256\n",
    "3. A dense layer of 128\n",
    "4. A dense layer (the output layer) of 1 with the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Pb3pBO7dpS4"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(5000,)))\n",
    "network.add(layers.Dense(256, activation='relu'))\n",
    "network.add(layers.Dense(128, activation='relu'))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tqZ-8UO30wu"
   },
   "source": [
    "Again, we are predicting a binary 1,0 classification (was it a positive review or not) so we will use `binary_crossentropy` as our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNchu2tmerGA"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "network.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5PiezmrexYS",
    "outputId": "29f01ac4-6bf7-4656-b913-2659fa769c06"
   },
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTbirosf4JSO"
   },
   "source": [
    "### fitting to the data\n",
    "Now it is time to fit the network to the data. Let's use 20% of the data for validation and run for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEP-UYpIe2Vb",
    "outputId": "5e0a9e73-e81b-4592-885c-ee1f122faf9c"
   },
   "outputs": [],
   "source": [
    "history = network.fit(\n",
    "      imdb_train_text, imdb_train_labels,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_split=0.2,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1R-XCz-4gEN"
   },
   "source": [
    "### Our accuracy and loss\n",
    "Let's plot out both the training and validation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "x6oPJSbM4gW8",
    "outputId": "06113a1a-ed2a-46af-e497-12ed1e1b018e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiqOLIsJ5bg8"
   },
   "source": [
    "What is your interpretation of the plots?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "Okay, that definitely looks like overfitting. We will examine ways of ameliorating overfitting shortly.  \n",
    "\n",
    "Let's see how our network performs on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Byb8BGdL8Xtr",
    "outputId": "af9c881c-7cf7-48ae-8ea7-c649da6593dc"
   },
   "outputs": [],
   "source": [
    "scoreSeg = network.evaluate(imdb_test_text, imdb_test_labels)\n",
    "print(\"Accuracy: \", scoreSeg[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaPCNeGgrTdF"
   },
   "source": [
    "Not bad for our first attempt at text classification!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3Y8Ck5hCfYk"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>Climate Change ...</font> \n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/factfake.jpeg)\n",
    "\n",
    "On Twitter there are people who deny climate change:\n",
    "\n",
    "> I don't know about you guys, but I think climate change is -- as Lord Monckton said -- bullsh*t\n",
    "\n",
    "and people who believe it is real:\n",
    "\n",
    "> Millennials, and Gen z, and all these folks that come after us, are looking up and we’re like ‘the world will end in 12 years if we don’t address climate change, and your biggest issue is how are we gonna pay for it?\n",
    "\n",
    "We are going to investigate the sentiment of tweets from the Twitter Climate Change Sentiment Dataset compiled by Edward Qian. The dataset consists of 43,943 tweets. Instead of a binary label (a positive sentiment on climate change or a negative, there are four possible labels:\n",
    "\n",
    "Label | Description\n",
    ":--- | :---- \n",
    "News | the tweet links to factual news about climate change\n",
    "Pro | the tweet supports the belief of man-made climate change\n",
    "Neutral |  the tweet neither supports nor refutes the belief of man-made climate change\n",
    "Anti | the tweet does not believe in man-made climate change\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>1. Load the data.</font> \n",
    "\n",
    "The file is \n",
    "\n",
    "https://raw.githubusercontent.com/zacharski/ml-class/master/data/climateSentiment.csv.zip\n",
    "\n",
    "You will need:\n",
    "\n",
    "* to load the file\n",
    "* convert the text of the tweet to a tf-idf representation. We will start with using the 5,000 most common words\n",
    "* convert the labels\n",
    "* divide into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "gvC3tJUiCppb",
    "outputId": "73e4d977-2c9c-4596-c1ba-780f5dd5e7d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "10bOFMCa-2Y9",
    "outputId": "5b3b8e79-617c-4380-d9d5-c58588944f38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4H5XRUog_as5",
    "outputId": "895889ff-b33c-493a-cc77-2e785c7d5a2d"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXTd0E9wBZQK",
    "outputId": "5a08a17d-e7c2-497b-fd8d-f2276e5a2715"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "uTiI3Ejz_rRe",
    "outputId": "9f6c094c-2341-46a8-eee2-5624a9e0fb88"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzPe1NnLpeG3"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Create a deep learning densely connected network.</font> \n",
    "\n",
    "You can decide how many layers and how many nodes per layer. Keep in mind that for the imdb task the last layer was:\n",
    "\n",
    "```\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "The `1` was selected because we only had a binary choice (positive or negative). `sigmoid` was also selected because we had a binary choice.  This will not be the same for this task. In the imdb example, we used the binary_crossentropy loss function because, again, we only had a binary choice.\n",
    "\n",
    "Create the network, compile it, and fit it to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_TOQQHKBKHG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3MqPKtsEalS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob7PnaBgFB__",
    "outputId": "07780ac6-54ef-40c9-80f4-a14ccb5e9a17"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMNg3FIoqrEE"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Plot the accuracy and loss for both the training and validation sets.</font> \n",
    "\n",
    "Also, state in a few sentences what you see in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "bBr_F10YFOzJ",
    "outputId": "8400bcae-0cbb-45f3-a33d-92493543bb27"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qipxYocrmy6"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>4. Accuracy on the test data</font> \n",
    "\n",
    "What is the accuracy on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh9VDWCGFfGc",
    "outputId": "d22a4991-e71d-4728-8f39-3f0cc43dfb15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6sHJ2hEryR3"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>5. Can you do better than the baseline accuracy? +5-10 xp</font> \n",
    "Can you create a network that has better accuracy than that shown in #4 above?\n",
    "\n",
    "You can change:\n",
    "\n",
    "* the number of layers, \n",
    "* the number of nodes in each layer\n",
    "* change the `num_words` used in the tokenizer\n",
    "* add one or more dropout layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0U5UMresDkj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
