{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "L8bIz-dx6t3l",
    "outputId": "b6a77ca1-e6a5-4ecb-fb69-f79c69bc5e98"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcwCWVoR6t3q"
   },
   "source": [
    "# Using word embeddings\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/wordEmbeddingsPic.png)\n",
    "#\n",
    "\n",
    "## Meaning\n",
    "For hundreds of years linguists have been investigating how to encode the semantic information of a word. Some believe that there is a universal mental concept, for example `MOON`, with some universal representation in our brains that gets represented in English as *moon*, in Uyghur as ئاي, as *Hanhepi wi* in Lakota, and as \n",
    "月 in Japanese. It is also believed that  concepts such as `MOON` and `SUN` share some common semantic features. Similar words would cluster together in semantic space. For example, moon, sun, Mars, Neptune, the Milky Way Galaxy seem to have some similarities and form a group as does a group we could label domestic animals that might include horses, dogs, and cats, which in turn are different from wild animals. So words like *dog*, *cat*, *horse*, have, as part of their semantic meaning, some feature that is the same among these words  and that feature is not present in the representations for the words *moon* or *sun*. \n",
    "\n",
    "We might consider doing this by hand. For example, consider the words:\n",
    "\n",
    "* astronaut\n",
    "* cat\n",
    "* chair\n",
    "* dog\n",
    "* donkey\n",
    "* owl\n",
    "\n",
    "We might represent the semantic closeness of the words by distance. So dog and cat are the most similar:\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/semantics2.png)\n",
    "\n",
    "From there we can decide on a list of semantic features and represent each word as a collection of these features, \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/semantics3.png) \n",
    "\n",
    "As you can imagine, coming up with this feature list, and then labeling each word, is extremely difficult. Linguists have been struggling with this for centuries.\n",
    "\n",
    "But we can have a computer do it.\n",
    "\n",
    "\n",
    "## Latent Features\n",
    "Latent features are simply features that are hidden---we don't know what they are but we know words share some features. These semantic features impact how a word occurs in different contexts. \n",
    "\n",
    "\n",
    "For example, it would be rare for a verb to follow the word *the*:\n",
    "\n",
    "* *The says ...\n",
    "* *The haven't\n",
    "* *The see\n",
    "\n",
    "(Here the asterisk * represents a sentence most people would find odd). And again, this is just probability. Certainly, you can construct sentences that have these sequences that are perfectly fine (*The says who questioning method .*, *Just experimenting with the Says Who basics on myself or with clients*, *The haven't had time excuse*). But the likelihood of the word *says* following *the* is extremely rare. \n",
    "\n",
    "Similarly, sentences like\n",
    "\n",
    "* I fed my ___\n",
    "* I fed my dog\n",
    "* I fed my poodle\n",
    "* I fed my cat\n",
    "* I fed my horse\n",
    "\n",
    "occur much more frequently than\n",
    "\n",
    "* I fed my ___\n",
    "* I fed my moon\n",
    "* I fed my sun\n",
    "* I fed my Neptune\n",
    "* I fed my Milky Way Galaxy.\n",
    "\n",
    "So, we come to the quote by Firth:\n",
    "\n",
    "> You shall know a word by the company it keeps - John Rupert Firth\n",
    "\n",
    "The idea is a simple one and is based on counting and statistics. If we find a bunch of words that occur in the same context, we can assume that they share some semantic feature. And, if we didn't know a language we could still do this analysis. Considier:\n",
    "\n",
    "* le di de comer a mi gato\n",
    "* le di de comer a mi perro\n",
    "* le di de comer a mi burro\n",
    "* le di de comer a mi caballo\n",
    "\n",
    "* el perro comió la carne\n",
    "* el burro comió el grano\n",
    "* el gato comió pescado en el almuerzo\n",
    "* el caballo comio la hierba\n",
    "\n",
    "> NOTE: These sentences may not be correct Spanish sentence as I used Google Translate. If you are a Spanish speaker and notice an error, please let me know.\n",
    "\n",
    "In these examples, since the words *gato, perro, burro, cabello*, occur in the same contexts we can assume they share some semantic feature.  We wouldn't know that the feature is animal but we would know that they share some feature, let's call it x29. This, as we talked about is a latent feature.\n",
    "\n",
    "Now, without going into any detail whatsoever, we can imagine giving an algorithm a gigaword corpus and telling it to come up with 100 semantic features that explain the distribution of the words in the corpus. \n",
    "\n",
    "That is what **word embeddings** are in their simplist form. \n",
    "\n",
    "\n",
    "### Sparse vs. dense dimensions.\n",
    "In our previous work with text, we determined the size of our vocabulary, say 10,000 words, and created a vector where each column represented a different word in the vocabulary. So let's say column 1 was *a* and 2 *the*, and 7,253 *computer* and so on. If we encode a sentence that starts *the computer*, the word *the* would be represented by a '1' in column 1 and zeroes in the other 9,999 columns. The word *computer* would have a '1' in column 7,253 and zeroes in the other 9,999. That is an awful lot of zeroes, and this vector is called **sparse**. This looks like the first image below, where the blue square represents a '1' and the black represents all zeroes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cH3xchp86t3r"
   },
   "source": [
    "![word embeddings vs. one hot encoding](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/wordEmbeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3l4_QvP6t3r"
   },
   "source": [
    "With word embeddings we determine a feature size, 100, 200, 300 dimensions, and then each word is represented by a vector with values between 0 and 1. This is shown in the dense diagram above where the different colors represent different fractional values. \n",
    "\n",
    "In the one-hot-encoding method, each word was represented by a sparse vector the size of the vocabulary and vectors of 10,000 or 20,000 entries are not uncommon. In contrast, in the word embedding method, each word is represented by a dense vector of only 100 or 300 values. Thus, in the word embedding approach information is packed into a much smaller vector. \n",
    "\n",
    "In addition, in the one-hot encoding method there were no relationships between words. For example, *poodle* might be word 9,712 and *dog* 1,797, and they were treated completely separately. In the word embedding approach, the similarities of *poodle* and *dog* are represented within the word embeddings. \n",
    "\n",
    "\n",
    "#### Once again for emphasis\n",
    "While the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the \n",
    "number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors \n",
    "\n",
    "\n",
    "#### Note\n",
    "The following notebook is a remix of one by Francis Cholet (see the end of the notebook for more information)\n",
    "\n",
    "\n",
    "### Obtaining word embeddings\n",
    "There are two ways to obtain word embeddings:\n",
    "\n",
    "* Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). \n",
    "In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
    "* Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. \n",
    "These are called \"pre-trained word embeddings\". \n",
    "\n",
    "Let's take a look at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbhGdm0e6t3s"
   },
   "source": [
    "## Learning word embeddings with the `Embedding` layer\n",
    "\n",
    "\n",
    "The simplest way to associate a dense vector to a word would be to pick the vector at random. For example, we could assign binary numbers to each word randomly.\n",
    "\n",
    "index | binary | word\n",
    "--: | --: | :---\n",
    "1 | 00000001 | dog\n",
    "2 | 00000010 | moon\n",
    "3 | 00000011 | chair\n",
    "... | ... | ...\n",
    "238 | 11101110 | standard poodle\n",
    "239 | 11101111 | smart phone\n",
    "\n",
    "\n",
    "The problem with this approach is that the \n",
    "resulting embedding space would have no structure. For instance, the words \"accurate\" and \"exact\" may end up with completely different \n",
    "embeddings, even though they are interchangeable in most sentences. It would be very difficult for a deep neural network to make sense of \n",
    "such a noisy, unstructured embedding space. \n",
    "\n",
    "To get a bit more abstract: the geometric relationships between word vectors should reflect the semantic relationships between these words. \n",
    "Word embeddings are meant to map human language into a geometric space. For instance, in a reasonable embedding space, we would expect \n",
    "synonyms to be embedded into similar word vectors, and in general we would expect the geometric distance (e.g. L2 distance) between any two \n",
    "word vectors to relate to the semantic distance of the associated words (words meaning very different things would be embedded to points \n",
    "far away from each other, while related words would be closer). Even beyond mere distance, we may want specific __directions__ in the \n",
    "embedding space to be meaningful. \n",
    "\n",
    "\n",
    "\n",
    "In real-world word embedding spaces, common examples of meaningful geometric transformations are \"gender vectors\" and \"plural vector\". For \n",
    "instance, by adding a \"female vector\" to the vector \"king\", one obtain the vector \"queen\". By adding a \"plural vector\", one obtain \"kings\". \n",
    "Word embedding spaces typically feature thousands of such interpretable and potentially useful vectors.\n",
    "\n",
    "Is there some \"ideal\" word embedding space that would perfectly map human language and could be used for any natural language processing \n",
    "task? Possibly, but in any case, we have yet to compute anything of the sort. Also, there isn't such a thing as \"human language\", there are \n",
    "many different languages and they are not isomorphic, as a language is the reflection of a specific culture and a specific context. But more \n",
    "pragmatically, what makes a good word embedding space depends heavily on your task: the perfect word embedding space for an \n",
    "English-language movie review sentiment analysis model may look very different from the perfect embedding space for an English-language \n",
    "legal document classification model, because the importance of certain semantic relationships varies from task to task.\n",
    "\n",
    "It is thus reasonable to __learn__ a new embedding space with every new task. Thankfully, backpropagation makes this really easy, and Keras makes it \n",
    "even easier. It's just about learning the weights of a layer: the `Embedding` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81dFNJsz6t3s"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# The Embedding layer takes at least two arguments:\n",
    "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
    "# and the dimensionality of the embeddings, here 64.\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUqdtrTj6t3v"
   },
   "source": [
    "\n",
    "The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n",
    "as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tPFE8U-6t3w"
   },
   "source": [
    "\n",
    "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of \n",
    "integers. So, for example, we may have a dataset of 10,000 tweets so that 10,000 is the `samples` and each sample consists of a sequence of integers representing the words in the tweet. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have \n",
    "shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must \n",
    "have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded \n",
    "with zeros, and sequences that are longer should be truncated.\n",
    "\n",
    "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. \n",
    "\n",
    "When you instantiate an `Embedding` layer, its weights (its internal dictionary of token vectors) are initially random, just like with any \n",
    "other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the \n",
    "downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for \n",
    "the specific problem you were training your model for.\n",
    "\n",
    "Let's apply this idea to the IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare \n",
    "the data. We will restrict the movie reviews to the top 20,000 most common words \n",
    "and cut the reviews after only 50 words. Our network will simply learn 50-dimensional embeddings for each of the 20,000 words, turn the \n",
    "input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single `Dense` \n",
    "layer on top for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3ORYjSkkdo9",
    "outputId": "021aab35-41a5-4d76-c400-77aac93f449b"
   },
   "outputs": [],
   "source": [
    "!wget http://zacharski.org/files/courses/cs419/imdb.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Sc_HJW6lov9Q",
    "outputId": "c487b4ab-0e90-4916-a9e6-f0271b8d2ffe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('imdb.zip')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dU805Bd7pAOO",
    "outputId": "cd7a7a83-fe36-4fa3-8433-cecf22cbe4fa"
   },
   "outputs": [],
   "source": [
    "data_text = data.review\n",
    "data_label =data.sentiment\n",
    "data_label =  data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "data_label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvb9ry5-o_6u"
   },
   "source": [
    "### Converting words to integers\n",
    "The first step we will do is convert words represented as strings to integers\n",
    "\n",
    "We will use the Keras Tokenizer and specify the size of the vocabulary.\n",
    "\n",
    "* num_words: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DB6KIHrZpBOk",
    "outputId": "39918e81-3b33-4da8-d877-baccd5651c87"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = 20000)\n",
    "#build the word index\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "# now convert the words to integers\n",
    "data_sequences = tokenizer.texts_to_sequences(data_text)\n",
    "data_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8zymRGLrOtA"
   },
   "source": [
    "Just for grins, let's convert an IMDB review back to text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0SOKyfKrri7",
    "outputId": "cb253727-c235-424c-ac9c-d3fcfe96bd2c"
   },
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts([data_sequences[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAs2Yc6HvSya"
   },
   "source": [
    "### Recap.\n",
    "We stated we wanted the vocabulary size to be 20,000. This means that if we have a sentence like *xenophobic manifestations moved online* and *xenophobic* was not among the 20,000 most frequent words in the corpus, our encoding of that sentence would skip that word. *xenophobic manifestations moved online* becomes *manifestations moved online*\n",
    "\n",
    "Now let's do a bit more preprocessing and truncate each review after 50 words (if the review is shorter than 50 words we will pad it with blank words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHpnZnf-6t3w",
    "outputId": "98c8e4ca-d17f-454a-f1c1-70ebfb520014"
   },
   "outputs": [],
   "source": [
    "# Cut texts after this number of words \n",
    "# (among top max_features most common words)\n",
    "maxlen = 50\n",
    "\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "# This turns our lists of integers\n",
    "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
    "data50 = pad_sequences(data_sequences, maxlen=maxlen)\n",
    "data50[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhLEQdtOwrOC"
   },
   "source": [
    "# Now let's divide the data into training and testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeKwllN9w0dR",
    "outputId": "c9e811b9-2cf9-40f3-b20b-50cf74440f86"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train50, test50, train_labels, test_labels = train_test_split(data50, data_label, test_size = 0.2, random_state=42)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYl2Y9WAxV94"
   },
   "source": [
    "# Time to build the model\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/theModel.png)\n",
    "So far we have:\n",
    "\n",
    "* a 10,000 word vocabulary\n",
    "* each review is limited to 50 words\n",
    "\n",
    "And we want to create a word embedding with 50 features.\n",
    "\n",
    "(Those 50 are completely separate parameters) The 50 word limit is not related to the 50 features.)\n",
    "\n",
    "Let's create a network with an embedding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDdDVHVb6t3z",
    "outputId": "e81e6e6c-07bc-41cd-93e2-da5602eaaab5"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "vocabulary_size = 20000\n",
    "embedding_size = 50\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, embedding_size)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * embedding_size)`\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyAB-38my7Vg"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mIT1Lbvy9so",
    "outputId": "cae0c6c4-163e-4170-8f00-17eb3b089fe9"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train50, train_labels,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=10,\n",
    "      validation_split=0.2,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B1mpi9B6t31"
   },
   "source": [
    "We get to a validation accuracy of ~82%, which is pretty good considering that we only look at the first 50 words in every review. But \n",
    "note that merely flattening the embedded sequences and training a single `Dense` layer on top leads to a model that treats each word in the \n",
    "input sequence separately, without considering inter-word relationships and structure sentence (e.g. it would likely treat both _\"this movie \n",
    "is shit\"_ and _\"this movie is the shit\"_ as being negative \"reviews\"). It would be much better to add recurrent layers or 1D convolutional \n",
    "layers on top of the embedded sequences to learn features that take into account each sequence as a whole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "a9jXqsHX2Z8E",
    "outputId": "11f2cbd5-8fb9-461b-ffa5-5577a9ae2712"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTCuQtE22k3l"
   },
   "source": [
    "Arghh. These graphs look like overfitting. Let's try on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DT2t_RM2zEc",
    "outputId": "faa30690-bd48-410a-a0bd-5f561b4526bf"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc, = model.evaluate(test50, test_labels)\n",
    "print(\"Accuracy %f    Loss: %f\" % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOrcukRO6t31"
   },
   "source": [
    "## Using pre-trained word embeddings\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/pretrained.png)\n",
    "\n",
    "Sometimes, we have so little training data available that we could never use the data alone to learn an appropriate task-specific embedding \n",
    "of your vocabulary. What to do then?\n",
    "\n",
    "Instead of learning word embeddings jointly with the problem we want to solve, we could be loading embedding vectors from a pre-computed \n",
    "embedding space known to be highly structured and to exhibit useful properties -- that captures generic aspects of language structure. The \n",
    "rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets \n",
    "in image classification: we don't have enough data available to learn truly powerful features on our own, but we expect the features that \n",
    "we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a \n",
    "different problem.\n",
    "\n",
    "Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or \n",
    "documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space \n",
    "for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking \n",
    "off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec \n",
    "algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.\n",
    "\n",
    "There are various pre-computed databases of word embeddings that can download and start using in a Keras `Embedding` layer. Word2Vec is one \n",
    "of them. Another popular one is called \"GloVe\", developed by Stanford researchers in 2014. It stands for \"Global Vectors for Word \n",
    "Representation\", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made \n",
    "available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.\n",
    "\n",
    "Finally, there is FastText, also developed by Mikolov at Facebook. While state of the art, it is also the most resource intensive scheme.\n",
    "\n",
    "Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec \n",
    "embeddings or any other word embedding database that you can download. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpnanKp26t32"
   },
   "source": [
    "## Summary so far,\n",
    "\n",
    "We...\n",
    "\n",
    "1. downloaded the text data which was in the form of a csv file\n",
    "2. loaded the file into pandas.\n",
    "3. divided the csv into the text columns and the labels columns\n",
    "4. tokenized the text into sequences of integers\n",
    "5. limited each text to 50 words\n",
    "5. divided the sequences and labels into training and test data\n",
    "\n",
    "So, for example, the first entry of our data was\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqTEl1ys6t33",
    "outputId": "d9a6213b-1490-430b-a935-955a73aa059c"
   },
   "outputs": [],
   "source": [
    "train50[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3rxV_MqLQhw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7GNizUp6t38"
   },
   "source": [
    "### Download the GloVe word embeddings\n",
    "\n",
    "\n",
    "Head to https://nlp.stanford.edu/projects/glove/ (where you can learn more about the GloVe algorithm), and download the pre-computed \n",
    "embeddings from 2014 English Wikipedia. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
    "400,000 words (or non-word tokens). The downloading will take a bit of time, which might be an understatement. \n",
    "\n",
    "For convenience, I have made available a zip of the exact file we will need at http://zacharski.org/files/courses/cs419/glove.6B.100d.zip This is only 134MB\n",
    "\n",
    "\n",
    "Next, un-zip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GN0y6D-PBOcA",
    "outputId": "3c722522-dc0b-44c4-8338-21932f9a2ff3"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "!curl http://zacharski.org/files/courses/cs419/glove.6B.100d.zip > glove.6B.100d.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nphXkQhgC_EJ",
    "outputId": "25f12773-7c6b-475b-f131-49cbad92fb65"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "!unzip glove.6B.100d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6hBJa5p6t39"
   },
   "source": [
    "### Pre-process the embeddings\n",
    "\n",
    "\n",
    "Let's parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number \n",
    "vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zboejuFj6t39",
    "outputId": "a666c59e-706a-47bf-9ea1-ae88315ee035"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBlb3xFu6t3_"
   },
   "source": [
    "So embeddings_index is a Python dictionary whose keys are words and values are the 100 length vector. Let's get the vector for the word *dog*:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORc86o_40bt3",
    "outputId": "d6b8d514-4ae2-475c-fd87-d674d44f6162"
   },
   "outputs": [],
   "source": [
    "embeddings_index['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MSSrCeZ0cSn"
   },
   "source": [
    "So this embedding file has a vocabulary size of 400,000 words and each word is associated with a 100 element vector. Let's not use all 400,000, but restrict our vocabulary to our original 20,000 (`max_words`).\n",
    "\n",
    "Now let's build an embedding matrix that we will be able to load into an `Embedding` layer. It must be a matrix of shape `(max_words, \n",
    "embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
    "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4djtC3A6t3_"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_words = 20000\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EEPKbcd6t4C"
   },
   "source": [
    "### Define a model\n",
    "\n",
    "We will be using the same model architecture as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmRvo6sb6t4C",
    "outputId": "1359a773-c21c-423d-c8d7-26488c55348f"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSVZCgJy6t4F"
   },
   "source": [
    "### Load the GloVe embeddings in the model\n",
    "\n",
    "\n",
    "The `Embedding` layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with \n",
    "index `i`. Simple enough. Let's just load the GloVe matrix we prepared into our `Embedding` layer, the first layer in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GUR2IMr6t4G"
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyiOZNs76t4J"
   },
   "source": [
    "\n",
    "Additionally, we freeze the embedding layer (we set its `trainable` attribute to `False`), following the same rationale as what you are \n",
    "already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like our `Embedding` layer), \n",
    "and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting \n",
    "what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already \n",
    "learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taMHdF-a6t4J"
   },
   "source": [
    "### Train and evaluate\n",
    "\n",
    "Let's compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-mHYyp86t4J",
    "outputId": "3a898de2-baf1-407c-d92a-a533fbdef845"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train50, train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                     validation_split=0.2,\n",
    "      validation_steps=50)\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "212yP1DI6t4M"
   },
   "source": [
    "Let's plot its performance over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "w4Eince06t4M",
    "outputId": "234a0106-a931-4a66-fca6-f95092c8ad6a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSUsNaK9a-pn"
   },
   "source": [
    "The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for \n",
    "the same reason, but seems to reach high 50s.\n",
    "\n",
    "Let's check the accuracy and loss on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxLT8H3Ca-3q",
    "outputId": "413fd07a-03cb-4f39-899e-cf72e93cc4ec"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc, = model.evaluate(test50, test_labels)\n",
    "print(\"Accuracy %f    Loss: %f\" % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec7-y6ZTbgtZ"
   },
   "source": [
    "### A one-dimensional CNN\n",
    "\n",
    "Let's try a slightly more complex model using convolutional layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDq47rpDbuDl",
    "outputId": "7c367dfb-91a2-47da-f09b-ede43ca66886"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "cnn_model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(layers.Dropout(0.5))\n",
    "cnn_model.add(layers.MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(32, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0CHffQ1dPVC"
   },
   "outputs": [],
   "source": [
    "cnn_model.layers[0].set_weights([embedding_matrix])\n",
    "cnn_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65Fn8YhWdQtP",
    "outputId": "9e495e65-23d4-4453-f83a-3123ca3836d4"
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = cnn_model.fit(train50, train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                     validation_split=0.2,\n",
    "      validation_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hM-GdMKqeCrK",
    "outputId": "32df415f-8e80-4aa0-b329-0799265e54e2"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc, = cnn_model.evaluate(test50, test_labels)\n",
    "print(\"Accuracy %f    Loss: %f\" % (test_acc, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K0Ed_X7eSec"
   },
   "source": [
    "This is better than our first attempt at using pre-trained embeddings but not as good as when we learned the embeddings on our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB79P4nm6t4R"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Let's Reflect\n",
    "\n",
    "### Experiment 1\n",
    "We  trained the model without loading the pre-trained word embeddings and without freezing the embedding layer. In that \n",
    "case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings\n",
    "when lots of data is available. \n",
    "\n",
    "### Experiment 2\n",
    "However, we used a pre-existing word-embedding. This is especially useful when we have a limited amount of training data.\n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "## <font color='#EE4C2C'>You Try Experiment 3</font> \n",
    "WLet's try a new approach. Suppose we loaded the GloVe pre-trained word embeddings but this time didn't freeeze the layer, meaning it would continue to refine the embeddings using our training data. How does that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxHX4E_A6t4S",
    "outputId": "e84ac3d7-6447-47fc-b79e-83fd6584d665"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJGU2CPq6t4U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rE3fGB6Herue",
    "outputId": "a7e123e4-d592-4ace-da50-6a0f9baaada1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lnz4cX7cfSU5",
    "outputId": "7114b013-95eb-4d13-8bf4-96861cbfd1f2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xN-GWKg6t4W"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "# <font color='#EE4C2C'>. You try - Climate Change Tweets</font> \n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/climateChange2.png)\n",
    "\n",
    "We are going to investigate the sentiment of tweets from the Twitter Climate Change Sentiment Dataset compiled by Edward Qian. The dataset consists of 43,943 tweets. Instead of a binary label (a positive sentiment on climate change or a negative, there are four possible labels:\n",
    "\n",
    "Label | Description\n",
    ":--- | :---- \n",
    "News | the tweet links to factual news about climate change\n",
    "Pro | the tweet supports the belief of man-made climate change\n",
    "Neutral |  the tweet neither supports nor refutes the belief of man-made climate change\n",
    "Anti | the tweet does not believe in man-made climate change\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>1. FastText.</font> \n",
    "\n",
    "Instead of using the GloVe embedding vectors we will use FastText. The FastText vectors for the most common 20,000 words is available at\n",
    "\n",
    "\n",
    "https://github.com/zacharski/ml-class/raw/master/data/fasttext.zip\n",
    "\n",
    "We can process it in the same way as we did the GloVe vector file.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "JVl6o8_96t4W",
    "outputId": "d709fd96-4c04-40ab-b8ca-1925ee089e3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsZzK-1FSISM"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>2. Download and process the data file</font> \n",
    "\n",
    "The file is \n",
    "\n",
    "https://raw.githubusercontent.com/zacharski/ml-class/master/data/climateSentiment.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGSlhkmURtgB",
    "outputId": "7686fa5d-ad94-4c35-aecb-2306e516ec11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1o3pm3QSk30"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>3. Create the network/model--fit it to the data</font> \n",
    "You can design your network anyway you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "xEvO_RvU0SUs",
    "outputId": "c91a8e1e-f5f5-4f1e-80f2-3d2a03a8d540"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx73-VQGS45v"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>4. Accuracy with the test data</font> \n",
    "How well did this do? Is it better than the accuracy on this dataset from our previous notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXouPhBp0y9t"
   },
   "source": [
    "encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H90SONxM01BU",
    "outputId": "922920b3-cf48-40b4-d867-5676b82162a1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dV_5H54TORs"
   },
   "source": [
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/PyDivideTwo.png)\n",
    "## <font color='#EE4C2C'>5. Better than 75% accuracy - bonus 10xp</font> \n",
    "Can you modify your network and training to get better results? For example, you can train for fewer or more epochs. You can add or subtract layers to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITvo0sfG1JGv",
    "outputId": "c3149497-d7a9-4073-875b-15288ddd6799"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEkeaxnyi2K0"
   },
   "source": [
    "#### MIT License\n",
    "Copyright (c) 2017 François Chollet\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
